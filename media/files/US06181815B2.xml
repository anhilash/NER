<?xml version="1.0" encoding="UTF-8"?>
<questel-patent-document lang="en" date-produced="20180805" produced-by="Questel" schema-version="3.23" file="US06181815B2.xml">
  <bibliographic-data lang="en">
    <publication-reference publ-desc="Granted patent as second publication">
      <document-id>
        <country>US</country>
        <doc-number>06181815</doc-number>
        <kind>B2</kind>
        <date>20010130</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>US6181815</doc-number>
      </document-id>
    </publication-reference>
    <original-publication-kind>B2</original-publication-kind>
    <application-reference is-representative="YES" family-id="12583107" extended-family-id="20735838">
      <document-id>
        <country>US</country>
        <doc-number>09030743</doc-number>
        <kind>A</kind>
        <date>19980225</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>1998US-09030743</doc-number>
      </document-id>
      <document-id data-format="questel_Uid">
        <doc-number>21267759</doc-number>
      </document-id>
    </application-reference>
    <language-of-filing>en</language-of-filing>
    <language-of-publication>en</language-of-publication>
    <priority-claims>
      <priority-claim kind="national" sequence="1">
        <country>JP</country>
        <doc-number>4053397</doc-number>
        <kind>A</kind>
        <date>19970225</date>
        <priority-active-indicator>Y</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="1">
        <doc-number>1997JP-0040533</doc-number>
      </priority-claim>
    </priority-claims>
    <dates-of-public-availability>
      <publication-of-grant-date>
        <date>20010130</date>
      </publication-of-grant-date>
    </dates-of-public-availability>
    <classifications-ipcr>
      <classification-ipcr sequence="1">
        <text>G06K   9/00        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>06</class>
        <subclass>K</subclass>
        <main-group>9</main-group>
        <subgroup>00</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="2">
        <text>G01C  15/00        20060101AFI20060310RMJP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>01</class>
        <subclass>C</subclass>
        <main-group>15</main-group>
        <subgroup>00</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>JP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20060310</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="3">
        <text>G01C   3/06        20060101ALI20060310RMJP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>01</class>
        <subclass>C</subclass>
        <main-group>3</main-group>
        <subgroup>06</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>JP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20060310</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="4">
        <text>G06T   1/00        20060101ALI20060310RMJP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>06</class>
        <subclass>T</subclass>
        <main-group>1</main-group>
        <subgroup>00</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>JP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20060310</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="5">
        <text>G06T   7/00        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>06</class>
        <subclass>T</subclass>
        <main-group>7</main-group>
        <subgroup>00</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
    </classifications-ipcr>
    <classification-national>
      <country>US</country>
      <main-classification>
        <text>382154000</text>
        <class>382</class>
        <subclass>154000</subclass>
      </main-classification>
      <further-classification sequence="1">
        <text>345419000</text>
        <class>345</class>
        <subclass>419000</subclass>
      </further-classification>
      <further-classification sequence="2">
        <text>348094000</text>
        <class>348</class>
        <subclass>094000</subclass>
      </further-classification>
      <further-classification sequence="3">
        <text>348139000</text>
        <class>348</class>
        <subclass>139000</subclass>
      </further-classification>
      <further-classification sequence="4">
        <text>382201000</text>
        <class>382</class>
        <subclass>201000</subclass>
      </further-classification>
      <further-classification sequence="5">
        <text>382285000</text>
        <class>382</class>
        <subclass>285000</subclass>
      </further-classification>
      <further-classification sequence="6">
        <text>382291000</text>
        <class>382</class>
        <subclass>291000</subclass>
      </further-classification>
    </classification-national>
    <classifications-ecla>
      <classification-ecla sequence="1">
        <text>G06T-007/00R7</text>
        <section>G</section>
        <class>06</class>
        <subclass>T</subclass>
        <main-group>007</main-group>
        <subgroup>00R7</subgroup>
      </classification-ecla>
      <classification-ecla sequence="2">
        <text>G06K-009/00F1</text>
        <section>G</section>
        <class>06</class>
        <subclass>K</subclass>
        <main-group>009</main-group>
        <subgroup>00F1</subgroup>
      </classification-ecla>
    </classifications-ecla>
    <patent-classifications>
      <patent-classification sequence="1">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>G06K-009/00228</classification-symbol>
        <section>G</section>
        <class>06</class>
        <subclass>K</subclass>
        <main-group>9</main-group>
        <subgroup>00228</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20170105</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="2">
        <classification-scheme office="EP" scheme="CPC">
          <date>20170101</date>
        </classification-scheme>
        <classification-symbol>G06T-007/55</classification-symbol>
        <section>G</section>
        <class>06</class>
        <subclass>T</subclass>
        <main-group>7</main-group>
        <subgroup>55</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20170102</date>
        </action-date>
      </patent-classification>
    </patent-classifications>
    <number-of-claims>14</number-of-claims>
    <exemplary-claim>1</exemplary-claim>
    <figures>
      <number-of-drawing-sheets>6</number-of-drawing-sheets>
      <number-of-figures>9</number-of-figures>
      <image-key data-format="questel">US6181815</image-key>
    </figures>
    <invention-title format="original" lang="en" id="title_en">Subject image extraction device</invention-title>
    <references-cited>
      <citation srep-phase="examiner">
        <patcit num="1">
          <text>MURAI SHUNJI, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4858157</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4858157</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="2">
          <text>USAMI YOSHIAKI, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4982438</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4982438</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="3">
          <text>CANON KK</text>
          <document-id>
            <country>JP</country>
            <doc-number>H07177423</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>JP07177423</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="4">
          <text>CANON KK</text>
          <document-id>
            <country>JP</country>
            <doc-number>H07220095</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>JP07220095</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="5">
          <text>CASIO COMPUTER CO LTD</text>
          <document-id>
            <country>JP</country>
            <doc-number>H08271914</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>JP08271914</doc-number>
          </document-id>
        </patcit>
      </citation>
    </references-cited>
    <parties>
      <applicants>
        <applicant data-format="original" app-type="applicant" sequence="1">
          <addressbook lang="en">
            <orgname>NEC Corporation</orgname>
            <address>
              <address-1>Tokyo, JP</address-1>
              <city>Tokyo</city>
              <country>JP</country>
            </address>
          </addressbook>
          <nationality>
            <country>JP</country>
          </nationality>
        </applicant>
        <applicant data-format="questel" app-type="applicant" sequence="2">
          <addressbook lang="en">
            <orgname>NEC</orgname>
          </addressbook>
          <nationality>
            <country>JP</country>
          </nationality>
        </applicant>
      </applicants>
      <inventors>
        <inventor data-format="original" sequence="1">
          <addressbook lang="en">
            <name>Marugame, Atsushi</name>
            <address>
              <address-1>Tokyo, JP</address-1>
              <city>Tokyo</city>
              <country>JP</country>
            </address>
          </addressbook>
          <nationality>
            <country>JP</country>
          </nationality>
        </inventor>
      </inventors>
      <agents>
        <agent sequence="1" rep-type="agent">
          <addressbook lang="en">
            <orgname>Ostrolenk, Faber, Gerb &amp; Soffen, LLP</orgname>
          </addressbook>
        </agent>
      </agents>
    </parties>
    <examiners>
      <primary-examiner>
        <name>Bella, Matthew</name>
      </primary-examiner>
    </examiners>
    <lgst-data>
      <lgst-status>LAPSED</lgst-status>
    </lgst-data>
  </bibliographic-data>
  <abstract format="original" lang="en" id="abstr_en">
    <p id="P-EN-00001" num="00001">
      <br/>
      A main image pickup section for picking up a main image and two or more subsidiary image pickup sections for picking up subsidiary images are used to shoot a subject.
      <br/>
      A 3-D object on which a predetermined pattern is drawn is also shot by the same image pickup sections, and parameters representing relative positions and attitudes of the image pickup sections are obtained using the images of the 3-D object.
      <br/>
      Feature points are extracted from each of the main image and the subsidiary images, and 3-D coordinates corresponding to each feature point in the main image are determined using the parameters and the feature points in the subsidiary images.
      <br/>
      Subsequently, judgment is executed and each of the feature points in the main image is accepted if the 3-D coordinates is within a designated 3-D space domain.
      <br/>
      Subsequently, subject outline points are extracted from the accepted feature points.
      <br/>
      Then, part of the main image which is surrounded by the subject outline points is extracted as the subject image.
      <br/>
      According to the device, subject image extraction can be extracted with high accuracy, without information of the subject or the image pickup device, without control or measurement of the positions of the image pickup devices, and with easy setting of a threshold value.
    </p>
  </abstract>
  <description format="original" lang="en" id="desc_en">
    <heading>BACKGROUND OF THE INVENTION</heading>
    <p num="1">The present invention relates to a subject image extraction device for extracting an image of a desired subject from an inputted image, and in particular, to a subject image extraction device which can extract an image of a subject which existed in a designated 3-D space domain at the moment of shooting.</p>
    <heading>DESCRIPTION OF THE PRIOR ART</heading>
    <p num="2">Various kinds of methods are proposed for extracting an image of a desired subject or object from an inputted image, such as a method utilizing prestored information concerning the subject as disclosed in Japanese Patent Application Laid-Open No.HEI7-220095 (hereafter, referred to as `document No.1`), a method utilizing two or more image pickup devices (cameras) as disclosed in Japanese Patent Application Laid-Open No.HEI7-177423 (hereafter, referred to as `document No.2`) and Japanese Patent Application No.HEI8-271914 (hereafter, referred to as `document No.3`), etc.</p>
    <p num="3">
      In the method utilizing prestored subject information which is disclosed in the document No.1, information concerning the image pickup device such as the focal length of the image pickup device, the size of a pixel of a sensor of the image pickup device, the number of pixels of the sensor, etc. and information concerning the subject such as the size of the subject, the distances between the center point on the subject and parts of the subject, etc. are preliminarily recorded and stored in a storage unit.
      <br/>
      Then, the size of the subject in the image which has been picked up by the image pickup device is estimated using the information concerning the image pickup device and the subject.
      <br/>
      Meanwhile, a plurality of parts of the image which are considered to be candidates of the image of the subject are extracted from the image.
      <br/>
      Then, it is judged whether each of the candidate parts is the genuine subject image or not, by comparing the size of the candidate part with the estimated size of the subject in the image.
      <br/>
      And one candidate part whose size is the nearest to the estimated size of the subject in the image is extracted as the image of the subject.
    </p>
    <p num="4">
      In the method utilizing two or more image pickup devices which is disclosed in the document No.2, corresponding points in each image which have been picked up by the two or more image pickup devices are searched for, and disparities between the images with regard to the corresponding points are obtained using the difference of coordinates between the corresponding points in each image.
      <br/>
      Then, part of an image whose disparity between the images are large (i.e. parts which existed in the foreground near to the image pickup devices at the moment of shooting) are extracted as the image of the subject.
    </p>
    <p num="5">
      In the method utilizing two or more image pickup devices which is disclosed in the document No.3. The subject is shot in the centers of images by the two or more image pickup devices, and the aforementioned search of point correspondence (hereafter, referred to as `point correspondence search`) is executed from the center of the images.
      <br/>
      Then, outline point judgment (i.e. judgment whether a point in an image is a point on the outline of the subject or not) is executed using 3-D coordinates (corresponding to the point) which are calculated using the corresponding points obtained by the point correspondence search, and the outline points are used for extraction of the subject image.
      <br/>
      Even when false correspondence occurred in the point correspondence search and parts of the outline points dropped out from the extraction, such parts are restored by means of outline restoration assuming continuity.
    </p>
    <p num="6">
      However, in the method disclosed in the document No.1, the aforementioned information concerning the subject such as the size of the subject, the distances between the center point on the subject and parts of the subject, etc. and the information concerning the image pickup device such as the focal length of the image pickup device, the size of a pixel, the number of pixels, the distance between the subject and the image pickup device, etc. are needed to be prepared, and thus images of subjects whose information have not been prepared are impossible to be extracted from the image which has been picked up by the image pickup device.
      <br/>
      Further, according to the method, either the distance between the center point on the subject and part of the subject or the distance between the principle point of the image and part of the image is necessary as the information.
      <br/>
      In the case where the former distance is used, expensive devices such as an infrared irradiation unit etc. are necessary for obtaining the distance.
      <br/>
      And in the case where the latter distance is used, methods such as autofocusing, which does not operate stably under insufficient shooting conditions such as poor lighting etc., have to be employed, and thus stable extraction of the subject image is difficult.
    </p>
    <p num="7">
      In the method utilizing two or more image pickup devices and judging the depth using the disparity, the accuracy of subject image extraction is highly dependent on setting of a threshold value (in disparity) which is used to distinguish between the background and the foreground, and the setting of the disparity threshold value for accurate subject image extraction is very difficult for the operator of the subject image extraction device since the disparity is not a value which can be directly measured.
      <br/>
      The method of the document No.2 tries to resolve the problem by obtaining 3-D coordinates using the disparities.
      <br/>
      However, the method needs troublesome control or measurement of relative positions of the two or more image pickup devices, angles between the optical axes of the image pickup devices, etc.
      <br/>
      Further, according to the method, when false correspondence occurred in the point correspondence search, the part where the false correspondence occurred is extracted as noise, or in other words, parts of the subject image drop out.
    </p>
    <p num="8">
      In the method disclosed in the document No.3, the aforementioned problem concerning the difficulty of the threshold value determination is resolved by calculating 3-D coordinates without using the disparities, in which parameters which represent relative positions of the image pickup devices and angles between the optical axes of the image pickup devices are calculated using images of a predetermined pattern picked up by the image pickup devices, instead of controlling or measuring the relative positions of the image pickup devices and angles between the optical axes of the image pickup devices.
      <br/>
      Further, with regard to the aforementioned problem concerning the point correspondence search, error rate in the point correspondence search is reduced in the method of the document No.3, by shooting the subject in the centers of the images by the image pickup devices and executing the point correspondence search starting from the center of the images with high priority.
      <br/>
      Furthermore, even in the case where the false correspondence occurred in the point correspondence search, parts which have dropped out from the extraction or parts which have been incorrectly extracted are corrected by the outline restoration on the assumption of continuity.
      <br/>
      However, according to the method, all the image pickup devices have to be correctly controlled to shoot the subject in the centers of images.
      <br/>
      Further, in the case where the background has a complex scene, error rate in the point correspondence search increases and thus the outline restoration is necessitated to be executed considerably oftener, causing difficulty in the subject image extraction especially when the subject has a complex outline.
    </p>
    <heading>SUMMARY OF THE INVENTION</heading>
    <p num="9">It is therefore the primary object of the present invention to provide a subject image extraction device by which an image of a desired subject can be extracted from an inputted image, without needing preparation of the information concerning the subject or the image pickup device.</p>
    <p num="10">Another object of the present invention is to provide a subject image extraction device by which an image of a desired subject can be extracted from an inputted image, with easy setting of a threshold value for distinguishing between the background and the foreground.</p>
    <p num="11">Another object of the present invention is to provide a subject image extraction device by which an image of a desired subject can be extracted from an inputted image, without needing precise control or measurement of the positions and attitudes of image pickup devices.</p>
    <p num="12">Another object of the present invention is to provide a subject image extraction device by which 3-D coordinates corresponding to a point in an inputted image can be accurately obtained and thereby subject image extraction can be executed with higher accuracy.</p>
    <p num="13">
      In accordance with the present invention, there is provided a subject image extraction device comprising an image storage means, a feature point extraction means, a parameter calculation means, a 3-D coordinates calculation means, a 3-D coordinates determination means, a judgment means, an outline point extraction means, and a subject extraction means.
      <br/>
      The image storage means includes main image storage section for storing images which have been picked up by a main image pickup section for picking up a main image from which an image of a subject is extracted, and two or more subsidiary image storage sections for storing images which have been picked up by two or more subsidiary image pickup sections for picking up subsidiary images to be referred to in subject image extraction.
      <br/>
      The feature point extraction means includes three or more feature point extraction sections for extracting feature point coordinates in the images which have been stored in each of the image storage sections.
      <br/>
      The parameter calculation means calculates parameters which represent relative positions and attitudes of the main image pickup section and the subsidiary image pickup sections, using the feature point coordinates in the images of a 3-D object on which a predetermined pattern is drawn which have been picked up by the main image pickup section and the subsidiary image pickup sections.
      <br/>
      The 3-D coordinates calculation means includes two or more 3-D coordinates calculation sections for calculating 3-D candidate coordinates which are considered to correspond to the feature point coordinates of a feature point in the main image, using the feature point coordinates in the main image, the feature point coordinates in the subsidiary images, and the parameters obtained by the parameter calculation means.
      <br/>
      The 3-D coordinates determination means determines genuine 3-D coordinates which correspond to the feature point in the main image, using the 3-D candidate coordinates calculated by the 3-D coordinates calculation sections in the 3-D coordinates calculation means.
      <br/>
      The judgment means judges whether or not the genuine 3-D coordinates corresponding to the feature point in the main image is in a designated 3-D space domain, and accepts the feature point if the genuine 3-D coordinates corresponding to the feature point is judged to be in the designated 3-D space domain.
      <br/>
      The outline point extraction means extracts subject outline points from a plurality of feature points which have been accepted by the judgment means.
      <br/>
      And the subject extraction means extracts part of the main image which is surrounded by the subject outline points as the subject image.
    </p>
    <p num="14">Preferably, the feature point extraction sections in the feature point extraction means extract edge-like parts in the images where large changes occur in color intensity or luminance as the feature point coordinates, in the extraction of the feature point coordinates from the main image or the subsidiary images.</p>
    <p num="15">Preferably, the parameter calculation means calculates eleven or more parameters per one image pickup section.</p>
    <p num="16">Preferably, the 3-D coordinates determination means determines the genuine 3-D coordinates by selecting a group of 3-D coordinates from each of the 3-D candidate coordinates calculated by each of the 3-D coordinates calculation sections so that the sum of distances between the 3-D coordinates included in the group will become the smallest and defining the genuine 3-D coordinates by the average of the 3-D coordinates included in the group.</p>
    <p num="17">Preferably, the subject image extraction device further comprises an image pickup means including the main image pickup section and the two or more subsidiary image pickup sections.</p>
    <p num="18">Preferably, the subject image extraction device further comprises a subject display means for displaying the part of the main image which has been extracted by the subject extraction means.</p>
    <p num="19">Preferably, the number of the image pickup sections is three.</p>
    <p num="20">
      In accordance with another aspect of the present invention, there is provided a subject image extraction device comprising an image storage means, a feature point neighborhood extraction means, a parameter calculation means, a corresponding point determination means, a 3-D coordinates calculation means, a 3-D coordinates determination means, a judgment means, an outline point extraction means, and a subject extraction means.
      <br/>
      The image storage means includes main image storage section for storing images which have been picked up by a main image pickup section for picking up a main image from which an image of a subject is extracted, and two or more subsidiary image storage sections for storing images which have been picked up by two or more subsidiary image pickup sections for picking up subsidiary images to be referred to in subject image extraction.
      <br/>
      The feature point neighborhood extraction means includes three or more feature point neighborhood extraction sections for extracting feature point coordinates in the images which have been stored in each of the image storage sections and extracting color/luminance information of pixels in the neighborhood of the feature point coordinates.
      <br/>
      The parameter calculation means calculates parameters which represent relative positions and attitudes of the main image pickup section and the subsidiary image pickup sections, using the feature point coordinates in the images of a 3-D object on which a predetermined pattern is drawn which have been picked up by the main image pickup section and the subsidiary image pickup sections.
      <br/>
      The corresponding point determination means includes two or more corresponding point determination sections for selecting a group of candidate corresponding points in the subsidiary image which are considered to correspond to the feature point in the main image, from the feature points in the subsidiary images, using the feature point coordinates in the main image and the subsidiary image and the color/luminance information of pixels in the neighborhood of the feature point coordinates which have been extracted by the feature point neighborhood extraction means.
    </p>
    <p num="21">
      The 3-D coordinates calculation means includes two or more 3-D coordinates calculation sections for calculating 3-D candidate coordinates which are considered to correspond to a feature point in the main image, using the feature point coordinates in the main image, the group of candidate corresponding points, and the parameters obtained by the parameter calculation means.
      <br/>
      The 3-D coordinates determination means determines genuine 3-D coordinates which correspond to the feature point in the main image, using the 3-D candidate coordinates calculated by the 3-D coordinates calculation sections in the 3-D coordinates calculation means.
    </p>
    <p num="22">
      The judgment means judges whether or not the genuine 3-D coordinates corresponding to the feature point in the main image is in a designated 3-D space domain, and accepts the feature point if the genuine 3-D coordinates corresponding to the feature point is judged to be in the designated 3-D space domain.
      <br/>
      The outline point extraction means extracts subject outline points from a plurality of feature points which have been accepted by the judgment means.
      <br/>
      And the subject extraction means extracts part of the main image which is surrounded by the subject outline points as the subject image.
    </p>
    <p num="23">Preferably, the feature point neighborhood extraction sections in the feature point extraction means extract edge-like parts in the images where sudden changes occur in color intensity or luminance as the feature point coordinates, in the extraction of the feature point coordinates from the main image or the subsidiary images.</p>
    <p num="24">Preferably, the parameter calculation means calculates eleven or more parameters per one image pickup section.</p>
    <p num="25">Preferably, the 3-D coordinates determination means determines the genuine 3-D coordinates by selecting a group of 3-D coordinates from each of the 3-D candidate coordinates calculated by each of the 3-D coordinates calculation sections so that the sum of distances between the 3-D coordinates included in the group will become the smallest and defining the genuine 3-D coordinates by the average of the 3-D coordinates included in the group.</p>
    <p num="26">Preferably, the subject image extraction device further comprises an image pickup means including the main image pickup section and the two or more subsidiary image pickup sections.</p>
    <p num="27">Preferably, the subject image extraction device further comprises a subject display means for displaying the part of the main image which has been extracted by the subject extraction means.</p>
    <p num="28">Preferably, the number of the image pickup sections is three.</p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading>
    <p num="29">
      The objects and features of the present invention will become more apparent from the consideration of the following detailed description taken in conjunction with the accompanying drawings, in which:
      <br/>
      FIG. 1 is a block diagram showing a subject image extraction device according to an embodiment of the present invention;
      <br/>
      FIG. 2 is a schematic diagram showing preferred arrangement of image pickup sections 1a, 1b and 1c of the subject image extraction device of FIG. 1;
      <br/>
      FIG. 3 is a perspective view showing an example of a 3-D object on which a predetermined pattern is drawn which is used in the embodiment of FIG. 1;
      <br/>
      FIG. 4A is a schematic diagram showing a main image used in the embodiment of FIG. 1;
      <br/>
      FIG. 4B is a schematic diagram showing a subsidiary image used in the embodiment of FIG. 1;
      <br/>
      FIG. 5 is a schematic diagram showing the principle of triangular surveying;
      <br/>
      FIG. 6 is a schematic diagram showing a 3-D coordinates determination process of the subject image extraction device of FIG. 1;
      <br/>
      FIG. 7 is a schematic diagram explaining `block matching` used in the embodiment of FIG. 8; and
      <br/>
      FIG. 8 is a block diagram showing a subject image extraction device according to another embodiment of the present invention.
    </p>
    <heading>DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
    <p num="30">Referring now to the drawings, a description will be given in detail of preferred embodiments in accordance with the present invention.</p>
    <p num="31">FIG. 1 is a block diagram showing a subject image extraction device according to an embodiment of the present invention.</p>
    <p num="32">The subject image extraction device of FIG. 1 comprises an image pickup unit 1, an image storage unit 2, a feature point extraction unit 3, a parameter calculation unit 4, a 3-D coordinates calculation unit 5, a 3-D coordinates determination unit 6, an outline point extraction unit 7, and a subject display unit 8.</p>
    <p num="33">
      The image pickup unit 1 is composed of an image pickup section 1a for picking up a main image to be mainly used in a subject image extraction process which will be described below, and image pickup sections 1b and 1c for picking up subsidiary images to be referred to in the subject image extraction process.
      <br/>
      The image pickup sections 1a, 1b and 1c are preliminarily used to shoot a 3-D object 21 on which a predetermined pattern is drawn in order to obtain parameters 400 which will be described below, and are used to shoot a subject 22 whose image will be extracted using the parameters 400 etc.
    </p>
    <p num="34">
      The image storage unit 2 is composed of image storage sections 2a, 2b and 2c.
      <br/>
      The image storage sections 2a, 2b and 2c store images 10a, 10b and 10c of the 3-D object 21 and images 20a, 20b and 20c of the subject 22 which have been picked up by the image pickup sections 1a, 1b and 1c, respectively.
    </p>
    <p num="35">
      The feature point extraction unit 3 is composed of feature point extraction sections 3a, 3b and 3c.
      <br/>
      The feature point extraction sections 3a, 3b and 3c extract coordinates 30a, 30b and 30c of a plurality of feature points from the images 10a, 10b and 10c which have been stored in the image storage sections 2a, 2b and 2c respectively, and extract coordinates 50a, 50b and 50c of a plurality of feature points from the images 20a, 20b and 20c which have been stored in the image storage sections 2a, 2b and 2c respectively.
    </p>
    <p num="36">The parameter calculation unit 4 calculates and stores the parameters 400 which represent relative positions and attitudes of the main image pickup section 1a and the subsidiary image pickup sections 1b and 1c, using the feature point coordinates 30a, 30b and 30c which have been extracted by the feature point extraction sections 3a, 3b and 3c respectively.</p>
    <p num="37">
      The 3-D coordinates calculation unit 5 is composed of 3-D coordinates calculation sections 5b and 5c.
      <br/>
      The 3-D coordinates calculation section 5b calculates 3-D candidate coordinates 60b using the parameters 400 and the feature point coordinates 50a and 50b which have been extracted by the feature point extraction sections 3a and 3b.
      <br/>
      The 3-D coordinates calculation section 5c calculates 3-D candidate coordinates 60c using the parameters 400 and the feature point coordinates 50a and 50c which have been extracted by the feature point extraction sections 3a and 3c.
      <br/>
      Incidentally, the 3-D candidate coordinates 60b is a group of 3-D coordinates (i.e. a group of 3-D vectors each of which is representing 3-D coordinates) which are considered to be candidates for genuine 3-D coordinates which correspond to the feature point (2-D) coordinates 50a in the main image 20a which have been extracted by the feature point extraction section 3a.
      <br/>
      Similarly, the 3-D candidate coordinates 60c is a group of 3-D coordinates (i.e. a group of 3-D vectors each of which is representing 3-D coordinates) which are considered to be candidates for genuine 3-D coordinates which correspond to the feature point (2-D) coordinates 50a in the main image 20a which have been extracted by the feature point extraction section 3a.
    </p>
    <p num="38">The 3-D coordinates determination unit 6 determines genuine 3-D coordinates 700 which correspond to the feature point coordinates 50a in the main image 20a, using the 3-D candidate coordinates 60b and the 3-D candidate coordinates 60c.</p>
    <p num="39">
      The outline point extraction unit 7 judges whether the feature point 50a is a point in the subject 22 or not, using the 3-D coordinates 700, and extracts subject outline point coordinates 800.
      <br/>
      Here, the subject outline point coordinates 800 is a group of 2-D coordinates (i.e. a group of 2-D vectors each of which is representing 2-D coordinates) which correspond to a plurality of subject outline points (i.e. feature points 50a which exist on the outline of the subject 22).
    </p>
    <p num="40">The subject display unit 8 displays part of the image 20a which are surrounded by the subject outline points, using the subject outline point coordinates 800 extracted by the outline point extraction unit 7.</p>
    <p num="41">
      The feature point extraction unit 3, the parameter calculation unit 4, the 3-D coordinates calculation unit 5, the 3-D coordinates determination unit 6, and the outline point extraction unit 7 are realized, for example, by a microprocessor unit which is composed of a CPU, ROM (Read Only Memory), RAM (Random Access Memory), etc., and necessary software.
      <br/>
      The image storage unit 2 is realized by one or more storage devices such as an HDD (Hard Disk Drive), an MO (Magneto-Optical disk), etc.
      <br/>
      The subject display unit 8 is realized, for example, by a display unit such as an LCD (Liquid Crystal Display), etc. and a microprocessor unit.
    </p>
    <p num="42">In the following, the operation of the subject image extraction device of FIG. 1 will be described, in which both a point itself and coordinates of the point are expressed by the same word `point` for brevity.</p>
    <p num="43">In this embodiment, the parameters 400 which represent relative positions and attitudes of the image pickup sections 1a, 1b and 1c are needed to be calculated by the parameter calculation unit 4 first of all, using the coordinates 30a, 30b and 30c of the feature points in the images 10a, 10b and 10c of the 3-D object 21.</p>
    <p num="44">For the calculation of the parameters 400, the image pickup sections 1a, 1b and 1c respectively pick up images 10a, 10b and 10c of the 3-D object 21 on which a predetermined pattern (which is shown in FIG. 3, for example) is drawn.</p>
    <p num="45">
      The optical axes of the image pickup sections 1a, 1b and 1c may be parallel, or may have convergence angles between them.
      <br/>
      Referring to FIG. 2, the main image pickup section 1a, for picking up the main image 10a is placed in the middle of the three image pickup sections 1a, 1b and 1c.
      <br/>
      Such arrangement is advantageous since occlusion can be made smaller.
    </p>
    <p num="46">The images 10a, 10b and 10c of the 3-D object 21 on which the predetermined pattern is drawn picked up by the image pickup sections 1a, 1b and 1c are stored in the image storage sections 2a, 2b and 2c respectively, and then supplied to the feature point extraction sections 3a, 3b and 3c in the feature point extraction unit 3, respectively.</p>
    <p num="47">
      After the images 10a, 10b and 10c are supplied to the feature point extraction unit 3, six or more feature points P0, P1, P2, P3, P4, P5, . . . are chosen so that all the feature points will not be in one plane of the 3-D object 21, more concretely, so that a 3-D coordinate system can be at least generated using four points out of the six or more feature points P0, P1, P2, P3, P4, P5, . . . Subsequently, coordinates 30a, 30b and 30c of the feature points P0, P1, P2, P3, P4, P5, . . . in the images 10a, 10b and 10c are extracted by the feature point extraction sections 3a, 3b and 3c, respectively.
      <br/>
      Here, each of the coordinates 30a, 30b and 30c is a group of 2-D coordinates (corresponding to the feature points P0, P1, P2, P3, P4, P5, . . . ) in the image 10a, 10b, 10c, respectively.
      <br/>
      The choice of the feature points can be done by various kinds of methods, for example, by instruction of the operator of the device who is watching the images 10a, 10b and 10c, or by automatic extraction by means of image processing such as pattern matching etc.
    </p>
    <p num="48">Then, the parameter calculation unit 4 calculates the parameters 400 using the coordinates 30a, 30b and 30c of the feature points P0, P1, P2, P3, P4, P5, . . . , and the calculated parameters 400 are stored in the parameter calculation unit 4.</p>
    <p num="49">
      The calculation of the parameters 400 can be executed based on a method which is disclosed in Japanese Patent Application No.HEI7-206932.
      <br/>
      In the following, the method for the calculation of the parameters 400 will be described.
    </p>
    <p num="50">
      First, a 3-D coordinate system is defined so that coordinates of the feature points P0, P1, P2 and P3 (shown in FIG. 3) in the 3-D coordinate system will be (0, 0, 0), (1, 0, 0), (0, 1, 0) and (0, 0, 1) respectively.
      <br/>
      Here, the 3-D coordinate system is not needed to be a rectangular coordinate system with rectangular axes as is defined by the P0, P1, P2 and P3 which are shown in FIG. 3, and generally, the 3-D coordinate system may be an oblique coordinate system which is defined by feature points P0, P1, P2 and P3 which exist on oblique axes.
      <br/>
      Hereafter, coordinates of the feature points P4 and P5 in the 3-D coordinate system will be expressed as (X4, Y4, Z4) and (X5, Y5, Z5) respectively.
    </p>
    <p num="51">
      Incidentally, 2-D coordinates of the feature points P0, P1, P2, P3, P4 and P5 in the image 10a will be hereafter expressed as
      <br/>
      (uic,vic)(i=0, . . . ,5),
    </p>
    <p num="52">
      and coordinates of the feature points P0, P1, P2, P3, P4 and P5 in the image 10b will be hereafter expressed as
      <br/>
      (uil, vil)(i=0, . . . ,5),
    </p>
    <p num="53">
      and coordinates of the feature points P0, P1, P2, P3 P4 and P5 in the image 10c will be hereafter expressed as
      <br/>
      (uir, vir)(i=0, . . . ,5),
    </p>
    <p num="54">
      Here, using the coordinates:
      <br/>
      (uic, vic),
    </p>
    <p num="55">
      of the feature points in the image 10a and the 3-D coordinates (Xk, Yk, Zk) (k=4, 5) of the feature points P4 and P5, and using the following parameters:
      <br/>
      cjc (j=1, . . . ,3),
    </p>
    <p num="56">which represent the attitude of the main image pickup section 1a, the following equations (1), (2), (3) and (4) are derived.</p>
    <p num="57">
      +X4(u4c -u1c)}c1c ++Y4(u4c -u2c)}c2c ++Z4(u4c -u3c)}c3c =(1-X4-Y4-Z4)(u0c -u4c) (1)
      <br/>
      +X4(v4c -v1c)}c1c ++Y4(v4c -v2c)}c2c ++Z4(v4c -v3c)}c3c =(1-X4-Y4-Z4)(v0c -v4c) (2)
      <br/>
      +X5(u5c -u1c)}c1c ++Y5(u5c -u2c)}c2c ++Z5(u5c -u3c)}c3c =(1-X5-Y5-Z5)(u0c -u5c) (3)
      <br/>
      +X5(v5c -v1c)}c1c ++Y5(v5c -v2c)}c2c ++Z5(v5c -v3c)}c3c =(1-X5-Y5-Z5)(v0c -v5c) (4)
    </p>
    <p num="58">
      The above parameters:
      <br/>
      cjc (j=1, . . . ,3),
    </p>
    <p num="59">
      are elaborated on in A. Marugame et al., "Structure Recovery from Scaled Orthographic and Perspective Views," Proceedings IEEE ICIP-96, vol.2, pp.851-854 (1996), and are representing the strain (extension/contraction) of three axes of the 3-D coordinate system due to projection of the 3-D coordinate system onto 2-D plane by the image pickup section 1a.
      <br/>
      The parameters can be obtained from the equations (1), (2), (3) and (4), using, for example, method of least squares, which is described for example in a document: K. Kanatani, "Image understanding", Morikita Shuppan, Japan (1990).
    </p>
    <p num="60">
      Parameters:
      <br/>
      cjl (j=1, . . . ,3),
    </p>
    <p num="61">which represent the attitude of the subsidiary image pickup section 1b and parameters:</p>
    <p num="62">cjr (j=1, . . . ,3),</p>
    <p num="63">which represent the attitude of the subsidiary image pickup section 1c can be obtained by the same method as the case of the main image pickup section 1a.</p>
    <p num="64">
      The parameters:
      <br/>
      cjc, cjl, cjr (j=1, . . . ,3),
    </p>
    <p num="65">
      which have been obtained by the above calculation and the following coordinates:
      <br/>
      (uic,vic), (uil,vil), (uir,vir)(i=0, . . . ,3),
    </p>
    <p num="66">
      are stored in the parameter calculation unit 4 as the parameters 400.
      <br/>
      Here, the number of parameters included in the parameters 400 per one image pickup section (1a, 1b or 1c) is 11.
      <br/>
      It is generally known that 11 parameters per one image pickup device are enough for expressing relative positions and attitudes of a plurality of image pickup devices.
    </p>
    <p num="67">
      Now that the parameters 400 representing relative positions and attitudes of the image pickup sections 1a, 1b and 1c have been obtained and stored in the parameter calculation unit 4, extraction of desired subject in the main image 20a can be executed.
      <br/>
      In the following, the subject image extraction process will be described in detail.
    </p>
    <p num="68">The subject 22 is shot by the image pickup sections 1a, 1b and 1c of the image pickup unit 1 and images 20a, 20b and 20c are obtained respectively.</p>
    <p num="69">The images 20a, 20b and 20c are stored in the image storage sections 2a, 2b and 2c of the image storage unit 2, respectively.</p>
    <p num="70">From the images 20a, 20b and 20c which have been stored in the image storage sections 2a, 2b and 2c respectively, feature point coordinates 50a, 50b and 50c are respectively extracted by the feature point extraction sections 3a, 3b and 3c of the feature point extraction unit 3.</p>
    <p num="71">
      The extraction of the feature points may be done by various kinds of methods, for example, by instruction of the operator of the device who is watching the images 20a, 20b and 20c, or by automatic extraction using differential filters such as Sobel filter, Laplacian filter, etc.
      <br/>
      The extraction of feature points 50a are usually executed in units of pixels in the main image 20a.
      <br/>
      In the case where differential filters are utilized for the aforementioned automatic extraction of the feature points 50a, edge-like parts in the main image 20a where large changes occur in color intensity (in cases of color image) or luminance (in cases of black and white image) are extracted as the feature points 50a, and thereby a few of all the pixels in the main image 20a are extracted as the feature points 50a by the feature point extraction section 3a, for example.
      <br/>
      The same goes for the extraction of feature points 50b and 50c from the images 20b and 20c.
    </p>
    <p num="72">
      A plurality of feature points 50a are extracted one after another by the feature point extraction section 3a in the feature point extraction unit 3, and each feature point 50a will be adopted as being a point on the outline of the subject 22 (i.e. adopted as a `subject outline point`) or rejected as not being a subject outline point, by the processes which will be described below.
      <br/>
      Finally, a group of subject outline points which are obtained as above will be used for extraction of the subject image.
    </p>
    <p num="73">
      The 3-D coordinates calculation section 5b in the 3-D coordinates calculation unit 5 calculates the 3-D candidate coordinates 60b using the parameters 400 and the feature points 50a and 50b, and the 3-D coordinates calculation section 5c calculates the 3-D candidate coordinates 60c using the parameters 400 and the feature points 50a and 50c.
      <br/>
      As mentioned above, the 3-D candidate coordinates 60b are a group of 3-D coordinates (i.e. a group of 3-D vectors each of which is representing 3-D coordinates) which are considered to be candidates for genuine 3-D coordinates which correspond to the feature point (2-D) coordinates 50a in the main image 20a which have been extracted by the feature point extraction section 3a, and the 3-D candidate coordinates 60c is a group of 3-D coordinates (i.e. a group of 3-D vectors each of which is representing 3-D coordinates) which are considered to be candidates for genuine 3-D coordinates which correspond to the feature point (2-D) coordinates 50a in the main image 20a which have been extracted by the feature point extraction section 3a.
    </p>
    <p num="74">In the following, the operation of the 3-D coordinates calculation section 5b will be described in detail.</p>
    <p num="75">
      The 3-D coordinates calculation section 5b chooses one of the feature points 50a one after another, and the following processes are executed to each of the feature points 50a.
      <br/>
      When 2-D coordinates of the feature point 50a in the image 20a is expressed as (uc, vc) as shown in FIG. 4A, the position of a feature point 501 in the image 20b which corresponds to the feature point 50a in the image 20a is restricted on a line 502 shown in FIG. 4B (which is called an epipolar line).
      <br/>
      The epipolar line 502 is determined according to the relation between the positions of the image pickup sections 1a and 1b.
      <br/>
      Therefore, the 3-D coordinates calculation section 5b calculates the epipolar line 502 using the following parameters which are included in the parameters 400.
      <br/>
      i cjc,cjl (j=1, . . . ,3), (uic,vic), (uil,vil)(i=0, . . . ,3),
    </p>
    <p num="76">The calculation of the epipolar line 502 can be executed according to the aforementioned document No.3 or a document: The proceedings of the 1st Image Media Processing Symposium (1996), pages 15-16, as follows.</p>
    <p num="77">
      First, using the parameters:
      <br/>
      (uic,vic)(i=0, . . . ,3), cjc (j=1, . . . ,3), (uc,vc),
    </p>
    <p num="78">
      the following primary intermediate parameters t11, t12, t13, t21, t22, t23, d1 and d2 are obtained.
      <br/>
      t11=+c1c (uc -u1c)-(uc -u0c)}  (5)
      <br/>
      t12=+c2c (uc -u2c)-(uc -u0c)}  (6)
      <br/>
      t13=+c3c (uc -u3c)-(uc -u0c)}  (7)
      <br/>
      t21=+c1c (vc -v1c)-(vc -v0c)}  (8)
      <br/>
      t22=+c2c (vc -v2c)-(vc -v0c)}  (9)
      <br/>
      t23=+c3c (vc -v3c)-(vc -v0c)}  (10)
      <br/>
      d1=u0c -uc  (11)
      <br/>
      d2=v0c -vc  (12)
    </p>
    <p num="79">
      Subsequently, using the primary intermediate parameters t11, t12, t13, t21, t22, t23, d1 and d2 and the following parameters:
      <br/>
      (Uil, Vil) (i=0, . . . ,3), Cjl (j=1, . . . ,3),
    </p>
    <p num="80">the following secondary intermediate parameters U0, U1, V0, V1, S0 and S1 are obtained.</p>
    <p num="81">
      U0 =u0l (t11 t22 -t12 t21)+(c1l u1l -u0l) (t22 d1 -t12 d2)+(c2l u2l -u0l)(t11 d2 -t21 d1)  (13)
      <br/>
      U1 =(c1l u1l -u0l)(t12 t23 -t13 t22)+(c2l u2l -u0l)(t13 t21 -t11 t23)+(c3l u3l -u0l)(t11 t22 -t12 t21)  (14)
      <br/>
      V0 =U0l (t11 t22 -t12 t21)+(c1l V1l -V0l) (t22 d1 -t12 d2)+(c2l v2l -v0l)(t11 d2 -t21 d1)  (15)
      <br/>
      V1 =(c1l v1l -v0l)(t12 t23 -t13 t22)+(c2l v2l -v0l)(t13 t21 -t11 t23)+(c3l v3l -v0l)(t11 t22 -t12 t21)  (16)
      <br/>
      S0 =(t11 t22 -t12 t21)+(c1l -1)(t22 d1 -t12 d2)+(c2l -1)(t11 d2 -t21 d1)  (17)
      <br/>
      S1 =(c1l -1)(t12 t23 -t13 t22)+(c2l -1)(t13 t21 -t11 t23)+(c3l -1)(t11 t22 -t12 t21)  (18)
    </p>
    <p num="82">
      Subsequently, using the secondary intermediate parameters U0, U1, V0, V1, S0 and S1, the parameters A, B and C of the epipolar line 502: Au+Bv+C=0 (Here, the (u, v) are 2-D coordinates in the image 20b.) are obtained by the following equations (19)-(21).
      <br/>
      As mentioned above, the epipolar line 502 is a line in the image 20b which restricts the position of the feature point 501 in the image 20b which corresponds to the feature point 50a in the image 20a.
    </p>
    <p num="83">
      A=S1 V0 -S0 V1  (19)
      <br/>
      B=-(S1 U0 -S0 U1)  (20)
      <br/>
      C=U0 V1 -U1 V0  (21)
    </p>
    <p num="84">Now that the parameters A, B and C of the epipolar line 502 have been obtained, feature points Qm : (um, vm) (m=1, . . . ) which exist in the  EPSILON 1 -neighborhood of the epipolar line 502 in the image 20b can be expressed by the following inequality.  (Equation image '1' not included in text)</p>
    <p num="85">
      Incidentally, the number of the feature points Qm in the  EPSILON 1 -neighborhood of the epipolar line 502 is finite (for example, of the order of 10), since feature points Qm are usually defined in units of pixels in the image 20b.
      <br/>
      The above feature points Qm : (um, vm) (m=1, . . . ) are candidate corresponding points in the image 20b which are considered to correspond to the feature point 50a: (uc, vc) in the image 20a.
    </p>
    <p num="86">
      After the feature points Qm : (um, vm) (m=1, . . . ) are obtained, the 3-D candidate coordinates 60b:
      <br/>
      (Xml,Yml,Zml) (m=1, . . . ),
    </p>
    <p num="87">are calculated for each pair of the feature point 50a and the feature point Qm (m=1, . . . ).</p>
    <p num="88">
      The calculation of the 3-D candidate coordinates 60b:
      <br/>
      (Xml,Yml,Zml) (m=1, . . . ),
    </p>
    <p num="89">
      can be executed based on the aforementioned Japanese Patent Application No.HEI7-206932 as follows, using the feature point 50a: (uC, vc), the feature points Qm : (um, vm) (m=1, . . . ), and the following parameters.
      <br/>
      (uic,vic), (uil,vil)(i=0, . . . ,3), cjc,cjl (j=1, . . . ,3),
    </p>
    <p num="90">
      First, referring to FIG. 5, a set of simultaneous equations which represent a back projection line 503 of the feature point 50a (uc, vc):
      <br/>
      +c1c (uc -u1c)-(uc -u0c)}Xml ++c2c (uc -u2c)-(uc -u0c)}Yml ++c3c (uc -u3c)-(uc -u0c)}Zml =u0c -uc  (23)
      <br/>
      +c1c (vc -v1c)-(vc -v0c)}Xml ++c2c (vc -v2c)-(vc -v0c)}Yml ++c3c (vc -v3c)-(vc -v0c)}Zml =v0c -vc  (24)
    </p>
    <p num="91">
      and a set of simultaneous equations which represent a back projection line 504 of the candidate corresponding point 51b (Qm : (um, vm)):
      <br/>
      +c1l (um -u1l)-(um -u0l)}Xml ++c2l (um -u2l)-(um -u0l)}Yml ++c3l)(um -u3l)-(um -u0l)}Zml =u0l -ul  (25)
      <br/>
      +c1l (vm -v1l)-(vm -v0l)}Xml ++c2l (vm -v2l)-(vm -v0l)}Yml ++c3l (vm -v3l)-(vm -v0l)}Zml =v0l -vl  (26)
    </p>
    <p num="92">are derived.</p>
    <p num="93">
      According to the principle of triangular surveying, the 3-D candidate coordinates 60b:
      <br/>
      (Xml,Yml,Zml)(m=1, . . . ),
    </p>
    <p num="94">
      can be obtained by solving the simultaneous equations (23). (24), (25) and (26).
      <br/>
      The solution:
      <br/>
      (Xml,Yml,Zml) (m=1, . . . ),
    </p>
    <p num="95">of the simultaneous equations (23), (24), (25) and (26) can be obtained by means of method of least squares etc.</p>
    <p num="96">
      The 3-D candidate coordinates 60b:
      <br/>
      (Xml,Yml,Zml) (m=1, . . . ),
    </p>
    <p num="97">obtained as above are sent to the 3-D coordinates determination unit 6, with the feature point coordinates 50a (Uc, Vc) attached thereto as a label.</p>
    <p num="98">
      Meanwhile, the 3-D coordinates calculation section 5c obtains the 3-D candidate coordinates 60c by the same method as the 3-D coordinates calculation section 5b, using the following parameters which are included in the parameters 400.
      <br/>
      (uic,vic), (uir,vir)(i=0, . . . ,3),cjc,Cjr (j=1, . . . ,3),
    </p>
    <p num="99">
      The obtained 3-D candidate coordinates 60c:
      <br/>
      (Xnr,Ynr,Znr) (n=1, . . . . ),
    </p>
    <p num="100">are sent to the 3-D coordinates determination unit 6, with the feature point coordinates 50a (uc, vc) attached thereto as a label.</p>
    <p num="101">The 3-D coordinates determination unit 6, which has been supplied with the 3-D candidate coordinates 60b and 60c from the 3-D coordinates calculation sections 5b and 5c, determines the genuine 3-D coordinates 700 using the 3-D candidate coordinates 60b and 60c which correspond to the same feature point 50a: (Uc, Vc).</p>
    <p num="102">
      In the following, the operation of the 3-D coordinates determination unit 6 will be described in detail.
      <br/>
      Referring to FIG. 6, the feature point 50a in the image 20a is in a one-to-one correspondence with a corresponding point 61 on the subject 22 in the 3-D coordinate system.
      <br/>
      Thus, a 3-D candidate point 60b which corresponds to a (2-D) candidate corresponding point 51b (Qm : (um, vm)) in the image 20b (Here, the candidate corresponding point 51b in the image 20b is a point which is considered to correspond to the feature point 50a in the image 20a.) and a 3-D candidate point 60c which corresponds to a (2-D) candidate corresponding point 51c in the image 20c (Here, the candidate corresponding point 51c in the image 20c is a point which is considered to correspond to the feature point 50a in the image 20a.) have to be the same point (i.e. the aforementioned corresponding point 61), and thus have to have almost the same 3-D coordinates.
      <br/>
      Therefore, for all the combinations of the 3-D candidate points 60b:
      <br/>
      (Xml,Yml,Zml) (m=1, . . . ),
    </p>
    <p num="103">
      and the 3-D candidate points 60c:
      <br/>
      (Xnr,Ynr,Znr)(n=1, . . . ),
    </p>
    <p num="104">
      , the following absolute norm dmn :
      <br/>
      dmn =(Xml -Xnr)2 +(Yml -Ynr)2 +(Zml -Znr)2  (27)
    </p>
    <p num="105">
      or the following L2 norm dmn :
      <br/>
      dmn =.vertline.Xml -Xnr.vertline.+.vertline.Yml -Ynr.vertline.+.vertline.Zml -Znr.vertline. (28 )
    </p>
    <p num="106">
      is calculated.
      <br/>
      Here, the norm dmn represents the distance between the 3-D candidate point 60b and the 3-D candidate point 60c.
      <br/>
      Then, if the minimun value d=dij (where dij =min(dmn)) is not larger than a threshold value  EPSILON 2, the feature point 50a: (uc, vc) is accepted and the 3-D coordinates (X, Y, Z) of the point 61 are determined as follows.
    </p>
    <p num="107">
      X=(X1l +Xjr)/2  (29)
      <br/>
      Y=(Yil +Yjr)/2  (30)
      <br/>
      Z=(Zil +Zjr)/2  (31)
    </p>
    <p num="108">
      And if the minimum value d=dij is larger than the threshold value  EPSILON 2, the feature point 50a: (uc, vc) is rejected or discarded as a point out of the subject.
      <br/>
      The above (preliminary) judgment using the threshold value  EPSILON 2 is executed in order to reject feature points 50a which have not been picked up by all of the image pickup sections 1a, 1b and 1c.
    </p>
    <p num="109">After the determination by the 3-D coordinates determination unit 6, the genuine 3-D coordinates 700: (X, Y, Z) of the point 61 is outputted to the outline point extraction unit 7, with the feature point coordinates 50a (uc, vc) in the image 20a attached thereto as a label.</p>
    <p num="110">
      The outline point extraction unit 7 first executes judgment whether the feature point 50a is a point in the subject 22 or not, in which the outline point extraction unit 7 accepts the feature point 50a: (uc, vc) (which has been attached to the 3-D coordinates 700: (X, Y, Z) of the point 61) if the 3-D coordinates 700: (X, Y, Z) is within the confines of a designated 3-D space domain which has been preliminarily determined or which is designated on the spot by the operator.
      <br/>
      A plurality of feature points 50a: (uc, vc) and their 3-D coordinates 700: (X, Y, Z) are supplied to the outline point extraction unit 7 successively, and the outline point extraction unit 7 executes the above judgment (whether the feature point 50a: (uc, vc) can be accepted or not) successively.
      <br/>
      Then, from a plurality of feature points 50a which have been accepted by the outline point extraction unit 7 and which exist on a horizontal line (i.e. a plurality of accepted feature points 50a: (uc, vc) which have the same Y coordinate vc), the outline point extraction unit 7 selects a left-hand feature point (a feature point 50a on the horizontal line whose X coordinate uc is the smallest) and a right-hand feature point (a feature point 50a on the horizontal line whose X coordinate uc is the largest), and sends the left-hand feature points and the right-hand feature points corresponding to each Y coordinate vc into the subject display unit 8 as the subject outline point coordinates 800.
    </p>
    <p num="111">Then, the subject display unit 8 displays part of the main image 20a which is surrounded by the subject outline point coordinates 800 (i.e. part of the image 20a between the left-hand feature points and the right-hand feature points corresponding to each Y coordinate Vc), thereby the image of the subject 22 is extracted from the main image 20a and displayed on the subject display unit 8.</p>
    <p num="112">
      As described above, in the first embodiment, the main image picking section 1a and the subsidiary image picking sections 1b and 1c are used to shoot the subject, and images 10a, 10b and 10c of the 3-D object 21 on which a predetermined pattern is drawn are also picked up by the image picking sections 1a, 1b and 1c in order to obtain parameters 400 which represent relative positions and attitudes of the image pickup sections 1a, 1b and 1c.
      <br/>
      In the embodiment, 11 parameters per one image pickup section (generally known to be enough for expressing relative positions and attitudes of a plurality of image pickup sections) are obtained by the parameter calculation unit 4.
      <br/>
      From the images 20a, 20b and 20c in which the subject 22 has been picked up, the feature points 50a, 50b and 50c are extracted by the feature point extraction unit 3, and the genuine 3-D coordinates 700 corresponding to each feature point 50a in the main image 20a are obtained by the 3-D coordinates calculation unit 5 and the 3-D coordinates determination unit 6 using the parameters 400 and the feature points 50b and 50c.
      <br/>
      In the outline point extraction unit 7, judgment whether each feature point 50a is a point in the subject 22 or not is executed using the 3-D coordinates 700 corresponding to the feature point 50a, in which a feature point 50a is accepted if the 3-D coordinates 700 corresponding to the feature point 50a is within the confines of a designated 3-D space domain which has been preliminarily determined or which is designated on the spot by the operator, and otherwise, the feature point 50a is rejected as not a point in the subject 22, and the subject outline point coordinates 800 are extracted from a plurality of feature points 50a which have been accepted.
      <br/>
      Then, part of the main image 20a which is surrounded by the subject outline point coordinates 800 is extracted and displayed by the subject display unit 8, thereby the subject image extraction from the main image 20a is completed.
    </p>
    <p num="113">Therefore, according to the first embodiment, an image of a desired subject can be extracted from the main image 20a, using the 3-D coordinates calculated for each extracted feature point 50a and executing the judgment with regard to the designated 3-D space domain which has been preliminarily determined or which is designated on the spot by the operator, without needing preparation of the information concerning the subject or the image pickup devices.</p>
    <p num="114">Further, according to the first embodiment, subject image extraction can be executed with easy setting of a threshold value for distinguishing between the background and the foreground, since 3-D coordinates corresponding to a feature point 50a in the image 20a are automatically calculated using the parameters 400 etc. and the operator of the device can set the threshold value by measurable value such as distance or by designating the 3-D space domain.</p>
    <p num="115">Further, according to the first embodiment, subject image extraction can be executed without needing precise control or measurement of the positions and attitudes of the image pickup devices, since the parameters 400 representing relative positions and attitudes of the image pickup sections 1a, 1b and 1c can be automatically obtained using the images 10a, 10b and 10c of the 3-D object 21 on which a predetermined pattern is drawn.</p>
    <p num="116">
      Further, according to the first embodiment, accuracy of subject image extraction can be remarkably improved, since the 3-D coordinates 700 which correspond to each extracted feature point 50a in the main image 20a can be accurately obtained using the parameters 400 calculated from the images 10a, 10b, 10c picked up by the image pickup sections 1a. 1b and 1c, and the feature points 50b and 50c extracted from the subsidiary images 20b and 20c.
      <br/>
      The accuracy of subject image extraction is increased with a synergistic effect by the extraction of the feature points 50a, 50b and 50c from the aforementioned edge-like parts of the images 20a, 20b and 20c and by the accurate calculation of the genuine 3-D coordinates 700 and by the designation of the designated 3-D space domain.
    </p>
    <p num="117">
      Incidentally, although three image pickup sections (i.e. one main image pickup section 1a and two subsidiary image pickup sections 1b and 1c) were used in the image pickup unit 1 in the above embodiment, it is also possible to use four or more image pickup sections (i.e. three or more subsidiary image pickup sections).
      <br/>
      In the case where N pieces of subsidiary image pickup sections are used, N+1 pieces of image storage sections, N+1 pieces of feature point extraction sections, and N pieces of 3-D coordinates calculation sections are used.
      <br/>
      In this case, (N+1) * 11 parameters (i.e. 11 parameters per image pickup sections) are calculated by the parameter calculation unit 4 and N groups of 3-D candidate coordinates (60b, 60c, . . . ) are obtained by the 3-D coordinates calculation unit 5, and in the 3-D coordinates determination unit 6, the sum of norms dmn (with regard to every pair of subsidiary images) are used instead of the norm dmn of the equation (27) or (28), and the averages in the equations (29) through (31) are replaced by averages divided by N (not by 2).
    </p>
    <p num="118">FIG. 8 is a block diagram showing a subject image extraction device according to the second embodiment of the present invention.</p>
    <p num="119">The subject image extraction device of FIG. 8 comprises an image pickup unit 1, an image storage unit 2, a feature point neighborhood extraction unit 33, a parameter calculation unit 4, a corresponding point determination unit 9, a 3-D coordinates calculation unit 55, a 3-D coordinates determination unit 6, an outline point extraction unit 7, and a subject display unit 8.</p>
    <p num="120">
      The image pickup unit 1 is composed of an image pickup section 1a for picking up the main image to be mainly used in the subject image extraction process, and image pickup sections 1b and 1c for picking up subsidiary images to be referred to in the subject image extraction process.
      <br/>
      The image pickup sections 1a, 1b and 1c are preliminarily used to shoot a 3-D object 21 on which a predetermined pattern is drawn in order to obtain the parameters 400, and are used to shoot a subject 22 whose image will be extracted using the parameters 400 etc.
    </p>
    <p num="121">
      The image storage unit 2 is composed of image storage sections 2a, 2b and 2c.
      <br/>
      The image storage sections 2a, 2b and 2c store images 10a, 10b and 10c of the 3-D object 21 and images 20a, 20b and 20c of the subject 22 which have been picked up by the image pickup sections 1a, 1b and 1c, respectively.
    </p>
    <p num="122">
      The feature point neighborhood extraction unit 33 is composed of feature point neighborhood extraction sections 33a, 33b and 33c.
      <br/>
      The feature point neighborhood extraction sections 33a, 33b and 33c extract coordinates 30a, 30b and 30c of a plurality of feature points from the images 10a, 10b and 10c which have been stored in the image storage sections 2a, 2b and 2c respectively, and extract coordinates 50a, 50b and 50c of a plurality of feature points from the images 20a, 20b and 20c which have been stored in the image storage sections 2a, 2b and 2c respectively, and further extract color/luminance information 52a, 52b and 52c of pixels in the neighborhood of the feature points 50a, 50b and 50c respectively.
    </p>
    <p num="123">The parameter calculation unit 4 calculates and stores the parameters 400 which represent relative positions and attitudes of the main image pickup section 1a and the subsidiary image pickup sections 1b and 1c, using the feature point coordinates 30a, 30b and 30c which have been extracted by the feature point extraction sections 3a, 3b and 3c respectively.</p>
    <p num="124">
      The corresponding point determination unit 9 is composed of corresponding point determination sections 9b and 9c.
      <br/>
      The corresponding point determination section 9b selects a group of candidate corresponding points 90b in the image 20b (points in the image 20b which are considered to correspond to the feature point 50a in the image 20a) from the feature points 50b, using the feature points 50a and 50b and the color/luminance information 52a and 52b of pixels in the neighborhood of the feature points 50a and 50b which have been extracted by the feature point neighborhood extraction sections 33a and 33b and using the parameters 400.
      <br/>
      The corresponding point determination section 9c selects a group of candidate corresponding points 90c in the image 20c (points in the image 20c which are considered to correspond to the feature point 50a in the image 20a) from the feature points 50c, using the feature points 50a and 50c and the color/luminance information 52a and 52c of pixels in the neighborhood of the feature points 50a and 50c which have been extracted by the feature point neighborhood extraction sections 33a and 33c and using the parameters 400.
    </p>
    <p num="125">
      The 3-D coordinates calculation unit 55 is composed of 3-D coordinates calculation sections 55b and 55c.
      <br/>
      The 3-D coordinates calculation section 55b calculates 3-D candidate coordinates 60b using the feature point 50a which has been extracted by the feature point neighborhood extraction section 33a, the candidate corresponding points 90b which have been selected by the corresponding point determination section 9b, and the parameters 400.
      <br/>
      The 3-D coordinates calculation section 55c calculates 3-D candidate coordinates 60c using the feature point 50a which has been extracted by the feature point neighborhood extraction section 33a, the candidate corresponding points 90c which have been selected by the corresponding point determination section 9c, and the parameters 400.
    </p>
    <p num="126">The 3-D coordinates determination unit 6 determines genuine 3-D coordinates 700 which correspond to the feature point coordinates 50a in the main image 20a, using the 3-D candidate coordinates 60b and the 3-D candidate coordinates 60c.</p>
    <p num="127">The outline point extraction unit 7 judges whether the feature point 50a is a point in the subject 22 or not, using the 3-D coordinates 700, and extracts subject outline point coordinates 800.</p>
    <p num="128">The subject display unit 8 displays part of the image 20a which are surrounded by the subject outline points, using the subject outline point coordinates 800 extracted by the outline point extraction unit 7.</p>
    <p num="129">
      In the following, the operations of the corresponding point determination unit 9 (which has been newly introduced into the second embodiment) and the feature point neighborhood extraction unit 33 and the 3-D coordinates calculation unit 55 (whose operations have been modified in the second embodiment) will be described in detail.
      <br/>
      The operations of the other blocks in FIG. 8 is the same as those of the first embodiment.
    </p>
    <p num="130">
      First, the operation of the feature point neighborhood extraction unit 33 will be described.
      <br/>
      When the images 10a, 10b and 10c of the 3-D object 21 on which the predetermined pattern is drawn are supplied to the feature point neighborhood extraction unit 33, the feature point neighborhood extraction sections 33a, 33b and 33c operate in the same way as the feature point extraction sections 3a, 3b and 3c of the first embodiment and extract the coordinates 30a, 30b and 30c of a plurality of feature points from the images 10a, 10b and 10c and output the feature point coordinates 30a, 30b and 30c to the parameter calculation unit 4, respectively.
      <br/>
      When the images 20a, 20b and 20c of the subject 22 are supplied to the feature point neighborhood extraction unit 33, the feature point neighborhood extraction sections 33a, 33b and 33c extract the feature points 50a, 50b and 50c respectively in the same way as the feature point extraction sections 3a, 3b and 3c of the first embodiment, and further extract color/luminance information (color intensity information in the case of color images or luminance information in the case of black and white images) 52a, 52b and 52c of pixels in the neighborhood of the feature points 50a, 50b and 50c respectively.
      <br/>
      The neighborhood can be defined in various ways, however, a rectangle whose center is on the feature point is generally used for the neighborhood.
    </p>
    <p num="131">
      Next, the operation of the corresponding point determination unit 9 will be described.
      <br/>
      The corresponding point determination section 9b in the corresponding point determination unit 9 extracts candidate corresponding points 5b (i.e. the Qm : (um, vm) (m=1, . . . ) which are obtained by the aforementioned inequality (22)) in the image 20b which corresponds to the feature point 50a by calculating the epipolar line in the same way as the 3-D coordinates calculation section 5b of the first embodiment.
      <br/>
      Subsequently, the corresponding point determination section 9b executes block matching, which is known as an image processing technique, between the color/luminance information 52a of the pixels in the neighborhood of the feature point 50a and the color/luminance information 52b of the pixels in the neighborhood of a candidate corresponding point 51b.
      <br/>
      FIG. 7 is a schematic diagram explaining the block matching.
      <br/>
      In the block matching between the color/luminance information 52a and 52b, two blocks of the same size respectively including the feature point 50a and the candidate corresponding point 51b are formed first, and comparison of the color/luminance between two corresponding points in the two blocks is executed.
      <br/>
      Then, the difference of the color/luminance is added up with respect to each point in the blocks (The sum is referred to as a matching error.).
      <br/>
      If the matching error is larger than a predetermined threshold value, the candidate corresponding point 51b is rejected as an inadequate point (a point not corresponding to the feature point 50a ).
      <br/>
      After the rejection by the block matching, a group of candidate corresponding points 90b from which the inadequate points have been removed is sent to the 3-D coordinates calculation section 55b in the 3-D coordinates calculation unit 55.
      <br/>
      At this point, the candidate corresponding points 90b have been screened by the block matching, and the number of the candidate corresponding points 90b have been considerably reduced due to the block matching.
      <br/>
      Meanwhile, the corresponding point determination section 9c determines a group of candidate corresponding points 90c in the same way as the corresponding point determination section 9b, and sends the group of candidate corresponding points 90c to the 3-D coordinates calculation section 55c in the 3-D coordinates calculation unit 55.
      <br/>
      Incidentally, although the above corresponding point determination unit 9 used the parameters 400 for calculating the epipolar line and obtaining the candidate corresponding points, it is also possible to obtain candidate corresponding points without using the parameters 400.
    </p>
    <p num="132">
      Lastly, the operation of the 3-D coordinates calculation unit 55 will be described.
      <br/>
      The 3-D coordinates calculation section 55b in the 3-D coordinates calculation unit 55 obtains almost all 3-D candidate coordinates 60b by the same operation as the 3-D coordinates calculation section 5b of the first embodiment.
      <br/>
      However, in the second embodiment, the candidate corresponding points 90b have been already obtained by the corresponding point determination section 9b, therefore the 3-D coordinates calculation section 55b does not calculate the candidate corresponding points Qm : (um, vm) (m=1, . . . ) and uses the candidate corresponding points 90b supplied by the corresponding point determination section 9b.
    </p>
    <p num="133">As described above, according to the second embodiment, as well as the same effects as those of the first embodiment, screening of corresponding points is preliminarily executed by the corresponding point determination unit 9 by means of the block matching, and thus accuracy of calculation and determination of the 3-D coordinates which correspond to the feature point 50a can be further improved, and calculation time for the 3-D coordinates (which was the major portion of calculation time of the subject image extraction device) can be considerably shortened.</p>
    <p num="134">As set forth hereinbefore by the subject image extraction device according to the present invention, an image of a desired subject can be extracted from an inputted image with high accuracy, without needing preparation of the information concerning the subject or the image pickup devices, without needing precise control or measurement of the positions and attitudes of the image pickup devices, and with easy setting of a threshold value for distinguishing between the background and the foreground.</p>
    <p num="135">
      The subject image extraction devices which have been described referring to FIG. 1 and FIG. 8 can be applied to reduction of the amount of data which is transmitted between video phone etc.
      <br/>
      In the case of video phone, a speaker talking near the video phone (i.e. in the designated 3-D space domain) can be extracted as the subject from an image, and the image of the speaker only can be encoded and transmitted, thereby the amount of data can be considerably reduced, performance of video phone can be improved, and privacy of speakers can be protected.
    </p>
    <p num="136">
      Incidentally, although the subject image extraction devices of FIG. 1 and FIG. 8 included the image pickup unit 1, subject image extraction can of course be executed by a subject image extraction devices which is not provided with the image pickup unit 1 as long as images of a 3-D object and images including a subject which have been picked up by three or more image pickup sections (devices) are supplied.
      <br/>
      Therefore, subject image extraction devices according to the present invention without image pickup units are also possible.
      <br/>
      Similarly, subject image extraction devices according to the present invention for subject image extraction only (i.e. not for displaying the extracted subject image) are also possible.
      <br/>
      Description of such subject image extraction devices is omitted for brevity.
    </p>
    <p num="137">
      While the present invention has been described with reference to the particular illustrative embodiments, it is not to be restricted by those embodiments but only by the appended claims.
      <br/>
      It is to be appreciated that those skilled in the art can change or modify the embodiments without departing from the scope and spirit of the present invention.
    </p>
  </description>
  <claims format="original" lang="en" id="claim_en">
    <claim num="1">
      <claim-text>What is claimed is:</claim-text>
      <claim-text>1.</claim-text>
      <claim-text>A subject image extraction device comprising:</claim-text>
      <claim-text>an image storage means including main image storage section for storing images which have been picked up by a main image pickup section for picking up a main image from which an image of a subject is extracted, and two or more subsidiary image storage sections for storing images which have been picked up by two or more subsidiary image pickup sections for picking up subsidiary images to be referred to in subject image extraction; a feature point extraction means including three or more feature point extraction sections for extracting feature point coordinates in the images which have been stored in each of the image storage sections; a parameter calculation means for calculating parameters which represent relative positions and attitudes of the main image pickup section and the subsidiary image pickup sections, using the feature point coordinates in the images of a 3-D object on which a predetermined pattern is drawn which have been picked up by the main image pickup section and the subsidiary image pickup sections; a 3-D coordinates calculation means including two or more 3-D coordinates calculation sections for calculating candidate 3-D coordinates which are considered to correspond to the feature point coordinates of a feature point in the main image, using the feature point coordinates in the main image, the feature point coordinates in the subsidiary images, and the parameters obtained by the parameter calculation means; a 3-D coordinates determination means for determining genuine 3-D coordinates which correspond to the feature point in the main image, using the candidate 3-D coordinates calculated by the 3-D coordinates calculation sections in the 3-D coordinates calculation means; a judgment means for judging whether or not the genuine 3-D coordinates corresponding to the feature point in the main image is in a designated 3-D space domain, and accepting the feature point if the genuine 3-D coordinates corresponding to the feature point is judged to be in the designated 3-D space domain; an outline point extraction means for extracting subject outline points from a plurality of feature points which have been accepted by the judgment means;</claim-text>
      <claim-text>and a subject extraction means for extracting part of the main image which is surrounded by the subject outline points as the subject image.</claim-text>
    </claim>
    <claim num="2">
      <claim-text>2. A subject image extraction device as claimed in claim 1, wherein the feature point extraction sections in the feature point extraction means extract edge-like parts in the images where sudden changes occur in color intensity or luminance as the feature point coordinates, in the extraction of the feature point coordinates from the main image or the subsidiary images.</claim-text>
    </claim>
    <claim num="3">
      <claim-text>3. A subject image extraction device as claimed in claim 1, wherein the parameter calculation means calculates eleven or more parameters per one image pickup section.</claim-text>
    </claim>
    <claim num="4">
      <claim-text>4. A subject image extraction device as claimed in claim 1, wherein the 3-D coordinates determination means determines the genuine 3-D coordinates by selecting a group of 3-D coordinates from each of the candidate 3-D coordinates calculated by each of the 3-D coordinates calculation sections so that the sum of distances between the 3-D coordinates included in the group will become the smallest and defining the genuine 3-D coordinates by the average of the 3-D coordinates included in the group.</claim-text>
    </claim>
    <claim num="5">
      <claim-text>5. A subject image extraction device as claimed in claim 1, further comprising an image pickup means including the main image pickup section and the two or more subsidiary image pickup sections.</claim-text>
    </claim>
    <claim num="6">
      <claim-text>6. A subject image extraction device as claimed in claim 1, further comprising a subject display means for displaying the part of the main image which has been extracted by the subject extraction means.</claim-text>
    </claim>
    <claim num="7">
      <claim-text>7. A subject image extraction device as claimed in claim 1, wherein the number of the image pickup sections is three.</claim-text>
    </claim>
    <claim num="8">
      <claim-text>8. A subject image extraction device comprising: an image storage means including main image storage section for storing images which have been picked up by a main image pickup section for picking up a main image from which an image of a subject is extracted, and two or more subsidiary image storage sections for storing images which have been picked up by two or more subsidiary image pickup sections for picking up subsidiary images to be referred to in subject image extraction; to a feature point neighborhood extraction means including three or more feature point neighborhood extraction sections for extracting feature point coordinates in the images which have been stored in each of the image storage sections and extracting color/luminance information of pixels in the neighborhood of the feature point coordinates; a parameter calculation means for calculating parameters which represent relative positions and attitudes of the main image pickup section and the subsidiary image pickup sections, using the feature point coordinates in the images of a 3-D object on which a predetermined pattern is drawn which have been picked up by the main image pickup section and the subsidiary image pickup sections; a corresponding point determination means including two or more corresponding point determination sections for selecting a group of candidate corresponding points in the subsidiary image which are considered to correspond to the feature point in the main image, from the feature points in the subsidiary images, using the feature point coordinates in the main image and the subsidiary image and the color/luminance information of pixels in the neighborhood of the feature point coordinates which have been extracted by the feature point neighborhood extraction means; a 3-D coordinates calculation means including two or more 3-D coordinates calculation sections for calculating candidate 3-D coordinates which are considered to correspond to a feature point in the main image, using the feature point coordinates in the main image, the group of candidate corresponding points, and the parameters obtained by the parameter calculation means; a 3-D coordinates determination means for determining genuine 3-D coordinates which correspond to the feature point in the main image, using the candidate 3-D coordinates calculated by the 3-D coordinates calculation sections in the 3-D coordinates calculation means; a judgment means for judging whether or not the genuine 3-D coordinates corresponding to the feature point in the main image is in a designated 3-D space domain, and accepting the feature point if the genuine 3-D coordinates corresponding to the feature point is judged to be in the designated 3-D space domain; an outline point extraction means for extracting subject outline points from a plurality of feature points which have been accepted by the judgment means;</claim-text>
      <claim-text>and a subject extraction means for extracting part of the main image which is surrounded by the subject outline points.</claim-text>
    </claim>
    <claim num="9">
      <claim-text>9. A subject image extraction device as claimed in claim 8, wherein the feature point neighborhood extraction sections in the feature point extraction means extract edge-like parts in the images where sudden changes occur in color intensity or luminance as the feature point coordinates, in the extraction of the feature point coordinates from the main image or the subsidiary images.</claim-text>
    </claim>
    <claim num="10">
      <claim-text>10. A subject image extraction device as claimed in claim 8, wherein the parameter calculation means calculates eleven or more parameters per one image pickup section.</claim-text>
    </claim>
    <claim num="11">
      <claim-text>11. A subject image extraction device as claimed in claim 8, wherein the 3-D coordinates determination means determines the genuine 3-D coordinates by selecting a group of 3-D coordinates from each of the candidate 3-D coordinates calculated by each of the 3-D coordinates calculation sections so that the sum of distances between the 3-D coordinates included in the group will become the smallest and defining the genuine 3-D coordinates by the average of the 3-D coordinates included in the group.</claim-text>
    </claim>
    <claim num="12">
      <claim-text>12. A subject image extraction device as claimed in claim 8, further comprising an image pickup means including the main image pickup section and the two or more subsidiary image pickup sections.</claim-text>
    </claim>
    <claim num="13">
      <claim-text>13. A subject image extraction device as claimed in claim 8, further comprising a subject display means for displaying the part of the main image which has been extracted by the subject extraction means.</claim-text>
    </claim>
    <claim num="14">
      <claim-text>14. A subject image extraction device as claimed in claim 8, wherein the number of the image pickup sections is three.</claim-text>
    </claim>
  </claims>
</questel-patent-document>