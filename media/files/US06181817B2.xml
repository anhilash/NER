<?xml version="1.0" encoding="UTF-8"?>
<questel-patent-document lang="en" date-produced="20180805" produced-by="Questel" schema-version="3.23" file="US06181817B2.xml">
  <bibliographic-data lang="en">
    <publication-reference publ-desc="Granted patent as second publication">
      <document-id>
        <country>US</country>
        <doc-number>06181817</doc-number>
        <kind>B2</kind>
        <date>20010130</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>US6181817</doc-number>
      </document-id>
    </publication-reference>
    <original-publication-kind>B2</original-publication-kind>
    <application-reference is-representative="YES" family-id="26746044" extended-family-id="42108502">
      <document-id>
        <country>US</country>
        <doc-number>09192991</doc-number>
        <kind>A</kind>
        <date>19981116</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>1998US-09192991</doc-number>
      </document-id>
      <document-id data-format="questel_Uid">
        <doc-number>43164972</doc-number>
      </document-id>
    </application-reference>
    <language-of-filing>en</language-of-filing>
    <language-of-publication>en</language-of-publication>
    <priority-claims>
      <priority-claim kind="national" sequence="1">
        <country>US</country>
        <doc-number>19299198</doc-number>
        <kind>A</kind>
        <date>19981116</date>
        <priority-active-indicator>Y</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="1">
        <doc-number>1998US-09192991</doc-number>
      </priority-claim>
      <priority-claim kind="national" sequence="2">
        <country>US</country>
        <doc-number>6582897</doc-number>
        <kind>P</kind>
        <date>19971117</date>
        <priority-active-indicator>Y</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="2">
        <doc-number>1997US-60065828</doc-number>
      </priority-claim>
    </priority-claims>
    <dates-of-public-availability>
      <publication-of-grant-date>
        <date>20010130</date>
      </publication-of-grant-date>
    </dates-of-public-availability>
    <classifications-ipcr>
      <classification-ipcr sequence="1">
        <text>G06F  17/30        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>06</class>
        <subclass>F</subclass>
        <main-group>17</main-group>
        <subgroup>30</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="2">
        <text>G06T   7/40        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>06</class>
        <subclass>T</subclass>
        <main-group>7</main-group>
        <subgroup>40</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
    </classifications-ipcr>
    <classification-national>
      <country>US</country>
      <main-classification>
        <text>382170000</text>
        <class>382</class>
        <subclass>170000</subclass>
      </main-classification>
      <further-classification sequence="1">
        <text>382305000</text>
        <class>382</class>
        <subclass>305000</subclass>
      </further-classification>
      <further-classification sequence="2">
        <text>707999006</text>
        <class>707</class>
        <subclass>999006</subclass>
      </further-classification>
      <further-classification sequence="3">
        <text>707999104</text>
        <class>707</class>
        <subclass>999104</subclass>
      </further-classification>
      <further-classification sequence="4">
        <text>707E17023</text>
        <class>707</class>
        <subclass>E17023</subclass>
      </further-classification>
    </classification-national>
    <patent-classifications>
      <patent-classification sequence="1">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>G06F-017/30256</classification-symbol>
        <section>G</section>
        <class>06</class>
        <subclass>F</subclass>
        <main-group>17</main-group>
        <subgroup>30256</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130823</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="2">
        <classification-scheme office="EP" scheme="CPC">
          <date>20170101</date>
        </classification-scheme>
        <classification-symbol>G06T-007/41</classification-symbol>
        <section>G</section>
        <class>06</class>
        <subclass>T</subclass>
        <main-group>7</main-group>
        <subgroup>41</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20170102</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="3">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>Y10S-707/99936</classification-symbol>
        <section>Y</section>
        <class>10</class>
        <subclass>S</subclass>
        <main-group>707</main-group>
        <subgroup>99936</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130518</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="4">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>Y10S-707/99945</classification-symbol>
        <section>Y</section>
        <class>10</class>
        <subclass>S</subclass>
        <main-group>707</main-group>
        <subgroup>99945</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130518</date>
        </action-date>
      </patent-classification>
    </patent-classifications>
    <number-of-claims>28</number-of-claims>
    <exemplary-claim>1</exemplary-claim>
    <figures>
      <number-of-drawing-sheets>3</number-of-drawing-sheets>
      <number-of-figures>3</number-of-figures>
      <image-key data-format="questel">US6181817</image-key>
    </figures>
    <invention-title format="original" lang="en" id="title_en">Method and system for comparing data objects using joint histograms</invention-title>
    <references-cited>
      <citation srep-phase="examiner">
        <patcit num="1">
          <text>LIU DONG-CHYUAN</text>
          <document-id>
            <country>US</country>
            <doc-number>5594807</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5594807</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="2">
          <text>JAIN RAMESH, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5893095</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5893095</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="3">
          <text>ARMAN FARSHID, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5521841</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5521841</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="4">
          <text>BOLLE RUDOLF M, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5546475</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5546475</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="5">
          <text>BARBER RONALD J, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5579471</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5579471</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="6">
          <text>ARMAN FARSHID, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5606655</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5606655</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <nplcit num="1">
          <text>Androutsos et al. "Directional detail histogram for content based image retrieval" 1997 13th International Confernce on Digital Signal Processing Proceedings, vol. 1, pp. 22-228, Jul. 1997.</text>
        </nplcit>
      </citation>
      <citation srep-phase="examiner">
        <nplcit num="2">
          <text>Androutsos et al. "Efficient image database filtering using colour vector techniques" IEEE 1997 Canadian Conference on Electrical and Computer Engineering, May 1997. Engineering Innovation: Voyage of Discovery, vol. 2, pp. 827-830.</text>
        </nplcit>
      </citation>
      <citation srep-phase="examiner">
        <nplcit num="3">
          <text>Greg Pass &amp; Ramin Zabin, Histogram Refinement for Content-Based Image Retrieval, IEEE, Dec. 1996, pp. 96-102.*</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="4">
          <text>Virginia E. Ogle, Michael Stonebraker, Chabot: Retrieval from a Relational Database of Images, Theme Feature, Sep., 1995, pp. 40-48.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="5">
          <text>Myron Flickner et al., Query by Image and Video Content: The QBIC System, Theme Feature, Sep., 1995, pp. 23-32.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="6">
          <text>R.W. Picard, T.P. Minka, Vision texture for annotation, Multimedia Systems (1995) 3:3-14.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="7">
          <text>Jing Huang et al. Image Indexing Using Color Correlograms, 1997 IEEE, pp. 762-768. *1</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="8">
          <text>Brian V. Funt and Graham D. Finlayson, Color Constant Color Indexing, 1995 IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 17, No. 5, May 1995.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="9">
          <text>Michael J. Swain, Color Indexing, International Journal of Computer Vision. 7:1, 11-32, 1991.</text>
        </nplcit>
      </citation>
    </references-cited>
    <related-documents>
      <related-publication>
        <document-id>
          <country>US</country>
          <doc-number>60/065,828</doc-number>
          <date>19971117</date>
        </document-id>
        <document-id>
          <country>US</country>
          <doc-number>60/065828</doc-number>
          <date>19971117</date>
        </document-id>
      </related-publication>
    </related-documents>
    <parties>
      <applicants>
        <applicant data-format="original" app-type="applicant" sequence="1">
          <addressbook lang="en">
            <orgname>Cornell Research Foundation, Inc.</orgname>
            <address>
              <address-1>Ithaca, NY, US</address-1>
              <city>Ithaca</city>
              <state>NY</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </applicant>
        <applicant data-format="questel" app-type="applicant" sequence="2">
          <addressbook lang="en">
            <orgname>CORNELL RESEARCH FOUNDATION</orgname>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </applicant>
      </applicants>
      <inventors>
        <inventor data-format="original" sequence="1">
          <addressbook lang="en">
            <name>Zabih, Ramin D.</name>
            <address>
              <address-1>Ithaca, NY, US</address-1>
              <city>Ithaca</city>
              <state>NY</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
        <inventor data-format="original" sequence="2">
          <addressbook lang="en">
            <name>Pass, Gregory S.</name>
            <address>
              <address-1>Rockville, MD, US</address-1>
              <city>Rockville</city>
              <state>MD</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
      </inventors>
      <agents>
        <agent sequence="1" rep-type="agent">
          <addressbook lang="en">
            <orgname>Perkins, Smith &amp; Cohen, LLP</orgname>
          </addressbook>
        </agent>
        <agent sequence="2" rep-type="agent">
          <addressbook lang="en">
            <name>Erlich, Jacob N.</name>
          </addressbook>
        </agent>
        <agent sequence="3" rep-type="agent">
          <addressbook lang="en">
            <name>Cohen, Jerry</name>
          </addressbook>
        </agent>
      </agents>
    </parties>
    <examiners>
      <primary-examiner>
        <name>Au, Amelia</name>
      </primary-examiner>
    </examiners>
    <lgst-data>
      <lgst-status>GRANTED</lgst-status>
    </lgst-data>
  </bibliographic-data>
  <abstract format="original" lang="en" id="abstr_en">
    <p id="P-EN-00001" num="00001">
      <br/>
      A joint histogram is used for content-based data object comparison.
      <br/>
      For a data object which can be described by a plurality of features and values, a set of features describing the data object is selected.
      <br/>
      A joint histogram is a k-dimensional vector, such that each entry in the joint histogram contains the number of data points in the data object that are described by a k-tuple of feature values.
      <br/>
      The data object is analyzed according to the set of features and the results entered into the joint histogram.
      <br/>
      The joint histogram distinguishes the data object from other data objects.
      <br/>
      Joint histogram comparisons may be used to identify the similarity between two or more particular data objects.
    </p>
  </abstract>
  <description format="original" lang="en" id="desc_en">
    <heading>CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
    <p num="1">
      This application claims priority of U.S. provisional application Ser.
      <br/>
      No. 60/065,828 filed Nov. 17, 1997 and titled, "Histogram Refinement for Content-Based Image Retrieval" by the present inventors.
    </p>
    <heading>FIELD OF THE INVENTION</heading>
    <p num="2">This invention relates generally to information management using histograms and more particularly to image management on a computer system using histograms.</p>
    <heading>BACKGROUND OF THE INVENTION</heading>
    <p num="3">
      Many applications require methods for comparing data objects based on their content.
      <br/>
      Comparing images is useful, for example, in scene break detection, in parsing in video, and in image retrieval.
      <br/>
      One method of indexing images for later comparison is manual-tagging of images with human-selected keywords.
      <br/>
      This method, however, is neither flexible enough to satisfy the growing community of imagery users, nor fast enough to compete with the rate of information gathering.
      <br/>
      Fully automated content-based solutions are preferable.
    </p>
    <p num="4">Content-based image retrieval, in which a user presents an image and the retrieval system returns the most similar images, illustrates the demand for automatically comparing objects based on their content.</p>
    <p num="5">
      Most image retrieval systems operate in two distinct phases: an image summary phase, followed by a summary comparison phase.
      <br/>
      In the image summary phase, every image in the database is summarized as a vector, utilizing a particular method, such as color histogramming.
      <br/>
      The vectors are computed once and stored for later retrieval.
      <br/>
      In the summary comparison phase, a user presents a query image, and a comparison measure is used to compare the query image's summary with other image summaries and retrieve some number of the most similar vectors (images).
    </p>
    <p num="6">
      Color histogramming is the most widely used image summary method, employed in systems such as IBM's QBIC and Virage's VIR Engine.
      <br/>
      A color histogram is a vector where each entry stores the number of pixels of a given color in the image.
      <br/>
      All images are scaled to contain the same number of pixels before histogramming, and the colors of the image are mapped into a discrete colorspace containing n colors.
      <br/>
      Typically, images are represented in the RGB colorspace, using a few of the most significant bits per color channel to discretize the space.
    </p>
    <p num="7">
      Color histograms are widely used for content-based image retrieval because they are simple and quick to compute, and despite their simplicity, exhibit attractive properties.
      <br/>
      Color histograms are tolerant of movement of objects in the image and of changes in camera viewpoint, and are robust against occlusion.
      <br/>
      This invariance is due to the fact that color histograms do not relate spatial information with the pixels of a given color.
    </p>
    <p num="8">
      Color histograms have proven effective for small databases, e.g. 60-1140 images, but limitations become rapidly apparent with larger databases, e.g. tens of thousands of images.
      <br/>
      Because a color histogram records only color information, images with similar color histograms can have dramatically different appearances.
      <br/>
      For example, an image of a man in a red golf shirt may have a similar color histogram to an image of red flowers.
      <br/>
      In a large database, it is common for unrelated images to have similar color histograms.
    </p>
    <p num="9">
      There have been some attempts to improve color histograms by incorporating spatial information.
      <br/>
      Among the methods has been an attempt to capture the spatial arrangement of the different colors in the image.
      <br/>
      The image is partitioned into rectangular regions using maximum entropy, where each is region is predominantly a single color.
      <br/>
      Maximum entropy is the state at which all values in the distribution occur with equal probability.
      <br/>
      The similarity between two images is the degree of overlap between regions of the same color.
      <br/>
      While this method gives better results than color histograms without spatial information, it requires substantial computation, particularly in the partitioning algorithm.
      <br/>
      Additionally, the partitioning algorithm is affected by changes in orientation and position of the objects in the image.
    </p>
    <p num="10">
      Another histogramming method divides the image into five partially overlapping regions and computes the first three moments of the color distributions in each image.
      <br/>
      The moments of a distribution are the sums of the integer powers of the values.
      <br/>
      The mean of a distribution is derived from the first moment (1st power), the variance and standard deviation from second moment (2nd power), etc.
      <br/>
      The moments are computed for each color channel in the HSV colorspace, where pixels close to the border of the image have less weight.
      <br/>
      The distance between two regions is a weighted sum of the differences in each of the three moments.
      <br/>
      The distance between two images is the sum of the distance between the center regions, plus (for each of the four side regions) the minimum distance of that region to the corresponding region in the other image, when rotated by 0, 90, 180 or 270 degrees.
      <br/>
      Because the regions overlap, this method is insensitive to small rotations and translations.
    </p>
    <p num="11">
      Another histogramming method captures the spatial correlation between colors.
      <br/>
      This approach is called color correlograms, and is related to the correlogram technique from spatial data analysis.
      <br/>
      A color correlogram for a given pair of colors (i,j) and a distance k contains the probability that a pixel with color i will be k pixels away from a pixel of color j. To reduce the storage requirements, they concentrate on autocorrelograms, where i=j.
    </p>
    <p num="12">
      Another histogramming method uses color histograms to recognize individual objects contained in an image by comparing the histogram of a query object with the histograms of the images in the database.
      <br/>
      For the best matches, a histogram backprojection is performed to segment the objects from their backgrounds.
      <br/>
      The histogram comparison using this method can be upset by a pixel in the background in two ways: (1) the pixel has the same color as one of the colors in the query object. (2) the number of pixels of that color in the object is less than the number of pixels of that color in the query object.
    </p>
    <p num="13">
      In sum, it remains desirable to have an efficient image retrieval system which allows for significant differences in the appearance of similar images, such as: rotation and translation of objects in the image; addition, occlusion and subtraction of objects in the image; and changes in camera viewpoint and magnification.
      <br/>
      It is also important that the image summary phase be efficient in order to handle large imagery collections.
    </p>
    <p num="14">It is an object of the present invention to provide a method and apparatus to perform efficient data object comparisons based on data object content.</p>
    <p num="15">It is an object of the present invention to provide a method and apparatus to perform efficient image comparisons based on image content.</p>
    <p num="16">It is another object of the present invention to provide a method and apparatus to perform image retrieval which allows for significant differences in the appearance of images having similar content.</p>
    <p num="17">It is another object of the present invention to provide a method and apparatus to perform image object retrieval, in which similar objects within images may be identified allowing for significant differences in the appearance of images having similar content.</p>
    <heading>SUMMARY OF THE INVENTION</heading>
    <p num="18">The problems of efficiently and accurately summarizing and comparing the content of data objects are solved by the present invention of comparing data objects using joint histograms.</p>
    <p num="19">
      The joint histogram, which is a multidimensional histogram, is created from a set of data object features.
      <br/>
      For example, in a joint histogram of a visual image, analyzed pixel by pixel, each entry in the joint histogram would contain the number of pixels in the image that are described by a particular combination of feature values.
      <br/>
      In a joint histogram that combines color information with intensity gradient, the joint histogram will contain ncolor * ngradient entries.
      <br/>
      Each pixel in the image has a color value and an intensity gradient value.
      <br/>
      The value stored in each entry in the joint histogram is the number of pixels in the image with a particular color value and a particular intensity gradient value.
    </p>
    <p num="20">
      Generally, given a set of k features, where the l'th feature has n1 possible values, a joint histogram may be constructed.
      <br/>
      A joint histogram is a k-dimensional vector, such that each entry in the joint histogram contains the number of instances in a data object that are described by a k-tuple of feature values.
      <br/>
      The size of the joint histogram is n= PI l-1k nl, the number of possible combinations of the values of each feature.
      <br/>
      Just as a color histogram approximates the density of pixel color, a joint histogram approximates the joint density of several data object features.
      <br/>
      Joint histograms can be compared using the same methods as one dimensional vectors.
    </p>
    <p num="21">
      In image retrieval, joint histograms reduce the probability that distinct components of the content within an image and between images are considered to be the same.
      <br/>
      For example, a particularly-constructed joint histograms could reduce the probability that a pixel of a particular color in an object in an image will be incorrectly matched against a pixel of that same color in the background of the image.
      <br/>
      Different similarly-colored regions of the image will tend to have additional features which are distinct, and the joint histogram can summarize these differences.
      <br/>
      The choice of features to incorporate in the joint histogram will determine the range of transformations (scaling, zooming, etc.) that can be applied to the image without significantly changing the summary.
      <br/>
      The ability to separate similarly-colored components of content in an image is an advantage of joint histograms.
      <br/>
      Color histograms do not have this property.
      <br/>
      Replacing color histograms with joint histograms therefore improves the results of histogram-based image retrieval.
    </p>
    <p num="22">
      In general, joint histograms may be used to distinguish large numbers of data objects from each other, and identify those data objects which have similar content.
      <br/>
      Color is only one property from among many properties that may be used in a joint histogram.
      <br/>
      Similar data objects will tend to have similar joint histograms, and different data objects will tend to have distinct joint histograms.
      <br/>
      Thus the similarity between two data objects can be determined by comparing their joint histograms.
    </p>
    <p num="23">
      In addition, a subset of one particular data object may be compared with other data objects to identify the presence of that subset in another data object.
      <br/>
      In this case, only the joint histogram's entries which characterize the subset of the data object are used to compare two data objects.
      <br/>
      Subset comparison is therefore a natural complement of data object comparison.
    </p>
    <p num="24">In addition to the joint histogram itself, a technique called reduced intersection data provides that only a few entries, or a few pixels, per data object suffices to provide a recognizable cue for that object.</p>
    <p num="25">The present invention together with the above and other advantages may best be understood from the following detailed description of the embodiments of the invention illustrated in the drawings, wherein:</p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading>
    <p num="26">
      FIG. 1 is a flow chart of developing a joint histogram according to principles of the invention;
      <br/>
      FIG. 2 is a flow chart of the data object retrieval process according to principles of the present invention; and,
      <br/>
      FIG. 3 is a block diagram of the data object retrieval system of FIG. 1 and FIG. 2 according to principles of the present invention.
    </p>
    <heading>DETAILED DESCRIPTION OF PREFERRED EMBODIMENTS</heading>
    <p num="27">
      FIG. 1 is a flow chart of the method of developing a joint histogram for the summary of a data object.
      <br/>
      In a first embodiment of the invention, the retrieval of images from a database is used, but the present invention may be practiced in any domain which can be described by features.
    </p>
    <p num="28">
      Generally speaking, given a set of k features, where the l'th feature has nl possible values, a joint histogram may be constructed describing a data object.
      <br/>
      A joint histogram is a k-dimensional vector, such that each entry in the joint histogram contains the number of data points in the data object that are described by a k-tuple of feature values.
      <br/>
      The size of the joint histogram is therefore n= PI l-1k nl, the number of possible combinations of the values of each feature.
      <br/>
      Just as a color histogram approximates the density of pixel color, a joint histogram approximates the joint density of several data object features.
    </p>
    <p num="29">
      Referring now to FIG. 1, a joint histogram is constructed by first identifying and selecting features of the data object (or objects) at hand, block 10.
      <br/>
      In the first embodiment of the invention, the data object is an image having pixels, and a set of pixel features is selected.
      <br/>
      The features are selected according to the properties of data they describe as it affects the retrieval process.
      <br/>
      In image retrieval, it is optimal to select features and components of the image that do not change when certain transformations are applied to objects in the image.
      <br/>
      These transformations may include translation, rotation and scaling.
      <br/>
      Also, the selected features must describe the same components of the image in order to construct a successful joint histogram.
      <br/>
      For example, a single set of features must describe either single-pixel components or multiple-pixel components, but not both.
    </p>
    <p num="30">
      A number of features may be used to create the joint histogram for image retrieval.
      <br/>
      The following features are merely exemplary, and the present invention is not limited to these features.
    </p>
    <p num="31">
      A first feature which may be used in the joint histogram is color.
      <br/>
      In the present embodiment, the standard RGB colorspace is used, however, in alternative embodiments, other colorspaces may be used.
    </p>
    <p num="32">
      A second feature which may be used is edge density which is defined at pixel (j, k) to be the ratio of edges to pixels in a small neighborhood surrounding the pixel.
      <br/>
      The edge representation of the image may be computed with a standard method.
    </p>
    <p num="33">A third feature is texturedness which is defined at pixel (j, k) to be the number of neighboring pixels whose intensities differ by more than a fixed value.</p>
    <p num="34">
      A fourth feature is gradient magnitude which is a measure of how rapidly intensity is changing in the direction of greatest change.
      <br/>
      The gradient magnitude at a pixel (j, k) may be computed using standard methods.
    </p>
    <p num="35">A fifth feature is rank which is defined with respect to pixel (j, k) as being the number of pixels in the local neighborhood whose intensity is less than the intensity at (j, k).</p>
    <p num="36">
      An arbitrary feature can have a large (possibly infinite) range of possible values.
      <br/>
      To reduce the number of possible values to store in the joint histogram, the range is discretized, or partitioned, block 20.
      <br/>
      For example, the RGB colorspace, which has 2563 values, may be partitioned into 64 quantized colors.
      <br/>
      The range of values may be partitioned so that each discrete value appears with approximately equal likelihood.
      <br/>
      A substantially uniform discretization maximizes the information conveyed by a feature.
    </p>
    <p num="37">
      A feature may be discretized by approximating its cumulative distribution over a subset of data objects, and dividing the distribution into partitions with equal probability.
      <br/>
      The approximation is generated using a population of data objects.
      <br/>
      Each partition of the cumulative distribution is indicative of the range of continuous values which will be treated as a single discrete value.
    </p>
    <p num="38">After a set of features is selected and the range of possible values is determined, a joint histogram template is created, block 30, which has nfeature1 * nfeature2 * nfeature(n) entries to receive the data analyzed from the data objects.</p>
    <p num="39">
      Next, a summary computation is performed on each data object, i.e. each data object is analyzed for the selected features, block 40.
      <br/>
      Summary computation need be performed only once for each object if the summaries are stored, for example, in a database.
    </p>
    <p num="40">Finally, the values of each feature of the analyzed object are entered in combination into a joint histogram which is stored for future comparison, block 50.</p>
    <p num="41">
      In the first embodiment of the invention, each entry in the joint histogram contains the number of pixels in the image that are described by a particular combination of feature values.
      <br/>
      For example, if the selected set of features is composed of color information and intensity gradient, the joint histogram would be as follows.
      <br/>
      A given pixel in the image has a color value (in a discretized range 0 . . . ncolor -1) and an intensity gradient value (in a discretized range 0 . . . ngradient -1).
      <br/>
      The joint histogram for color and intensity gradient will contain ncolor * ngradient entries.
      <br/>
      The value stored in each entry in the joint histogram is the number of pixels in the image with a particular color value and a particular intensity gradient value.
    </p>
    <p num="42">
      FIG. 2 shows the data object comparison process according to principles of the present invention.
      <br/>
      First a query is developed, that is, a summary computation is performed on the data object to be matched, forming a query vector, block 100.
    </p>
    <p num="43">
      Next, the joint histogram of the data object to be matched is compared to other joint histograms, block 110.
      <br/>
      Joint histograms are vectors which may be compared using a variety of known methods of vector comparison.
      <br/>
      One such method is comparing using the L1 distance, which is the sum of absolute differences between the vectors being compared.
      <br/>
      That is, given two vectors, for every entry in the vector, the values in the corresponding entries are subtracted, the absolute value of each difference is taken, and all the absolute values are added.
    </p>
    <p num="44">To retrieve an image, the results of all the vector comparisons are tracked and the results are sorted to find a vector (or vectors) closest to the query vector, block 120.</p>
    <p num="45">In order to make comparing multiple vectors more efficient, the joint histograms may be indexed according to an appropriate schema.</p>
    <p num="46">
      Also, to making comparing more efficient, a reduced intersection method may be used.
      <br/>
      In the reduced intersection method, only the c largest entries in each histogram are compared.
      <br/>
      Comparing only a small number of entries can produce results which closely approximate retrieval using all n entries in the histogram.
      <br/>
      Generally, the largest entries in a histogram capture the most distinctive features of the data object.
      <br/>
      Additionally, the smaller entries in the histogram are likely to be noise, i.e. inaccurate feature analyses.
      <br/>
      Hence, reducing the number of entries compared by eliminating the smaller entries can improve the comparison results.
      <br/>
      The time required for comparison of histograms without using the reduced intersection method is O(mn), where m is the number of vectors to be compared and n is the number of entries in the histogram.
      <br/>
      In reduced intersection, if the histogram entries have been sorted and stored prior to the comparison, the time required for comparison with a fixed scope is O(mc), where m is the number of vectors to be compared and c is the number of histogram entries used for comparing.
      <br/>
      For comparing many histograms, using the reduced intersection method is considerably faster when c&lt;n.
    </p>
    <p num="47">
      FIG. 3 shows a data object retrieval system 200 which executes the process described above.
      <br/>
      A database 210 stores a plurality of joint histograms 220 which were entered using the object analyzer 230.
      <br/>
      Each joint histogram in the plurality of joint histograms was analyzed according to the selected set of features and discretized value ranges 240.
      <br/>
      When a query 250 is input to the data object retrieval system 200, the object analyzer 230 forms a query vector and performs a comparison with the contents of the database.
      <br/>
      The system 200 then generations matches from the database to the query.
    </p>
    <p num="48">
      In image retrieval, joint histograms reduce the probability that distinct components of the content within an image and between images will be considered to be the same.
      <br/>
      For example, a particularly-constructed joint histogram could reduce the probability that a pixel of a particular color in an object in an image will be incorrectly matched against a pixel of that same color in the background of the image.
      <br/>
      Different similarly-colored regions of the image will tend to have additional features which are distinct, and the joint histogram can summarize these differences.
      <br/>
      The choice of features to incorporate in the joint histogram will determine the range of transformations (scaling, zooming, etc.) that can be applied to the image without signficantly changing the summary.
      <br/>
      The ability to separate similarly-colored components of content in an image is an advantage of joint histograms.
      <br/>
      Color histograms do not have this property.
      <br/>
      Replacing color histograms with joint histograms therefore improves the results of histogram-based image retrieval.
    </p>
    <p num="49">
      In general, joint histograms may be used to distinguish large numbers of data objects from each other, and identify those data objects which have similar content.
      <br/>
      Color is only property from among many properties that may be used in a joint histogram.
      <br/>
      Similar data objects will tend to have similar joint histograms, and different data objects will tend to have distinct joint histograms.
      <br/>
      Thus the similarity between two data objects can be determined by comparing their joint histograms.
    </p>
    <p num="50">
      In addition, a subset of one particular data object may be compared with other data objects to identify the presence of that subset in another data object.
      <br/>
      In this case, only the joint histogram's entries which characterize the subset of the data object are used to compare the two data objects.
      <br/>
      Subset comparison is therefore a natural complement of data object comparison.
    </p>
    <p num="51">
      While the size of a joint histogram is significantly larger than a color histogram, generally most of the entries in a joint histogram are zero so storage requirements are not, in fact, much greater for joint histograms than they are for color histograms.
      <br/>
      Standard compression schemes, such as run-length encoding, can be applied to good effect.
      <br/>
      In addition, dimensionality-reducing techniques, such as Principle Component Analysis (PCA), naturally offer an improved representation of the joint histogram.
    </p>
    <p num="52">Also, while the number of entries in a joint histogram increases substantially with additional features, the actual number of nonzero entries that must be stored does not increase at that same rate and remains practical compared to color histograms.</p>
    <p num="53">
      Because both color histograms and joint histograms use the same measures for comparison, the efficiency of comparison is the same for both methods.
      <br/>
      The speed of comparison is determined by the number of histogram entries to be compared.
    </p>
    <p num="54">
      While an image retrieval process was used to describe an embodiment of the present invention, the invention may be used with any type of data object which may be described using features.
      <br/>
      Other applications include music data, representations of molecules, and astronomical charts.
    </p>
    <p num="55">
      It is to be understood that the above-described embodiments are simply illustrative of the principles of the invention.
      <br/>
      Various and other modifications and changes may be made by those skilled in the art which will embody the principles of the invention and fall within the spirit and scope thereof.
    </p>
  </description>
  <claims format="original" lang="en" id="claim_en">
    <claim num="1">
      <claim-text>What is claimed is:</claim-text>
      <claim-text>1.</claim-text>
      <claim-text>A computer-implemented method for comparing data objects where those data objects may be described by features, comprising the steps of:</claim-text>
      <claim-text>selecting a set of features describing a data object, each feature having a range of values; creating a joint histogram template based on the set of features, the joint histogram template having entries for combinations of features and values; analyzing the data object according to the set of features to obtain joint histogram values;</claim-text>
      <claim-text>and, entering the joint histogram values into the joint histogram template to create a joint histogram describing the data object, whereby the data object is identifiable by the joint histogram from other data objects.</claim-text>
    </claim>
    <claim num="2">
      <claim-text>2. The method of claim 1 wherein said selecting step further comprises selecting features according to requirements of a selected data comparison process.</claim-text>
    </claim>
    <claim num="3">
      <claim-text>3. The method of claim 1 further comprising the step of: discretizing the range of feature values to reduce the number of value entries to be made in the joint histogram.</claim-text>
    </claim>
    <claim num="4">
      <claim-text>4. The method of claim 3 wherein each range is discretized such that each discrete value occurs with approximately equal likelihood over a representative set of data objects.</claim-text>
    </claim>
    <claim num="5">
      <claim-text>5. The method of claim 1 further comprising the step of: storing the joint histogram in a database.</claim-text>
    </claim>
    <claim num="6">
      <claim-text>6. The method of claim 5 further comprising the step of: indexing the database.</claim-text>
    </claim>
    <claim num="7">
      <claim-text>7. The method of claim 1 further comprising the steps of: determining a query vector of a query object; comparing the query vector with the joint histogram of the data object to determine a similarity between the data object and the query object.</claim-text>
    </claim>
    <claim num="8">
      <claim-text>8. The method of claim 7 further comprising tracking results of the comparing step to produce a plurality of joint histograms matching the query vector.</claim-text>
    </claim>
    <claim num="9">
      <claim-text>9. The method of claim 7 wherein the comparing step uses a reduced intersection of histogram entries for vector comparison.</claim-text>
    </claim>
    <claim num="10">
      <claim-text>10. A computer-implemented method for comparing images where those images may be described by features, comprising the steps of: selecting a set of features describing an image, each feature having a range of values; creating a joint histogram template based on the set of features, the joint histogram template having entries for combinations of features and value; analyzing the image according to the set of features to obtain joint histogram values;</claim-text>
      <claim-text>and, entering the obtained joint histogram values into the joint histogram template to create a joint histogram describing the image, whereby the image is identifiable by the joint histogram from other images.</claim-text>
    </claim>
    <claim num="11">
      <claim-text>11. The method of claim 10 wherein the selecting step further comprises selecting features which remain constant when transformations are applied to objects in the image.</claim-text>
    </claim>
    <claim num="12">
      <claim-text>12. The method of claim 10 further comprising the step of: discretizing the range of feature values to reduce the number of value entries to be made in the joint histogram.</claim-text>
    </claim>
    <claim num="13">
      <claim-text>13. The method of claim 12 wherein each range of values is discretized such that each discrete value occurs with approximately equal likelihood over a representative set of images.</claim-text>
    </claim>
    <claim num="14">
      <claim-text>14. The method of claim 10 further comprising the step of: storing the joint histogram in an indexed database.</claim-text>
    </claim>
    <claim num="15">
      <claim-text>15. The method of claim 10 further comprising the steps of: determining a query vector of a query image; comparing the query vector with the joint histogram of the image to determine the similarity between the image and the query image.</claim-text>
    </claim>
    <claim num="16">
      <claim-text>16. The method of claim 15 further comprising the step of: tracking results of the comparing step to produce a plurality of joint histograms matching the query vector.</claim-text>
    </claim>
    <claim num="17">
      <claim-text>17. The method of claim 15 wherein the comparing step uses a reduced intersection of histogram entries for vector comparison.</claim-text>
    </claim>
    <claim num="18">
      <claim-text>18. A computer-implemented method of image retrieval from a database, comprising the steps of: selecting a set of features describing an image; creating a joint histogram template based on the set of features, the joint histogram template having entries for combinations of features and values; analyzing the image according to the set of features to obtain histogram values; entering the obtained histogram values into the joint histogram template to create a joint histogram describing the image;</claim-text>
      <claim-text>and storing the joint histogram in the database, whereby the image is identifiable by the joint histogram from other images in the database.</claim-text>
    </claim>
    <claim num="19">
      <claim-text>19. The method of claim 18 wherein the selecting step further comprises selecting features which remain constant when transformations are applied to objects in the image.</claim-text>
    </claim>
    <claim num="20">
      <claim-text>20. The method of claim 18 further comprising the step of: discretizing the range of feature values to reduce the number of value entries to be made in the joint histogram.</claim-text>
    </claim>
    <claim num="21">
      <claim-text>21. The method of claim 20 wherein each range of values is discretized such that each discrete value occurs with approximately equal likelihood over a representative set of data objects.</claim-text>
    </claim>
    <claim num="22">
      <claim-text>22. The method of claim 18 further comprising the step of: indexing the database.</claim-text>
    </claim>
    <claim num="23">
      <claim-text>23. The method of claim 18 further comprising the steps of: determining a query vector based on an image to be retrieved from the database; comparing the query vector with histograms stored in the database to find a histogram matching the query vector.</claim-text>
    </claim>
    <claim num="24">
      <claim-text>24. The method of claim 23 further comprising the step of: tracking results of the comparing step to produce a plurality of joint histograms matching the query vector.</claim-text>
    </claim>
    <claim num="25">
      <claim-text>25. The method of claim 23 wherein the comparing step uses a reduced intersection of histogram entries for vector comparison.</claim-text>
    </claim>
    <claim num="26">
      <claim-text>26. A system for data object retrieval from a database, comprising: means for selecting a set of features describing a data object; means for creating a joint histogram template based on the set of features, the joint histogram template having entries for combinations of features and values; an object analyzer for analyzing the data object according to the selected set of features, the object analyzer producing histogram values, the object analyzer creating a joint histogram describing the data object by entering the values into the joint histogram template;</claim-text>
      <claim-text>and, means for storing the joint histogram in the database.</claim-text>
    </claim>
    <claim num="27">
      <claim-text>27. The system of claim 26 further comprising: a means for indexing the database.</claim-text>
    </claim>
    <claim num="28">
      <claim-text>28. The system of claim 26 further comprising: means for determining a query vector to retrieve the data object from the database;</claim-text>
      <claim-text>and means for comparing the query vector with histograms stored in the database to find the histogram matching the query vector.</claim-text>
    </claim>
  </claims>
</questel-patent-document>