<?xml version="1.0" encoding="UTF-8"?>
<questel-patent-document lang="en" date-produced="20180805" produced-by="Questel" schema-version="3.23" file="US06185335B2.xml">
  <bibliographic-data lang="en">
    <publication-reference publ-desc="Granted patent as second publication">
      <document-id>
        <country>US</country>
        <doc-number>06185335</doc-number>
        <kind>B2</kind>
        <date>20010206</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>US6185335</doc-number>
      </document-id>
    </publication-reference>
    <original-publication-kind>B2</original-publication-kind>
    <application-reference family-id="22336325" extended-family-id="1186834">
      <document-id>
        <country>US</country>
        <doc-number>09111047</doc-number>
        <kind>A</kind>
        <date>19980707</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>1998US-09111047</doc-number>
      </document-id>
      <document-id data-format="questel_Uid">
        <doc-number>1228063</doc-number>
      </document-id>
    </application-reference>
    <language-of-filing>en</language-of-filing>
    <language-of-publication>en</language-of-publication>
    <priority-claims>
      <priority-claim kind="national" sequence="1">
        <country>US</country>
        <doc-number>11104798</doc-number>
        <kind>A</kind>
        <date>19980707</date>
        <priority-active-indicator>Y</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="1">
        <doc-number>1998US-09111047</doc-number>
      </priority-claim>
    </priority-claims>
    <dates-of-public-availability>
      <publication-of-grant-date>
        <date>20010206</date>
      </publication-of-grant-date>
    </dates-of-public-availability>
    <classifications-ipcr>
      <classification-ipcr sequence="1">
        <text>B41J   2/52        20060101AFI20051220RMJP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>B</section>
        <class>41</class>
        <subclass>J</subclass>
        <main-group>2</main-group>
        <subgroup>52</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>JP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051220</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="2">
        <text>G06T   7/00        20060101ALI20051220RMJP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>06</class>
        <subclass>T</subclass>
        <main-group>7</main-group>
        <subgroup>00</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>JP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051220</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="3">
        <text>G06T   7/60        20060101ALI20051220RMJP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>06</class>
        <subclass>T</subclass>
        <main-group>7</main-group>
        <subgroup>60</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>JP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051220</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="4">
        <text>H04N   1/40        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>1</main-group>
        <subgroup>40</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="5">
        <text>H04N   1/405       20060101ALI20051220RMJP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>1</main-group>
        <subgroup>405</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>JP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051220</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="6">
        <text>H04N   1/52        20060101ALI20051220RMJP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>1</main-group>
        <subgroup>52</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>JP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051220</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="7">
        <text>H04N   1/60        20060101ALI20051220RMJP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>1</main-group>
        <subgroup>60</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>JP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051220</date>
        </action-date>
      </classification-ipcr>
    </classifications-ipcr>
    <classification-national>
      <country>US</country>
      <main-classification>
        <text>382224000</text>
        <class>382</class>
        <subclass>224000</subclass>
      </main-classification>
      <further-classification sequence="1">
        <text>358001900</text>
        <class>358</class>
        <subclass>001900</subclass>
      </further-classification>
      <further-classification sequence="2">
        <text>358534000</text>
        <class>358</class>
        <subclass>534000</subclass>
      </further-classification>
      <further-classification sequence="3">
        <text>382168000</text>
        <class>382</class>
        <subclass>168000</subclass>
      </further-classification>
      <further-classification sequence="4">
        <text>382172000</text>
        <class>382</class>
        <subclass>172000</subclass>
      </further-classification>
    </classification-national>
    <classifications-ecla>
      <classification-ecla sequence="1">
        <text>H04N-001/40L</text>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>001</main-group>
        <subgroup>40L</subgroup>
      </classification-ecla>
    </classifications-ecla>
    <patent-classifications>
      <patent-classification sequence="1">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>H04N-001/40062</classification-symbol>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>1</main-group>
        <subgroup>40062</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
    </patent-classifications>
    <number-of-claims>32</number-of-claims>
    <exemplary-claim>1</exemplary-claim>
    <figures>
      <number-of-drawing-sheets>5</number-of-drawing-sheets>
      <number-of-figures>5</number-of-figures>
      <image-key data-format="questel">US6185335</image-key>
    </figures>
    <invention-title format="original" lang="en" id="title_en">Method and apparatus for image classification and halftone detection</invention-title>
    <references-cited>
      <citation srep-phase="applicant">
        <patcit num="1">
          <text>STOFFEL JAMES C</text>
          <document-id>
            <country>US</country>
            <doc-number>4194221</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4194221</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="2">
          <text>HSIEH ROBERT C</text>
          <document-id>
            <country>US</country>
            <doc-number>4403257</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4403257</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="3">
          <text>FOX SIDNEY J, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4554593</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4554593</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="4">
          <text>KANNAPELL HENRY N, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4577235</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4577235</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="5">
          <text>IBARAKI HISASHI, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4722008</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4722008</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="6">
          <text>MURAKAMI TATSUYA, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4893188</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4893188</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="7">
          <text>NANNICHI TOSHIHIKO</text>
          <document-id>
            <country>US</country>
            <doc-number>5016118</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5016118</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="8">
          <text>IKEDA YOSHINORI, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5119185</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5119185</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="9">
          <text>KASANO AKIRA</text>
          <document-id>
            <country>US</country>
            <doc-number>5142593</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5142593</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="10">
          <text>KOWALSKI ROBERT P, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5193122</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5193122</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="11">
          <text>MURAKAMI TATSUYA, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5268771</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5268771</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="12">
          <text>YAMADA KIYOSHI</text>
          <document-id>
            <country>US</country>
            <doc-number>5271095</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5271095</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="13">
          <text>SEMASA TAKAYOSHI</text>
          <document-id>
            <country>US</country>
            <doc-number>5291309</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5291309</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="14">
          <text>SHIAU JENG-NAN, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5293430</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5293430</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="15">
          <text>KOIZUMI NOBORU</text>
          <document-id>
            <country>US</country>
            <doc-number>5317419</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5317419</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="16">
          <text>ROBINSON DAVID C</text>
          <document-id>
            <country>US</country>
            <doc-number>5339172</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5339172</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="17">
          <text>SHIAU JENG-NAN</text>
          <document-id>
            <country>US</country>
            <doc-number>5341226</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5341226</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="18">
          <text>SEMASA TAKAYOSHI</text>
          <document-id>
            <country>US</country>
            <doc-number>5361142</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5361142</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="19">
          <text>FUJISAWA TETSUO, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5410619</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5410619</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="20">
          <text>GOVRIN OMRI</text>
          <document-id>
            <country>US</country>
            <doc-number>5412737</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5412737</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="21">
          <text>ZUNIGA OSCAR A</text>
          <document-id>
            <country>US</country>
            <doc-number>5546474</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5546474</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="22">
          <text>RICOH KK</text>
          <document-id>
            <country>EP</country>
            <doc-number>0291000</doc-number>
            <kind>A2</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>EP-291000</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="23">
          <document-id>
            <country>EP</country>
            <doc-number>0195925</doc-number>
            <kind>B1</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>EP-195925</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="24">
          <document-id>
            <country>EP</country>
            <doc-number>0187724</doc-number>
            <kind>B1</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>EP-187724</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="25">
          <text>XEROX CORP</text>
          <document-id>
            <country>EP</country>
            <doc-number>0521662</doc-number>
            <kind>A1</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>EP-521662</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="26">
          <text>XEROX CORP</text>
          <document-id>
            <country>EP</country>
            <doc-number>0621725</doc-number>
            <kind>A1</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>EP-621725</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="27">
          <text>YOZAN INC, et al</text>
          <document-id>
            <country>EP</country>
            <doc-number>0527488</doc-number>
            <kind>B1</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>EP-527488</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="28">
          <text>MATSUSHITA ELECTRIC IND CO LTD</text>
          <document-id>
            <country>EP</country>
            <doc-number>0596412</doc-number>
            <kind>B1</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>EP-596412</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="29">
          <text>KONISHIROKU PHOTO IND</text>
          <document-id>
            <country>GB</country>
            <doc-number>2127647</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>GB2127647</doc-number>
          </document-id>
        </patcit>
      </citation>
    </references-cited>
    <parties>
      <applicants>
        <applicant data-format="original" app-type="applicant" sequence="1">
          <addressbook lang="en">
            <orgname>Electronics for Imaging, Inc.</orgname>
            <address>
              <address-1>Foster City, CA, US</address-1>
              <city>Foster City</city>
              <state>CA</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </applicant>
        <applicant data-format="questel" app-type="applicant" sequence="2">
          <addressbook lang="en">
            <orgname>ELECTRONICS FOR IMAGING</orgname>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </applicant>
      </applicants>
      <inventors>
        <inventor data-format="original" sequence="1">
          <addressbook lang="en">
            <name>Karidi, Ron J.</name>
            <address>
              <address-1>Menlo Park, CA, US</address-1>
              <city>Menlo Park</city>
              <state>CA</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
        <inventor data-format="original" sequence="2">
          <addressbook lang="en">
            <name>Nordstrom, Niklas</name>
            <address>
              <address-1>Alameda, CA, US</address-1>
              <city>Alameda</city>
              <state>CA</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
      </inventors>
      <agents>
        <agent sequence="1" rep-type="agent">
          <addressbook lang="en">
            <name>Glenn, Michael A.</name>
          </addressbook>
        </agent>
      </agents>
    </parties>
    <examiners>
      <primary-examiner>
        <name>Lee, Thomas D.</name>
      </primary-examiner>
    </examiners>
    <lgst-data>
      <lgst-status>EXPIRED</lgst-status>
    </lgst-data>
  </bibliographic-data>
  <abstract format="original" lang="en" id="abstr_en">
    <p id="P-EN-00001" num="00001">
      <br/>
      A method and apparatus for image classification includes a first embodiment, in which halftone detection is performed based on the size of a boundary set between a class of light pixels and a class of dark pixels, and further based upon image information contained within each single image plane (i.e within one color plane).
      <br/>
      This embodiment of the invention is based upon the distinctive property of images that halftone areas within the image have larger boundary sets than non-halftone areas within the image.
      <br/>
      A second, equally preferred embodiment of the invention provides a cross color difference correlation technique that is used to detect halftone pixels.
    </p>
  </abstract>
  <description format="original" lang="en" id="desc_en">
    <heading>BACKGROUND OF THE INVENTION</heading>
    <p num="1">
      1.
      <br/>
      Technical Field
    </p>
    <p num="2">
      The invention relates to image processing.
      <br/>
      More particularly, the invention relates to image classification and halftone detection, especially with regard to digitized documents, acquired for example by digital scanning, and the reproduction of such images on digital color printers.
    </p>
    <p num="3">2. Description of the Prior Art</p>
    <p num="4">
      Electronic documents contain a variety of information types in various formats.
      <br/>
      A typical page of such document might contain both text (i.e. textual information) and images (i.e. image information).
      <br/>
      These various types of information are displayed and reproduced in accordance with a particular formatting scheme, where such formatting scheme provides a particular appearance and resolution as is appropriate for such information and printing device.
      <br/>
      For example, text may be reproduced from a resident font set and images may be reproduced as continuous tone (contone) or halftone representations.
      <br/>
      In cases where a halftone is used, information about the specific screen and its characteristics (such lines per inch (Ipi)) is also important.
    </p>
    <p num="5">
      It is desirable to process each type of information in the most appropriate manner, both in terms of processing efficiency and in terms of reproduction resolution.
      <br/>
      It is therefore useful to be able to identify the various information formats within each page of a document.
      <br/>
      For example, it is desirable to identify halftone portions of a document and, as appropriate, descreen the halftone information to provide a more aesthetically pleasing rendition of, e.g an image represented by such information.
    </p>
    <p num="6">
      In this regard, various schemes are known for performing halftone detection.
      <br/>
      See, for example, T. Hironori, False Halftone Picture Processing Device, Japanese Publication No. JP 60076857 (1 May 1985); I. Yoshinori, I. Hiroyuki, K. Mitsuru, H. Masayoshi, H. Toshio, U. Yoshiko, Picture Processor, Japanese Publication No. JP 2295358 (Dec. 6, 1990); M. Hiroshi, Method and Device For Examining Mask, Japanese Publication No. JP 8137092 (May 31, 1996); T. Mitsugi, Image Processor, Japanese Publication No. JP 5153393 (Jun. 18, 1993); J.-N. Shiau, B. Farrell, Improved Automatic Image Segmentation, European Patent Application No. 521662 (Jan. 7, 1993); H. Ibaraki, M. Kobayashi, H. Ochi, Halftone Picture Processing Apparatus, European Patent No. 187724 (Sep. 30, 1992); Y. Sakano, Image Area Discriminating Device, European Patent Application NO. 291000 (Nov. 17, 1988); J.-N. Shiau, Automatic Image Segmentation For Color Documents, European Patent Application No. 621725 (Oct. 26, 1994); D. Robinson, Apparatus and Method For Segmenting An Input Image In One of A Plurality of Modes, U.S. Pat. No. 5,339,172 (Aug. 16, 1994); T. Fujisawa, T. Satoh, Digital Image Processing Apparatus For Processing A Variety of Types of Input Image Data, U.S. Pat. No. 5,410,619 (Apr. 25, 1995); R. Kowalski, D. Bloomberg, High Speed Halftone Detection Technique, U.S. Pat. No. 5,193,122 (Mar. 9, 1993); K. Yamada, Image Processing Apparatus For Estimating Halftone Images From Bilevel and Pseudo Halftone Images, U.S. Pat. No. 5,271,095 (Dec. 14, 1993); S. Fox, F. Yeskel, Universal Thresholder/Discriminator, U.S. Pat. No. 4,554,593 (Nov. 19, 1985); H. Ibaraki, M. Kobayashi, H. Ochi, Halftone Picture Processing Apparatus, U.S. Pat. No. 4,722,008 (Jan. 26, 1988); J. Stoffel, Automatic Multimode Continuous Halftone Line Copy Reproduction, U.S. Pat. No. 4,194,221 (Mar. 18, 1980); T. Semasa, Image Processing Apparatus and Method For Multi-Level Image Signal, U.S. Pat. No. 5,361,142 (Nov. 1, 1994); J.-N. Shiau, Automatic Image Segmentation For Color Documents, U.S. Pat. No. 5,341,226 (Aug. 23, 1994); R. Hsieh, Halftone Detection and Delineation, U.S. Pat. No. 4,403,257 (Sep. 6, 1983); J.-N. Shiau, B. Farrell,Automatic Image Segmentation Using Local Area Maximum and Minimum Image Signals, U.S. Pat. No. 5,293,430 (Mar. 8, 1994); and T. Semasa, Image Processing Apparatus and Method For Multi-Level Image Signal, U. S. U.S. Pat. No. 5,291,309 (Mar. 1, 1994).
    </p>
    <p num="7">
      While there is a substantial volume of art that addresses various issues associated with halftone generation and detection, there has not heretofore been available a fast and efficient technique for effective image classification and for detection of halftone segments and other components of a document.
      <br/>
      In particular, such techniques as are known do not effectively detect halftone information and classify image regions, especially with regard to efficient algorithms based upon such factors as boundary sets for image information within a single image plane and cross color differences for image information across multiple images planes.
    </p>
    <p num="8">It would be advantageous to provide an improved technique for image classification and halftone detection.</p>
    <p num="9">It would also be advantageous to provide a technique that has the to detect halftone components of a document without having predetermined information about the halftone technique used to produce the original image, and moreover, without having detailed information on the specific characteristics of that halftone technique, such as the type of screen used, the threshold array, or the Ipi.</p>
    <heading>SUMMARY OF THE INVENTION</heading>
    <p num="10">The invention provides a method and apparatus for image classification and halftone detection.</p>
    <p num="11">
      In a first embodiment of the invention, image classification and halftone detection is performed based on the size of a boundary set, and further based upon image information contained within a single image plane (i.e within one color plane).
      <br/>
      This embodiment of the invention is based upon the distinctive property of images that halftone areas within the image have a larger boundary set than non-halftone areas within the image.
    </p>
    <p num="12">
      For example, consider a window of size K * K. In this example, a threshold T1 is adaptively determined and all pixels having a value &lt;T1 are declared to be dark, while all other pixels are declared to be light.
      <br/>
      This threshold may be set in any of several ways that may include, for example a histogram technique: a histogram of values may be computed in the current window.
      <br/>
      A right peak area and left peak area are then found in the histogram.
      <br/>
      If these two areas merge, the threshold is set to the median, otherwise the threshold is set to the end of the larger peak.
    </p>
    <p num="13">As an alternative to the adaptive threshold, another technique, based on a weighted support decision mechanism, can be used to mark the pixels as dark or light.</p>
    <p num="14">
      Given another threshold T2 and a window in the image, the number of vertical class changes and horizontal class changes which occur in the window is counted, where "class change" means a change from a dark pixel to a light pixel or from a light pixel to a dark pixel.
      <br/>
      The percentage of light pixels in the window is denoted as p, while the percentage of dark pixels is denoted as q. The expected number of vertical and horizontal changes on a K * K window is 4 p q K (K-1).
    </p>
    <p num="15">
      The type of a current pixel is determined by comparing the actual number of class changes to the probability based estimate.
      <br/>
      If the ratio of these two numbers is higher than the threshold T2, then the pixel is declared a halftone pixel.
    </p>
    <p num="16">
      In a second, equally preferred embodiment of the invention, cross color difference correlation is used to detect halftone pixels.
      <br/>
      This is in contrast to most prior art techniques in which halftone detection and image region classification methods are applied separately to each color component.
    </p>
    <p num="17">
      In this embodiment, for each pixel having R, G, and B components in an image, there is a surrounding K * K window (K odd).
      <br/>
      The RGB values of the pixels in this window are denoted R(i), G(i), and B(i), where i=0, . . . , k*k-1; and the RGB averages are denoted aR, aG, and aB.
      <br/>
      It has been empirically determined that a window size of K=3 or K=5 provides the best results in terms of cost/performance.
    </p>
    <p num="18">The Euclidean norms of the R( ), G( ), B( ) vectors are denoted IRI, IGI, IBI, and the following sums are computed:</p>
    <p num="19">
      xRG= SIGMA ((R(i)-R)(G(i)-G)) xaRG= SIGMA ((R(i)-aR)(G(i)-aG))
      <br/>
      xGB= SIGMA ((G(i)-G)(B(i)-B)) xaGB= SIGMA ((G(i)-aG)(B(i)-aB))
      <br/>
      xBR= SIGMA ((B(i)-B)(R(i)-R)) xaBR= SIGMA ((B(i)-aB)(R(i)-aR))
    </p>
    <p num="20">
      The normalized results xRG/(IRI IGI), . . . correspond to a cosine of the angle between components in the window.
      <br/>
      This angle is relatively small for contone and text image information and higher for standard halftone screens used in color printing, where each color screen is tilted differently with respect to the page orientation.
    </p>
    <p num="21">The decision as to whether or not a pixel belongs to the halftone area is made by comparing the results above to a predetermined threshold, which is typically 0.6-0.7.</p>
    <p num="22">This detection technique is not as efficient in detecting line screens or screens that are exactly the same for all components and is not applicable to areas of an image in which a single ink is used.</p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading>
    <p num="23">
      FIG. 1 is a block schematic diagram of an image processing system which includes an image classification and halftone detection module according to the invention;
      <br/>
      FIG. 2 is a flow diagram of an image reconstruction path which includes an image classification and halftone detection step according to the invention;
      <br/>
      FIG. 3 is a flow diagram illustrating a boundary technique for image classification and halftone detection according to the invention;
      <br/>
      FIG. 4 is a flow diagram illustrating a cross correlation technique for image classification and halftone detection according to the invention; and
      <br/>
      FIG. 5 is a flow diagram illustrating a combined boundary detection/cross correlation technique for image classification and halftone detection according to the invention.
    </p>
    <heading>DETAILED DESCRIPTION OF THE INVENTION</heading>
    <p num="24">
      The invention provides a method and apparatus for image classification and halftone detection.
      <br/>
      The method and apparatus provides two independent techniques that may be combined to provide a robust image classification and halftone detection scheme.
      <br/>
      These techniques are referred to herein as the boundary technique and the cross correlation technique, respectively, and are discussed in detail below.
    </p>
    <p num="25">
      FIG. 1 is a block schematic diagram of an image processing system which includes an image classification and halftone detection module according to the invention.
      <br/>
      Image information is provided to the system, either as scanner RGB 15 (e.g. in the case of a digital color copier) or from memory 10.
      <br/>
      Also, a scanned image may be cropped by a cropping function 12, resulting in a video signal 11.
      <br/>
      The image information may also include JPEG data 14.
    </p>
    <p num="26">
      The image information is decompressed and deblocked, up-sampled, and converted to RGB as necessary 16.
      <br/>
      The image information is then provided to an image reconstruction path 21 (discussed in greater detail below in connection with FIG. 2).
    </p>
    <p num="27">
      The processed image in RGB or CMYK 22 may be routed to a print engine 24 and memory 19.
      <br/>
      Compression 23 is typically applied to reconstructed image information that is to be stored in the memory.
    </p>
    <p num="28">
      FIG. 2 is a flow diagram of an image reconstruction path which includes an image classification and halftone detection step according to the invention.
      <br/>
      Scanner RGB 13 is typically input to the image reconstruction path 21.
      <br/>
      The data are first subjected to preliminary color adjustment 30 and dust and background removal 31.
      <br/>
      Thereafter, halftone detection 33 is performed (as is discussed in greater detail below) and the image is descreened 34.
      <br/>
      Thereafter, the image is scaled 35, text enhancement is performed 36, and the image data are color converted 37, producing output RGB or CMYK 22 as appropriate for the system print engine.
    </p>
    <p num="29">Boundary Technique</p>
    <p num="30">
      In a first embodiment of the invention, halftone detection is performed based on the size of a boundary set, and further based upon image information contained within a single image plane (i.e within one color plane).
      <br/>
      This embodiment of the invention is based upon the distinctive property of images that halftone areas within the image have a larger boundary set than non-halftone areas within the image.
      <br/>
      When short on resources, such as computing time or memory, it is of advantage to apply this technique to the intensity component instead of applying it separately to each of the R,G,B components.
    </p>
    <p num="31">
      FIG. 3 is a flow diagram illustrating the boundary technique.
      <br/>
      The boundary technique may be expressed as follows:
    </p>
    <p num="32">In the neighborhood of every pixel, separate the neighbor pixels into two classes, i.e. dark and light (100).</p>
    <p num="33">
      In a neighborhood (which may be different than the neighborhood described above that is used to separate the pixels into classes), measure the size of the boundary between the two classes (110).
      <br/>
      The size of the boundary is estimated according to a probability based model (115).
      <br/>
      If the ratio between the actual (measured) size and the estimated size is less than a threshold T2 which is adaptively computed (120), then the pixel is not a halftone pixel (130); if the ratio between the actual (measured) size and the estimated size is equal to or greater than the threshold T2 (120), then the pixel is a halftone pixel (140) and descreening techniques may be applied thereto (150).
    </p>
    <p num="34">
      For example, consider a window of size K * K. In this example, a threshold T1 is adaptively determined and all pixels having a value &lt;T1 are declared to be dark, while all other pixels are declared to be light.
      <br/>
      This threshold may be set in any of several ways including, for example, a histogram technique: a histogram of values may be computed in the current window.
      <br/>
      A right peak area and left peak area are then found in the histogram.
      <br/>
      If these two areas merge, the threshold is set to the median, otherwise the threshold is set to the end of the larger peak.
    </p>
    <p num="35">As an alternative to the adaptive threshold, another technique, based on a weighted support decision mechanism, can be used to mark the pixels as dark or light.</p>
    <p num="36">Histogram Technique</p>
    <p num="37">A histogram of values may be computed in the current window, the histogram is analyzed, and a threshold is determined by which the class of a pixel under consideration is set as follows:</p>
    <p num="38">If the pixel value is less than the threshold, the pixel is dark; and</p>
    <p num="39">If the pixel value is greater than or equal to the threshold, then the pixel is light.</p>
    <p num="40">
      In this technique, a right peak area and left peak area are found in the histogram.
      <br/>
      If these two areas intersect, the threshold is set to the median, otherwise the threshold is set to the end of the larger peak.
    </p>
    <p num="41">Table 1 below is a pseudo code listing showing histogram analysis for the boundary echnique.</p>
    <p num="42">
      -- TABLE 1
      <br/>
      -- Histogram Analysis for the Boundary Technique
      <br/>
      --      /*
      <br/>
      --       * Histogram analysis for the "Boundary Method:
      <br/>
      --       * Go from both sides until BRK consecutive bins have count &lt;=1
      <br/>
      --       * Then compare the group sizes of the dark-class and the light-class,
      <br/>
      --       * and cut closer to the bigger group.
      <br/>
      --       *
      <br/>
      --       * The purpose of this piece of code is to compute the
      <br/>
      --       * "class_threshold":
      <br/>
      --       *
      <br/>
      --       * This number will determine the threshold from light to dark
      <br/>
      --       * in the current window.
      <br/>
      Pixels with higher intensity than the
      <br/>
      --       * threshold are considered light.
      <br/>
      The others are dark.
      <br/>
      --       *
      <br/>
      --       * We also compute "class_size" which is the size of the larger class.
      <br/>
      --       * If this size is too big, it is hard to separate the pixels into two
      <br/>
      --       * classes.
      <br/>
      --       *
      <br/>
      --       * We do the following:
      <br/>
      --       *
      <br/>
      --       * If the difference between the lightest pixel to the darkest pixel
      <br/>
      --  in
      <br/>
      --       * the window is large enough, we set the threshold to be the median.
      <br/>
      --       * If not, we decide that this is NOT a HT pixel.
      <br/>
      --       *
      <br/>
      --       */
      <br/>
      -- -define BRK 3
      <br/>
      -- -define Histogram_size 32
      <br/>
      -- const int Q_factor=256/Histogram_sizes;
      <br/>
      --      /*
      <br/>
      --       * light class
      <br/>
      --       */
      <br/>
      --      light_size=0;
      <br/>
      --      brk_cnt=0;
      <br/>
      --      light_threshold=0;
      <br/>
      --      win_max=-1;
      <br/>
      --      for (icnt=Histogram_size-1; icnt&gt;=0; 8cnt--) +
      <br/>
      --         cnt=Histogram �cnt�
      <br/>
      --         light_size+=cnt;
      <br/>
      --         if (light_size *2&lt;size) median=Q_factor *icnt;
      <br/>
      --         if ( (win_max==-1_&amp;&amp;;lightt_size&gt;=BRK)
      <br/>
      --         win_max=Qfactor * (icnt+1) -1;
      <br/>
      --         /* increment or restart  */
      <br/>
      --         brk_cnt=(light_size&gt;=BRK)  * (cnt&lt;=1)  * (brk_cnt+1);
      <br/>
      --         if (brk_cnt == BRK) +
      <br/>
      --           light_threshold=Q_factor *icnt;
      <br/>
      --           break;
      <br/>
      --           }
      <br/>
      --      }
      <br/>
      --      /*
      <br/>
      --       *  dark class
      <br/>
      --       */
      <br/>
      --      dark_size=0;
      <br/>
      --      brk_cnt=0;
      <br/>
      --      dark_threshold=256
      <br/>
      --      win_min=-1;
      <br/>
      --      for (icnt=0; icnt&lt;Histogram_size; icnt++) +
      <br/>
      --         cnt=histogram +icnt�;
      <br/>
      --         dark_size+=cnt;
      <br/>
      --         if (dark_size *2&lt;size) median=Q_factor * (icnt+1);
      <br/>
      --         if ( (win_min==-1) &amp;&amp; dark_size&gt;=BRK)
      <br/>
      --         win_min=Q_factor *icnt;
      <br/>
      --         /* increment or restart  */
      <br/>
      --         brk_cnt=(dark_size&gt;=BRK)  * (cnt&lt;=1)  * (brk_cnt+1);
      <br/>
      --         if (brk_cnt == BRK) +
      <br/>
      --         dark_threshold=Q_factor * (icnt+1);
      <br/>
      --         break;
      <br/>
      --         }
      <br/>
      --      }
      <br/>
      --      /*
      <br/>
      --       *  set threshold
      <br/>
      --       */
      <br/>
      --      win_dif=win_max-win_min;
      <br/>
      --      if  (light_size &gt;= dark_size) +
      <br/>
      --      class_threshold=light_threshold;
      <br/>
      --      class_size=light_size;
      <br/>
      --      }
      <br/>
      --      else +
      <br/>
      --      class_threshold=dark_threshold
      <br/>
      --      class_size=dark_size;
      <br/>
      --      }
      <br/>
      --      if  (class_size &gt;= size-BRK) +
      <br/>
      --         /*   uni-modal population
      <br/>
      --          * if support is wide enough, cut at the median,
      <br/>
      --          * otherwise - do not cut
      <br/>
      --          */
      <br/>
      --         if (win_dif &gt;= BRK *Q_factor) +
      <br/>
      --         class_threshold=median;
      <br/>
      --         }
      <br/>
      --         else +
      <br/>
      --             decision+NOT_HT;/* virtually constant block  */
      <br/>
      --             return ( );
      <br/>
      --         }
      <br/>
      --      }
    </p>
    <p num="43">Weighted Support Technique.</p>
    <p num="44">The following definitions are used in connection with discussion herein of the weighted support technique:</p>
    <p num="45">
      W=win-width, which is the width of the window to one side.
      <br/>
      For example, if the window is a 5 * 5 window, then there are two pixels to each side of the central (examined) pixel and the window width is W=2.
    </p>
    <p num="46">WL=win-length, which is the length of the window and which is equal to (2*W)+1.</p>
    <p num="47">WS=win-size, which is the window size and which is equal to WL*WL.</p>
    <p num="48">N_compares=2*WL*(WL-1)</p>
    <p num="49">VB=Vertical boundary, which is the number of pixels that are of a different class than the pixel directly above them.</p>
    <p num="50">HB=Horizontal boundary, which is the number of pixels that are of a different class than the pixel directly to the left of them.</p>
    <p num="51">BT=Boundary threshold, which is a parameter set by the application.</p>
    <p num="52">LC=Light class, which is the number of light pixels.</p>
    <p num="53">DC=Dark class, which is the number of dark pixels.</p>
    <p num="54">Algorithm.</p>
    <p num="55">Let:</p>
    <p num="56">
      center=intensity of the center pixel;
      <br/>
      cnt_d=number of pixels within the window that are darker than the center pixel;
      <br/>
      cnt_l=number of pixels within the window that are lighter than the center pixel;
      <br/>
      avg_d=average of intensities that are darker than the center pixel;
      <br/>
      avg_l=average of intensities that are lighter than the center pixel:
      <br/>
      avg=average of intensities in the window; and
      <br/>
      dev=standard deviation of intensities in the window.
    </p>
    <p num="57">
      Then:
      <br/>
      threshold=(avg_d+avg_l)/2;
    </p>
    <p num="58">
      and
      <br/>
      D1=(center-avg_d)/(avg_l-center) &lt;= 1/2
      <br/>
      D2=center&lt;threshold-8
      <br/>
      D3=cnt_d/cnt_l &lt;= 1/3
      <br/>
      D4=cnt_d/cnt_l &lt;= 1/2
      <br/>
      D5=center&lt;50
      <br/>
      D6=cnt_d&lt;cnt_l
      <br/>
      L1=(avg_l-center)/(center-avg_d) &gt;= 1/2
      <br/>
      L2=center&gt;threshold+8
      <br/>
      L3=cnt_l/cnt_d &lt;= 1/3
      <br/>
      L4=cnt_l/cnt_d &lt;= 1/2
      <br/>
      L5=center&gt;200
      <br/>
      L6=cnt_l&lt;cnt_d
    </p>
    <p num="59">
      Then:
      <br/>
      D_support=5*D1+4*D2+3*D3+2(D4+D5)+D6;
    </p>
    <p num="60">and</p>
    <p num="61">L_support=5*L1+4*L2+3*L3+2(L4+L5)+L6.</p>
    <p num="62">
      If (D_support&lt;L_support), then the center pixel is light;
      <br/>
      If (D_support=L_support), and (cnt_L&lt;cnt_D), then the center pixel is light;
    </p>
    <p num="63">Otherwise, the center pixel is dark.</p>
    <p num="64">
      In the event that one of the classes is too small (LC&lt;W or DC&lt;W) (FIG. 3: 300), the pixel is not halftone (310).
      <br/>
      Accordingly, the area examined is not descreened to avoid loss of shadow details.
    </p>
    <p num="65">Comparison With The Estimation Model</p>
    <p num="66">After all of the pixels have been marked with light/dark attributes using either the histogram technique or the weighted support technique, a final decision is made on the type of pixel (halftone or not halftone) based on the size of the boundary set between light pixels and dark pixels.</p>
    <p num="67">To measure the size of the boundary set in a window measuring WL * WL, perform 2*WL*(WL-1) XOR's (Exclusive OR's), where WL(WL-1) XOR's are applied for the vertical boundary and WL(WL-1) XOR's are for the horizontal boundary.</p>
    <p num="68">
      Denote N_compares=2*WL*(WL-1).
      <br/>
      A probabilistic model is then introduced for a two class population distribution.
      <br/>
      Assuming for the sake of simplicity a binomial model, then the expected number of class changes is equal to N_compares*(pq+qp),
    </p>
    <p num="69">
      where:
      <br/>
      p=probability (dark pixel),
    </p>
    <p num="70">
      and
      <br/>
      q=1-p=probability (light pixel).
    </p>
    <p num="71">Approximate p by DC/WS, q by LC/WS, then the expected size of the boundary, which is denoted by Boundary_expected, is N_compares*2pq.</p>
    <p num="72">
      In accordance with the discussion above, it follows that:
      <br/>
      Boundary_expected=(2*N_compares) (LC/WS) (DC/WS)
    </p>
    <p num="73">An external parameter BT allows a degree of freedom when fitting to the binomial model.</p>
    <p num="74">BT is a number between 0 and 1 where a value closer to 1 corresponds to a good binomial approximation.</p>
    <p num="75">If the Boundary size=VB+HB&lt;Boundary_expected*BT, mark the pixel as not halftone;</p>
    <p num="76">Else, mark the pixel as halftone.</p>
    <heading>EXAMPLE--BOUNDARY TECHNIQUE</heading>
    <p num="77">Class map:</p>
    <p num="78">x=dark, o=light.</p>
    <p num="79">
      --
      <br/>
      --  (large circle)           X     (large circle)            (large circle)  X
      <br/>
      --  (large circle)           X     (large circle)            (large circle)  X
      <br/>
      -- X                X    X                X    X
      <br/>
      --  (large circle)            (large circle)   (large circle)           X
      <br/>
      --   (large circle)
      <br/>
      --  (large circle)            (large circle)   (large circle)            (large circle)
      <br/>
      --   (large circle)
      <br/>
      W=2;
      <br/>
      WL=5;
      <br/>
      WS=25,
      <br/>
      DC=10,
      <br/>
      LC=15,
      <br/>
      N_Compares=40
      <br/>
      VB=8,
      <br/>
      HB=8,
      <br/>
      BT=0.95.
    </p>
    <p num="80">Boundary_expected=2*40*15/25*10/25=19.2</p>
    <p num="81">
      The value 19.2*0.95=18.24 is not less than 16.
      <br/>
      Therefore, the pixel is not a halftone pixel.
    </p>
    <p num="82">Cross-Correlation.</p>
    <p num="83">
      In a second, equally preferred embodiment of the invention, cross color difference correlation is used to detect halftone pixels.
      <br/>
      This is in contrast to most prior art techniques in which halftone detection and image region classification methods are applied separately to each color component.
    </p>
    <p num="84">
      In this embodiment, for each pixel having R, G, and B components in an image, there is a surrounding K * K window (K odd).
      <br/>
      The RGB values of the pixels in this window are denoted R(i), G(i), and B(i), where i=0, . . . , k*k-1; and the RGB averages are denoted aR, aG, and aB.
      <br/>
      It has been empirically determined that of window size of K=3 or K=5 provides the best results in terms of cost/performance.
    </p>
    <p num="85">
      The Euclidean norms of the R( ), G( ), B( ) vectors are denoted IRI, IGI, IBI, and the following sums are computed:
      <br/>
      xRG= SIGMA ((R(i)-R)(G(i)-G)) xaRG= SIGMA ((R(i)-aR)(G(i)-aG))
      <br/>
      xGB= SIGMA ((G(i)-G)(B(i)-B)) xaGB= SIGMA ((G(i)-aG)(B(i)-aB))
      <br/>
      xBR= SIGMA ((B(i)-B)(R(i)-R)) xaBR= SIGMA ((B(i)-aB)(R(i)-aR))
    </p>
    <p num="86">
      The normalized results xRG/(IRI IGI), . . . correspond to the cosine of the angle between components in the window.
      <br/>
      This angle is relatively small for contone and text image information and higher for standard halftone screens used in color printing, where each color screen is tilted differently with respect to the page orientation.
    </p>
    <p num="87">The decision as to whether or not a pixel belongs to the halftone area is made by comparing the results above to a predetermined threshold, which is typically 0.6-0.7. This detection technique is not as efficient in detecting line screens or screens that are exactly the same for all components and is not applicable to areas of an image in which a single ink is used.</p>
    <p num="88">
      This embodiment of the invention computes the correlation between the R plane, the G plane, and the B plane inside a square neighborhood of a current pixel.
      <br/>
      A low correlation factor indicates a halftone (HT) area having halftone screens that are not the same for all colors.
      <br/>
      This method is usually used in conjunction with the boundary set method (discussed above) and is a complementary halftone detection method.
    </p>
    <p num="89">The computation is controlled by the following parameters:</p>
    <p num="90">W=Window size=The size of a neighborhood window.</p>
    <p num="91">MN=Minimal norm=Threshold for minimal norm within a single component window.</p>
    <p num="92">T=Correlation threshold=threshold for classifying a single pixel.</p>
    <p num="93">A=W * W=area of a window.</p>
    <p num="94">
      FIG. 4 is a flow diagram illustrating a cross correlation technique for image classification and halftone detection according to the invention.
      <br/>
      Consider a window of size W * W, with R,G,B components Rij, Gij, Bij and center pixel components R,G,B (400).
      <br/>
      The following discussion describes how this embodiment of the invention determines whether or not the current pixel should be marked as a halftone candidate.
    </p>
    <p num="95">Computations:</p>
    <p num="96">
      Calculate the variational norm within each single component window (410): N(R), N(G), N(B).
      <br/>
      Denote Rij=Rij-R, Gij=Gij-G, Bij=Bij-B,
      <br/>
      N(R)=( SIGMA Rij2)1/2
    </p>
    <p num="97">Calculate correlation factors (420).</p>
    <p num="98">
      SIGMA Cor(R,G)=2 if N(R) &lt;= MN or N(G) &lt;= MN
      <br/>
      SIGMA Cor(R,G)=.vertline.
      <br/>
      SIGMA Rij Gij .vertline./(N(R) N(G) otherwise
    </p>
    <p num="99">Similarly, define N (G), N(B), Cor(G,B), Cor(B,R).</p>
    <p num="100">
      Compare correlation factors with a threshold T (430).
      <br/>
      A pixel is marked as a halftone candidate (450) if at least one of the correlation factors Cor(R,G), Cor (G,B), Cor (B,R) is less than T (440).
      <br/>
      If not, i.e. if all of them are greater or equal to T, the current pixel is marked as non-halftone (460).
    </p>
    <heading>EXAMPLES--CORRELATION TECHNIQUE</heading>
    <p num="101">Example 1</p>
    <p num="102">Let W=3, MN=20.00, T=0.35</p>
    <p num="103">Neighborhood:</p>
    <p num="104">
      --
      <br/>
      --                    (R)
      <br/>
      --                         110    42     58
      <br/>
      --                         201    80     43
      <br/>
      --                         7      255    255
      <br/>
      --                    (G)
      <br/>
      --                         81     140    60
      <br/>
      --                         90     191    220
      <br/>
      --                         37     204    64
      <br/>
      --                    (B)
      <br/>
      --                         200    189    204
      <br/>
      --                         196    200    205
      <br/>
      --                         187    197    198
      <br/>
      N(R)=292.26, N(G)=287.95, N(B)=18.97.
    </p>
    <p num="105">Cor(R,G)=0.243, Cor(G,B)=2, Cor(B,R)=2.</p>
    <p num="106">Because 0.243&lt;T=0.35, the center pixel (80,191,200) is marked as HT candidate.</p>
    <p num="107">Example 2</p>
    <p num="108">
      --
      <br/>
      --                    (R)
      <br/>
      --                         147    145    237
      <br/>
      --                         147    243    131
      <br/>
      --                         231    134    146
      <br/>
      --                    (G)
      <br/>
      --                         72     81     29
      <br/>
      --                         75     15     72
      <br/>
      --                         24     85     80
      <br/>
      --                    (B)
      <br/>
      --                         251    255    253
      <br/>
      --                         250    250    254
      <br/>
      --                         255    255    255
    </p>
    <p num="109">N(R)=249.1, N(G)=154.45, N(B)=11.22.</p>
    <p num="110">Cor(R,G)=0.99, Cor (G,B)=2, Cor (B,R)=2.</p>
    <p num="111">Because all correlation factors are greater than 0.35, the pixel is marked non-halftone.</p>
    <p num="112">Combined Boundary Detection/Cross Correlation Technique.</p>
    <p num="113">
      As discussed above, the boundary detection technique and cross correlation technique may be combined.
      <br/>
      FIG. 5 is a flow diagram illustrating a combined boundary detection/cross correlation technique for image classification and halftone detection according to the invention.
      <br/>
      The boundary detection technique is preferably applied first (500), using either the histogram technique (520) or the weighted support technique (510).
      <br/>
      If the pixel is not detected to be a halftone pixel by the boundary detection technique (525), the cross correlation technique is then applied (530).
      <br/>
      If at least one technique detects the pixel as being a halftone pixel, then the pixel is marked as a halftone pixel (550).
      <br/>
      If neither boudnary detetion technique, nor the cross relation technique detect the pixel as a halftone pixel (525,535), then the pixel is marked as a non-halftone pixel (540).
      <br/>
      This combined technique is extremely accurate but is computationally expensive.
      <br/>
      However, this technique does provide two levels of determination with regard to pixel type and thus improves the image quality by reliably applying descreening techniques to halftone pixels.
    </p>
    <p num="114">
      Although the invention is described herein with reference to the preferred embodiment, one skilled in the art will readily appreciate that other applications may be substituted for those set forth herein without departing from the spirit and scope of the present invention.
      <br/>
      Accordingly, the invention should only be limited by the Claims included below.
    </p>
  </description>
  <claims format="original" lang="en" id="claim_en">
    <claim num="1">
      <claim-text>What is claimed is:</claim-text>
      <claim-text>1.</claim-text>
      <claim-text>A method for image classification and halftone detection, comprising:</claim-text>
      <claim-text>selecting a subject pixel; separating neighbor pixels about the subject pixel into light and dark classes in a first neighborhood; measuring a size of a boundary set between the light and dark classes in a second neighborhood; determining if the boundary size is less than a first threshold, wherein the subject pixel is not a halftone pixel;</claim-text>
      <claim-text>and determining if the boundary size is equal to or greater than the first threshold, wherein the subject pixel is a halftone pixel, and wherein descreening techniques may optionally be applied to the subject pixel.</claim-text>
    </claim>
    <claim num="2">
      <claim-text>2. The method of claim 1, wherein said first threshold is adaptively determined.</claim-text>
    </claim>
    <claim num="3">
      <claim-text>3. The method of claim 2, further comprising the steps of: counting the number of vertical class changes and horizontal class changes which occur in said second neighborhood; denoting the percentage of light pixels in said second neighborhood as p; denoting the percentage of dark pixels as q; determining the type of said subject pixel by comparing the actual number of class changes counted to a probability based estimate to determine a ratio therebetween;</claim-text>
      <claim-text>and declaring said subject pixel a halftone pixel if said ratio is higher than a second threshold; wherein said subject pixel is otherwise not a halftone pixel.</claim-text>
    </claim>
    <claim num="4">
      <claim-text>4. The method of claim 1, wherein the light/dark separation is performed using any of a histogram technique and a weighted support technique.</claim-text>
    </claim>
    <claim num="5">
      <claim-text>5. The method of claim 4, wherein a second threshold is set using said histogram technique, the method further comprising: computing a histogram of values in the first neighborhood; analyzing the histogram;</claim-text>
      <claim-text>and determining the threshold.</claim-text>
    </claim>
    <claim num="6">
      <claim-text>6. The method of claim 5, further comprising the steps of: finding a right peak area and left peak area in said histogram; setting said first threshold to a median if said right peak area and said left peak area intersect, otherwise setting said first threshold to the end of the larger of said right peak area and said left peak area.</claim-text>
    </claim>
    <claim num="7">
      <claim-text>7. The method of claim 4, wherein a second threshold is set using the weighted support technique, the method further comprising: setting:</claim-text>
      <claim-text>- threshold=(avg_d+avg_l)/2; wherein: - center=intensity of the subject pixel; - cnt_d=number of pixels within the second neighborhood that are darker than the subject pixel; - cnt_l=number of pixels within the second neighborhood that are lighter than the subject pixel; - avg_d=average of intensities that are darker than the subject pixel; - avg_l=average of intensities that are lighter than the subject pixel; - avg=average of intensities in the second neighborhood;</claim-text>
      <claim-text>and - dev=standard deviation of intensities in the second neighborhood.</claim-text>
    </claim>
    <claim num="8">
      <claim-text>8. The method of claim 7, further comprising the steps of: setting: - D1=(center-avg_d)/(avg_l-center) &lt;= 1/2 - D2=center&lt;threshold-8 - D3=cnt_d/cnt_l &lt;= 1/3 - D4=cnt_d/cnt_l &lt;= 1/2 - D5=center&lt;50 - D6=cnt_d&lt;cnt_l and - L1=(avg_l-center)/(avg-avg__d) &gt;= 1/2 - L2=center&gt;threshold+8 - L3=cnt_l/cnt_d &lt;= 1/3 - L4=cnt_l/cnt_d &lt;= 1/2 - L5=cint&gt;200 - L6=cnt_l&lt;cnt_d -  wherein:</claim-text>
      <claim-text>-  D_support=5*D1+4*D2+3*D3+2(D4+D5)+D6; -  and -  L_support=5*L1+4*L2+3*L3+2(L4+L5)+L6 and; wherein said center pixel is light if (D_support&lt;L_support);</claim-text>
      <claim-text>said center pixel is light if (D_support=L_support);</claim-text>
      <claim-text>and (cnt_L&lt;cnt_D) and otherwise said center pixel is dark.</claim-text>
    </claim>
    <claim num="9">
      <claim-text>9. The method of claim 8, wherein a model is introduced for a two class population distribution, comprising the steps of: performing 2*WL(WL-1) exclusive OR, in a window measuring WL * WL, where WL(WL-1) exclusive ORs are applied for a vertical boundary and WL(WL-1) exclusive ORs are applied for a horizontal boundary; denoting N_compares=2*W(WL-1), where there is an expected number of class changes that is equal to N_compares*(pq+qp), where: - p=probability (dark pixel), and - q=1-p=probability (light pixel);</claim-text>
      <claim-text>and approximating p by DC/WS, q by LC/WS, then the expected size of a boundary which is denoted by Boundary_expected, is N_compares*2pq. wherein: - W=the width of said second neighborhood to one side; - WL=the length of said second neighborhood, wherein WL is equal to (2*W)+1; - WS=the size of the second neighborhood, which is equal to WL*WL; - N_compares=2*WL*(WL-1); - VB=the number of pixels that are of a different class than a pixel directly above them; - HB=the number of pixels that are of a different class than a pixel directly to the left of them; - BT=Boundary threshold; - LC=the number of light pixels;</claim-text>
      <claim-text>and - DC=the number of dark pixels.</claim-text>
    </claim>
    <claim num="10">
      <claim-text>10. The method of claim 9, wherein: Boundary_expected=(2*N_compares) (LC/WS) (DC/WS).</claim-text>
    </claim>
    <claim num="11">
      <claim-text>11. The mehtod of claim 10, further comprising the step of: marking said subject pixel as not halftone if Boundary size=VB+HB&lt;Boundary_expected*BT;</claim-text>
      <claim-text>otherwise marking said subject pixel as half;</claim-text>
      <claim-text>wherein an external parameter BT allows a degree of freedom when fitting to a binamial model.</claim-text>
    </claim>
    <claim num="12">
      <claim-text>12. The method of claim 10, wherein said subject pixel is not halftone if LC&lt;W or DC&lt;W.</claim-text>
    </claim>
    <claim num="13">
      <claim-text>13. The method of claim 1, further comprising the step of: applying a cross correlation technique to detect halftone pixels.</claim-text>
    </claim>
    <claim num="14">
      <claim-text>14. The method of claim 13, comprising the step of: computing a correlation between an R plane, a G plane, and a B plane inside a square neighborhood of said subject pixel; wherein a low correlation factor indicates a halftone area having halftone screens that are not the same for all colors.</claim-text>
    </claim>
    <claim num="15">
      <claim-text>15. The method of claim 14, further comprising the steps of: calculating a variational norm within a single component window, wherein Rij=Rij-R, Gij=Gij_G, Bij=Bij_B: calculating correlation factors:  (Equation image '1' not included in text)   similarly defining N(G), N(B), Cor(G,B), Cor(B,R); comparing correlation factors with a threshold T; marking said subject pixel as a halftone candidate if at least one of said correlation factors Cor(R,G), Cor(G,B), Cor (B,R) is less than a threshold;</claim-text>
      <claim-text>and marking said subject pixel as not halftone if all of said correlation factors are greater or equal to said threshold.</claim-text>
    </claim>
    <claim num="16">
      <claim-text>16. A method for image classification and halftone detection, comprising the steps of: selecting a subject pixel; separating neighbor pixels about said subject pixel into light and dark classes in a first neighborhood; measuring the size of a boundary between said light and dark classes in a second neighborhood; determining if said boundary size is less than a first threshold, wherein said subject pixel is not a halftone pixel; determining if said boundary size is equal to or greater than said first threshold, wherein said subject pixel is a halftone pixel, and wherein descreening techniques may optionally be applied to said subject pixel;</claim-text>
      <claim-text>and applying a cross correlation technique to detect halftone pixels.</claim-text>
    </claim>
    <claim num="17">
      <claim-text>17. An apparatus for image classification and halftone detection, comprising: a first neighborhood consisting of: - a subject pixel;</claim-text>
      <claim-text>and - neighbor pixels about said subject pixel; - wherein said neighbor pixels about said subject pixel are separated into light and dark classes in said first neighborhood; a second neighborhood, wherein the size of a boundary set between said light and dark classes in said second neighborhood is measured; means for determining if said boundary size is less than a first threshold, wherein said subject pixel is not a halftone pixel; means for determining if said boundary size is equal to or greater than said first threshold, wherein said subject pixel is a halftone pixel;</claim-text>
      <claim-text>and means for optionally applying descreening techniques to said subject pixel.</claim-text>
    </claim>
    <claim num="18">
      <claim-text>18. The apparatus of claim 17, wherein said first threshold is adaptively determined.</claim-text>
    </claim>
    <claim num="19">
      <claim-text>19. The apparatus of claim 18, further comprising: means for counting the number of vertical class changes and horizontal class changes which occur in said second neighborhood; means for denoting the percentage of light pixels in said second neighborhood as p; means for denoting the percentage of dark pixels as q; means for determining said first threshold by comparing the actual number of class changes counted to a probability based estimate to determine a ratio therebetween;</claim-text>
      <claim-text>and means for declaring said subject pixel a halftone pixel if said ratio is higher than said a second threshold; wherein said subject pixel is otherwise not a halftone pixel.</claim-text>
    </claim>
    <claim num="20">
      <claim-text>20. The apparatus of claim 17, wherein said light/dark separation is performed using any of a histogram technique and a weighted support technique.</claim-text>
    </claim>
    <claim num="21">
      <claim-text>21. The apparatus of claim 20, wherein said light/dark separation is performed using said histogram technique, said apparatus further comprising: means for computing a histogram of values in said first neighborhood; means for analyzing said histogram; means for declaring a pixel as darkdarker than a second threshold and declaring said pixel as light if it is lighter than said second threshold;</claim-text>
      <claim-text>and means for determining said second threshold.</claim-text>
    </claim>
    <claim num="22">
      <claim-text>22. The apparatus of claim 21, further comprising: means for finding a right peak area and left peak area in said histogram;</claim-text>
      <claim-text>and means for setting said first threshold to a median if said right peak area and said left peak area intersect, otherwise setting said first threshold to the end of the larger of said right peak area and said left peak area.</claim-text>
    </claim>
    <claim num="23">
      <claim-text>23. The apparatus of claim 17, wherein said dark/light separation is performed using said weighted support technique, said apparatus further comprising: means for setting: - threshold=(avg_d+avg_l)/2; wherein: - center=intensity of said subject pixel; - cnt_d=number of pixels within said second neighborhood that are darker than said subject pixel; - cnt_l=number of pixels within said second neighborhood that are lighter than said subject pixel; - avg_d=average of intensities that are darker than said subject pixel; - avg_l=average of intensities that are lighter than said subject pixel; - avg=average of intensities in said second neighborhood;</claim-text>
      <claim-text>and - dev=standard deviation of intensities in said second neighborhood.</claim-text>
    </claim>
    <claim num="24">
      <claim-text>24. The apparatus of claim 23, further comprising: means for setting: - D1=(center-avg_d)/(avg_l-center) &lt;= 1/2 - D2=center&lt;threshold-8 - D3=cnt_d/cnt_l &lt;= 1/3 - D4=cnt_d/cnt_l &lt;= 1/2 - D5=center&lt;50 - D6=cnt_d&lt;cnt_l and - L1=(avg_l-center)/(avg-avg_d) &gt;= 1/2 - L2=center&gt;threshold+8 - L3=cnt_l/cnt_d &lt;= 1/3 - L4=cnt_l/cnt_d &lt;= 1/2 - L5=cint&gt;200 - L6=cnt_l&lt;cnt_d wherein: - D_support=5*D1+4*D2+3*D3+2(D4+D5)+D6; and - L_support=5*L1+4*L2+3*L3+2(L4+L5)+L6 and; wherein said center pixel is light if (D_support&lt;L_support);</claim-text>
      <claim-text>said center pixel is light if (D_support=L_support);</claim-text>
      <claim-text>and (cnt_L&lt;cnt_D) and otherwise said center pixel is dark.</claim-text>
    </claim>
    <claim num="25">
      <claim-text>25. The apparatus of claim 24, wherein a model is introduced for a two class population distribution, comprising: means for performing 2*WL(WL-1) exclusive ORs in a window measuring WL * WL, where WL(WL-1) exclusive ORs are applied for a vertical boundary and WL(WL-1) exclusive ORs are applied for a horizontal boundary; means for denoting N_compares=2*W*(WL-1) where there is an expected number of class changes that is equal to N_compares*(pq+qp), where: - p=probability (dark pixel), and - q=1-p=probability (light pixel);</claim-text>
      <claim-text>and means for approximating pN DC/WS, qN LC/WS, such that the expected size of a boundary which is denoted by Boundary_expected, is N_compares*2pq; wherein: - W=the width of said second neighborhood to one side; - WL=the length of said second neighborhood, wherein WL is equal to (2*W)+1; - WS=the size of the second neighborhood, which is equal to WL*WL; - N_compares=2*WL*(WL-1); - VB=the number of pixels that are of a different class than a pixel directly above them; - HB=the number of pixels that are of a different class than a pixel directly to the left of them; - BT=Boundary threshold; - LC=the number of light pixels;</claim-text>
      <claim-text>and - DC=the number of dark pixels.</claim-text>
    </claim>
    <claim num="26">
      <claim-text>26. The apparatus of claim 25, wherein: Boundary_expected=(2 *N_compares) (LC/WS) (DC/WS).</claim-text>
    </claim>
    <claim num="27">
      <claim-text>27. The apparatus of claim 26, further comprising: means for marking said subject pixel as not halftone if Boundary size=VB+HB&lt;Boundary_expected*BT;</claim-text>
      <claim-text>otherwise marking said subject pixel as half;</claim-text>
      <claim-text>wherein an external parameter BT allows a degree of freedom when fitting to be a binomial model.</claim-text>
    </claim>
    <claim num="28">
      <claim-text>28. The apparatus of claim 26, wherein said subject pixel is not halftone if LC&lt;W or DC&lt;W.</claim-text>
    </claim>
    <claim num="29">
      <claim-text>29. The apparatus of claim 17, further comprising: means for applying a cross correlation technique to detect halftone pixels.</claim-text>
    </claim>
    <claim num="30">
      <claim-text>30. The apparatus of claim 29, further comprising: means for computing a correlation between an R plane, a G plane, and a B plane inside a square neighborhood of said subject pixel; wherein a low correlation factor indicates a halftone area having halftone screens that are not the same for all colors.</claim-text>
    </claim>
    <claim num="31">
      <claim-text>31. The apparatus of claim 30, further comprising: means for calculating a variational norm within a single component window, wherein Rij=Rij-R, Gij=Gij-G , Bij=Bij-B: means for calculating correlation factors:  (Equation image '2' not included in text)  means for similarly defining N(G), N(B), Cor(G,B), Cor(B,R); means for comparing correlation factors with a threshold T; means for marking the subject pixel as a halftone candidate if at least one of the correlation factors Cor(R,G), Cor(G,B), Cor(B,R) is less than a threshold;</claim-text>
      <claim-text>and means for marking the subject pixel as not halftone if all of the correlation factors are greater or equal to the threshold.</claim-text>
    </claim>
    <claim num="32">
      <claim-text>32. An apparatus for image classification and halftone detection, comprising: means for selecting a subject pixel; means for separating neighbor pixels about said subject pixel into light and dark classes in a first neighborhood; means for measuring the size of a boundary between said light and dark classes in a second neighborhood; means for determining if said boundary size is less than a first threshold, wherein said subject pixel is not a halftone pixel; means for determining if said boundary size is equal to or greater than said first threshold, wherein said subject pixel is a halftone pixel, and wherein descreening techniques may optionally be applied to said subject pixel;</claim-text>
      <claim-text>and means for applying a cross correlation technique to detect halftone pixels if said boundary size is not equal to or greater than said first threshold; means for calculating a variational norm within a single component window; means for calculating correlation factors for said subtext pixel;</claim-text>
      <claim-text>and means for comparing said correlation factors with a second threshold, wherein said subject pixel is a halftone pixel if at least one of said correlation factors is less than said second threshold.</claim-text>
    </claim>
  </claims>
</questel-patent-document>