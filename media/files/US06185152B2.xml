<?xml version="1.0" encoding="UTF-8"?>
<questel-patent-document lang="en" date-produced="20180805" produced-by="Questel" schema-version="3.23" file="US06185152B2.xml">
  <bibliographic-data lang="en">
    <publication-reference publ-desc="Granted patent as second publication">
      <document-id>
        <country>US</country>
        <doc-number>06185152</doc-number>
        <kind>B2</kind>
        <date>20010206</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>US6185152</doc-number>
      </document-id>
    </publication-reference>
    <original-publication-kind>B2</original-publication-kind>
    <application-reference is-representative="YES" family-id="22828787" extended-family-id="42113126">
      <document-id>
        <country>US</country>
        <doc-number>09221664</doc-number>
        <kind>A</kind>
        <date>19981223</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>1998US-09221664</doc-number>
      </document-id>
      <document-id data-format="questel_Uid">
        <doc-number>43171201</doc-number>
      </document-id>
    </application-reference>
    <language-of-filing>en</language-of-filing>
    <language-of-publication>en</language-of-publication>
    <priority-claims>
      <priority-claim kind="national" sequence="1">
        <country>US</country>
        <doc-number>22166498</doc-number>
        <kind>A</kind>
        <date>19981223</date>
        <priority-active-indicator>Y</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="1">
        <doc-number>1998US-09221664</doc-number>
      </priority-claim>
    </priority-claims>
    <dates-of-public-availability>
      <publication-of-grant-date>
        <date>20010206</date>
      </publication-of-grant-date>
    </dates-of-public-availability>
    <classifications-ipcr>
      <classification-ipcr sequence="1">
        <text>G01S   3/80        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>01</class>
        <subclass>S</subclass>
        <main-group>3</main-group>
        <subgroup>80</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="2">
        <text>G01S   3/802       20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>01</class>
        <subclass>S</subclass>
        <main-group>3</main-group>
        <subgroup>802</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="3">
        <text>H04N   7/14        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>7</main-group>
        <subgroup>14</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="4">
        <text>H04N   7/18        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>7</main-group>
        <subgroup>18</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
    </classifications-ipcr>
    <classification-national>
      <country>US</country>
      <main-classification>
        <text>367118000</text>
        <class>367</class>
        <subclass>118000</subclass>
      </main-classification>
      <further-classification sequence="1">
        <text>348E07079</text>
        <class>348</class>
        <subclass>E07079</subclass>
      </further-classification>
      <further-classification sequence="2">
        <text>348E07085</text>
        <class>348</class>
        <subclass>E07085</subclass>
      </further-classification>
    </classification-national>
    <classifications-ecla>
      <classification-ecla sequence="1">
        <text>G01S-003/80</text>
        <section>G</section>
        <class>01</class>
        <subclass>S</subclass>
        <main-group>3</main-group>
        <subgroup>80</subgroup>
      </classification-ecla>
      <classification-ecla sequence="2">
        <text>G01S-003/802</text>
        <section>G</section>
        <class>01</class>
        <subclass>S</subclass>
        <main-group>3</main-group>
        <subgroup>802</subgroup>
      </classification-ecla>
      <classification-ecla sequence="3">
        <text>H04N-007/14A2</text>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>007</main-group>
        <subgroup>14A2</subgroup>
      </classification-ecla>
      <classification-ecla sequence="4">
        <text>H04N-007/18</text>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>7</main-group>
        <subgroup>18</subgroup>
      </classification-ecla>
    </classifications-ecla>
    <patent-classifications>
      <patent-classification sequence="1">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>H04N-007/18</classification-symbol>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>7</main-group>
        <subgroup>18</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="2">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>G01S-003/80</classification-symbol>
        <section>G</section>
        <class>01</class>
        <subclass>S</subclass>
        <main-group>3</main-group>
        <subgroup>80</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="3">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>G01S-003/802</classification-symbol>
        <section>G</section>
        <class>01</class>
        <subclass>S</subclass>
        <main-group>3</main-group>
        <subgroup>802</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="4">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>H04N-007/142</classification-symbol>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>7</main-group>
        <subgroup>142</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
    </patent-classifications>
    <number-of-claims>30</number-of-claims>
    <exemplary-claim>1</exemplary-claim>
    <figures>
      <number-of-drawing-sheets>6</number-of-drawing-sheets>
      <number-of-figures>8</number-of-figures>
      <image-key data-format="questel">US6185152</image-key>
    </figures>
    <invention-title format="original" lang="en" id="title_en">Spatial sound steering system</invention-title>
    <references-cited>
      <citation srep-phase="examiner">
        <patcit num="1">
          <text>FREUDENSCHUSS OTTO, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4239356</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4239356</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="2">
          <text>LIPSKY ALAN H</text>
          <document-id>
            <country>US</country>
            <doc-number>4312053</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4312053</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="3">
          <text>RIEDLINGER RAINER</text>
          <document-id>
            <country>US</country>
            <doc-number>4639904</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4639904</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="4">
          <text>WELLS DONALD R</text>
          <document-id>
            <country>US</country>
            <doc-number>5099456</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5099456</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="5">
          <document-id>
            <country>US</country>
            <doc-number>D381024</doc-number>
            <kind>S</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>USD381024</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="6">
          <document-id>
            <country>US</country>
            <doc-number>D389839</doc-number>
            <kind>S</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>USD389839</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="7">
          <text>FIELDS CRAIG I</text>
          <document-id>
            <country>US</country>
            <doc-number>4400724</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4400724</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="8">
          <text>WASHINO KINYA, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5625410</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5625410</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="9">
          <text>CHU PETER LEE, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5664021</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5664021</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="10">
          <text>BAKER ROBERT G</text>
          <document-id>
            <country>US</country>
            <doc-number>5686957</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5686957</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="11">
          <text>CHU PETER L</text>
          <document-id>
            <country>US</country>
            <doc-number>5715319</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5715319</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="12">
          <text>ELKO GARY WAYNE</text>
          <document-id>
            <country>US</country>
            <doc-number>5742693</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5742693</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="13">
          <text>CHU PETER L, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5778082</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5778082</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="14">
          <text>CHU PETER LEE, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5787183</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5787183</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="1">
          <text>"Concorde 4500 Including System 4000ZX Group Videoconferencing System", Copyright 1997 PictureTel Corporation, hosted by Onward Technologies, Inc., 2 pages.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="2">
          <text>"Developer's ToolKit For Live 50/100 and Groups Systems", Copyright 1997 PictureTel Corporation, hosted by Onward Technologies, Inc., 2 pages.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="3">
          <text>"LimeLight Dynamic Speaker Locating Technology", Copyright 1997 PictureTel Corporation, hosted by Onward Technologies, Inc., 3 pages.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="4">
          <text>"Product Specifications, Concorde 4500 Including System 4000ZX Group Videoconferencing system", Copyright 1997 PictureTel Corporation, hosted by Onward Technologies, Inc.,5 pages.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="5">
          <text>"Virtuoso Advnaced Audio Package", Copyright 1997 PictureTel Corporation, hosted by Onward Technologies, Inc., 1 page.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="6">
          <text>Begault, D.R., 3-D Sound for Virtual Reality and Multimedia, Academic Press, Inc., Chestnut Hill, MA, table of contents, (1994).</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="7">
          <text>Crossman, A., "Summary of ITU-T Speech/Audio Codes Used in the ITU-T Videoconferencing Standards", PictureTel Corporation, 1 page, (Jul. 1, 1997).</text>
        </nplcit>
      </citation>
    </references-cited>
    <parties>
      <applicants>
        <applicant data-format="original" app-type="applicant" sequence="1">
          <addressbook lang="en">
            <orgname>Intel Corporation</orgname>
            <address>
              <address-1>Santa Clara, CA, US</address-1>
              <city>Santa Clara</city>
              <state>CA</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </applicant>
        <applicant data-format="questel" app-type="applicant" sequence="2">
          <addressbook lang="en">
            <orgname>INTEL</orgname>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </applicant>
      </applicants>
      <inventors>
        <inventor data-format="original" sequence="1">
          <addressbook lang="en">
            <name>Shen, Albert</name>
            <address>
              <address-1>Portland, OR, US</address-1>
              <city>Portland</city>
              <state>OR</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
      </inventors>
      <agents>
        <agent sequence="1" rep-type="agent">
          <addressbook lang="en">
            <orgname>Schwegman, Lundberg, Woessner &amp; Kluth, P.A.</orgname>
          </addressbook>
        </agent>
      </agents>
    </parties>
    <examiners>
      <primary-examiner>
        <name>Pihulic, Daniel T.</name>
      </primary-examiner>
    </examiners>
    <lgst-data>
      <lgst-status>GRANTED</lgst-status>
    </lgst-data>
  </bibliographic-data>
  <abstract format="original" lang="en" id="abstr_en">
    <p id="P-EN-00001" num="00001">
      <br/>
      An apparatus and method for determining directionality of acoustic signals arriving from an acoustic source is disclosed.
      <br/>
      A plurality of reflectors for modifying the acoustic signals and a transducer located proximate to the reflectors is used.
      <br/>
      A notch detector detects and identifies spectral notches in the modified acoustic signals.
      <br/>
      A device then determines the direction of the acoustic source.
      <br/>
      In one embodiment, a microphone system capable of detecting three-dimensional sound is provided.
      <br/>
      The microphone system comprises an elliptical-shaped microphone enhancer having at least two reflectors located different distances apart from a microphone located in the center of the ellipse.
      <br/>
      The reflectors have asymmetric ridges which cause interference patterns in the signals received by the microphone, conceptually analogous to the patterns generated by the pinnae of the human ear.
    </p>
  </abstract>
  <description format="original" lang="en" id="desc_en">
    <heading>FIELD</heading>
    <p num="1">This invention relates generally to sound steering systems and, in particular the present invention relates to a spatial sound steering system utilizing three-dimensional spatial audio technology.</p>
    <heading>BACKGROUND</heading>
    <p num="2">
      Many systems have been proposed which can detect a signal from a sound source for a variety of different purposes.
      <br/>
      Videoconferencing systems and other communication systems, as well as various types of security, automation and monitoring systems are used to enhance, simplify or safeguard lives.
      <br/>
      More advanced sound-detecting systems also make it possible to determine the direction or location of the sound source using voice-directional camera image steering.
      <br/>
      In U.S. Pat. No. 5,778,082 to Chu et al., for example, a system and method for localization of an acoustic source is described using a pair of spatially separated microphones to obtain direction or location of speech or other acoustic signals from a common sound source.
      <br/>
      Similarly, U.S. Pat. No. 5,686,957 to Baker uses an array of microphones to determine the direction of a particular human speaker in a hemispheric viewing area, and to provide directional signals to a video camera system.
      <br/>
      However, since the Chu and Baker systems, as well as other similar systems can only detect the "differences" in audio signal amplitude, two or microphones must be used to determine directionality of a sound source, thus increasing the cost and complexity of these systems.
    </p>
    <p num="3">
      U.S. Pat. No. 5,742,693 to Elko discloses an acoustic transducer comprising a finite acoustical reflecting surface or baffle having one or more sensors to produce a first or second-order differential response pattern at a predetermined frequency.
      <br/>
      The sensors in Elko are image-derived sensors which must be positioned on the baffle itself.
      <br/>
      Furthermore, the transducer in Elko requires the baffle to be about one to one-half of an acoustic wavelength at a predetermined frequency.
      <br/>
      Such limitations make it difficult to use the device in Elko with standard videoconferencing equipment, such as video cameras.
    </p>
    <p num="4">For the reasons stated above, there is a need in the art for a more convenient and compact system for detecting the directionality of a sound source.</p>
    <heading>SUMMARY</heading>
    <p num="5">
      An apparatus and method for determining directionality of acoustic signals arriving from an acoustic source is disclosed.
      <br/>
      A plurality of reflectors for modifying the acoustic signals and a transducer located proximate to the reflectors is used.
      <br/>
      A notch detector detects and identifies spectral notches in the modified acoustic signals.
      <br/>
      A device then determines the direction of the acoustic source.
    </p>
    <p num="6">
      In one embodiment, a microphone system capable of detecting three-dimensional sound is provided.
      <br/>
      The microphone system comprises an elliptical-shaped microphone enhancer having at least two reflectors located different distances apart from a microphone located in the center of the ellipse.
      <br/>
      The reflectors have asymmetric ridges which cause interference patterns in the signals received by the microphone, conceptually analogous to the patterns generated by the pinnae of the human ear.
    </p>
    <p num="7">
      In another embodiment, a sound steering system is disclosed which utilizes the directional information to guide a camera towards a sound source.
      <br/>
      Unlike conventional videoconferencing systems which use specialized imaging equipment, or require at least two microphones to determine directionality, the sound steering system of the present invention requires only one microphone and can be integrated with conventional videoconferencing equipment.
    </p>
    <p num="8">
      In another embodiment, an analog signal from the microphone system is converted into separate frames of time-based digital audio information.
      <br/>
      Fourier analysis is used to convert this time-based information into frequency-based spectral coefficients.
      <br/>
      A spatial estimator then uses the spectral coefficients to find spectral notches or anti-resonances within frames.
      <br/>
      The spatial estimator associates these frames with known spatial coordinates using a look-up table and conveys this information to the camera motor.
    </p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading>
    <p num="9">
      FIG. 1 is a block diagram of a sound steering system in one embodiment of the present invention.
      <br/>
      FIG. 2A is a simplified bottom view of a microphone enhancer in one embodiment of the present invention.
      <br/>
      FIG. 2B is an exploded perspective view of a microphone system in one embodiment of the present invention.
      <br/>
      FIG. 3 is a simplified schematic illustration showing a top view of a signal from a sound source comprising three sound waves which interact with the microphone system in one embodiment of the present invention.
      <br/>
      FIG. 4 is a simplified schematic diagram showing the geometry of azimuth and elevational angles with respect to a microphone system receiving an acoustic signal from a sound source in one embodiment of the present invention.
      <br/>
      FIG. 5 is a block diagram of a controller used in the sound steering system in one embodiment of the present invention.
      <br/>
      FIG. 6 is a flow chart showing steps for detecting three dimensional sound in one embodiment of the present invention.
      <br/>
      FIG. 7 is a simplified schematic illustration of a videoconferencing system in one embodiment of the present invention.
    </p>
    <heading>DETAILED DESCRIPTION</heading>
    <p num="10">
      An apparatus and method for determining directionality of acoustic signals arriving from an acoustic source is disclosed.
      <br/>
      In the following detailed description, reference is made to the accompanying drawings which form a part hereof, and in which is shown by way of illustration specific embodiments in which the inventions may be practiced.
      <br/>
      In the drawings, like numerals describe substantially similar components throughout the several views.
      <br/>
      These embodiments are described in sufficient detail to enable those skilled in the art to practice the invention, and it is to be understood that other embodiments may be utilized and that mechanical, procedural, electrical and other changes may be made without departing from the spirit and scope of the present inventions.
      <br/>
      The following detailed description is, therefore, not to be taken in a limiting sense, and the scope of the present invention is defined only by the appended claims, along with the fill scope of equivalents to which such claims are entitled.
    </p>
    <p num="11">
      The ability to "generate" virtual acoustics or three-dimensional (3D) sound is known in the art.
      <br/>
      This technology is used in interactive/virtual reality systems, multimedia systems, communications systems, and so forth.
      <br/>
      Essentially, by using a suitably designed digital filter, it is possible to place sounds anywhere in the "virtual" space surrounding a headphone listener.
      <br/>
      A conventional 3-D sound system, therefore, uses processes that either complement or replace spatial attributes which originally existed in association with a given sound source in order to create a spatialized sound.
      <br/>
      In contrast, the present invention provides a unique 3-D sound steering system for "detecting" existing 3D sound from a sound source.
    </p>
    <p num="12">
      Referring to FIG. 1, a block diagram of a computerized sound steering system (hereinafter "system") 100 according to one embodiment of the invention is shown.
      <br/>
      The system 100 comprises a microphone system 101, a video camera 103, and a controller 105.
      <br/>
      The controller 105 includes a signal processor 106, a spatial estimator 108, a camera motor 110, an audio compressor 111, and an audio/video transmitter 112.
      <br/>
      Input into the system 100 is from an image/sound source 116.
      <br/>
      Specifically, input into the video camera 103 is from the image source 116A and input into the microphone enhancer 102 is from the sound source 116B.
      <br/>
      Output from the audio/video transmitter 112 passes through a transmission medium 118 to a remote audio/video receiver 120.
    </p>
    <p num="13">
      In another embodiment (not shown), the system 100 can comprise the microphone system 101 and controller 105 only.
      <br/>
      In this embodiment, the controller 105 can include only the signal processor 106 and spatial estimator 108.
      <br/>
      The audio compressor 111 and audio/video transmitter 112 can be external to, but in parallel with the system 100.
      <br/>
      In this embodiment, the camera motor 110 is essentially the "consumer" of the information provided by the controller 105.
    </p>
    <p num="14">
      There exist a number of interfaces, buses or other communication devices between the various components described herein, such as between the microphone system 101 and the controller 105, the video camera 103 and the controller 105, and so forth.
      <br/>
      Such interfaces comprise all necessary conventional electrical and mechanical components as is understood in the art, and will not be discussed in detail herein.
      <br/>
      The controller 105 may be a local or remote receiver only, or a computer, such as a lap top general purpose computer as is well-known in the art.
      <br/>
      In one embodiment, the controller 105 is a personal computer having all necessary components for processing or manipulating the input signals and generating appropriate output signals.
      <br/>
      In addition to the components listed above, the controller 105 can also include any suitable type of central processing unit, a utility, a driver, an event queue, an application, and so forth, although the invention is not so limited.
      <br/>
      In one embodiment, the components of the controller 105 are all computer programs executed by a processor of the computer, which operates under the control of computer instructions, typically stored in a computer-readable medium such as a memory.
      <br/>
      The controller 105 also desirably includes an operating system for running the computer programs, as can be appreciated by those within the art.
    </p>
    <p num="15">
      The microphone system 101 includes a microphone enhancer 102 and a transducer or microphone 104.
      <br/>
      In one embodiment, the microphone enhancer 102 is comprised of a microphone sleeve containing asymmetric ridges or folds which modify an acoustic signal from the sound source 116B by creating interference patterns as described herein.
      <br/>
      The modified acoustical signal is transduced by the microphone 104 into an electrical signal which is passed to the signal processor 106.
      <br/>
      In one embodiment, the signal processor 106 converts output from the microphone 104 from analog to digital form, such as a digital representation of an integer or floating point value.
      <br/>
      The digital signal is further processed by a signal processor algorithm to extract the spectral interference pattern in the signal.
      <br/>
      This information is transmitted to the spatial estimator 108 which uses established statistics to make a "best guess" as to the directionality of the sound source 116B.
      <br/>
      The directional information from the spatial estimator 108 is output to the camera motor 110.
      <br/>
      The camera motor 110 can comprise any conventional type of camera motor or tracking system which can move a camera.
      <br/>
      The camera motor 110 then moves the video camera 103 to point it in the direction of the image/sound source 116 so that the source 116 can be framed.
      <br/>
      In one embodiment, an image recognition algorithm is used to validate the output of the spatial estimator 108.
      <br/>
      An image associated with the sound source 116B is captured and processed by the audio/video transmitter 112.
      <br/>
      Simultaneously, a compressed electrical signal from the audio compressor 111 is also output to the audio/video transmitter 112.
      <br/>
      The image and compressed electrical signal are then transmitted by the audio/video transmitter 112 through the transmission medium 118, to the remote audio/video receiver 120 in a separate location.
      <br/>
      The remote audio/video receiver 120 processes and decodes the image and associated electrical signal.
      <br/>
      The resulting video output can be viewed by a third party 122 on a monitor 124 and the audio output can be listened to using one or more speakers 126.
    </p>
    <p num="16">
      The microphone enhancer 102 can be any suitable component capable of interacting with incoming acoustic signals to create frequency characteristics unique to each spatial direction and transducing these signals to the microphone 104.
      <br/>
      These interactions or interference patterns are manifested in the spectrum of each signal as spectral notches.
      <br/>
      FIG. 2A shows one embodiment wherein the microphone enhancer 102 has an elliptical or oval configuration, such that it has a major axis 206 and a minor axis 208, although the invention is not so limited.
      <br/>
      As shown in FIG. 2B, the oval configuration maximizes the difference or delay between ridges 214 located on opposite sides of the microphone hole 202, while not increasing the size of the microphone enhancer 102 in the vertical direction.
      <br/>
      With an oval configuration, the minimum dimension of the major axis 206 is defined by the resolution of the signal, which can vary depending on the application.
      <br/>
      If the major axis 206 is too small for a particular application, the ridges 214 contained in the microphone enhancer 102 can not generate detectable patterns.
      <br/>
      If the major axis 206 is too large for a particular application, there may be no difference between the reflection of the signals from the intended sound source and those of other nearby sound sources, which may cause the system to detect unwanted background noise.
      <br/>
      In one embodiment, the microphone enhancer 102 has a major axis 206 of about two (2) cm to about six (6) cm or more, and a minor axis 208 of about 0.5 cm to about three (3) cm or more.
    </p>
    <p num="17">
      The microphone enhancer 102 further has a front face 210 (shown in FIG. 2B), a back face 212 (shown in FIG. 2A), and can also have a rim or lip 201.
      <br/>
      The microphone enhancer 102 also has means to support or house the microphone 104.
      <br/>
      In the embodiment shown in FIGS. 2A and 2B, the microphone enhancer 102 has a microphone hole 202 in the center into which the microphone 104 can be installed.
      <br/>
      Essentially, the microphone hole 202 is a circular cutout having a flat edge.
      <br/>
      In this embodiment, the microphone enhancer 102 is bowl-shaped such that when placed with the back face 212 down towards a surface, the microphone enhancer 102 rests only on the edge of the microphone hole 202.
    </p>
    <p num="18">
      FIG. 2B shows one arrangement of ridges 214 and wells 216 secured to or integral with the elliptical front face 210 of the microphone enhancer 102, although the invention is not so limited.
      <br/>
      Generally, the microphone enhancer 102 contains asymmetric ridges 214 which act as reflectors for longitudinal sound waves, conceptually analogous to the pinnae of the human ear.
      <br/>
      In the case of the human ear, however, the middle and inner ear are the intended "receivers" of these vibrations.
      <br/>
      In the present invention, the microphone 104, now equipped with its own enhancer or "ear," is the intended receiver of the sound waves.
      <br/>
      In one embodiment, there are a greater number of ridges 214 and wells 216 in one hemisphere of the microphone enhancer 102.
      <br/>
      In an alternative embodiment, larger ridges 214 are located nearer to the rim 201.
      <br/>
      In another alternative embodiment, the ridges 214 are arranged similar to petals of a flower at varying stages of opening, such that they curve generally in an outwardly direction, and may or may not extend beyond the rim 201.
    </p>
    <p num="19">
      The ridges 214 can be any suitable height, as long as a suitable interference pattern is generated.
      <br/>
      Generally, taller ridges 214 create more powerful reflection or interference signals which are easier to detect and measure.
      <br/>
      In one embodiment the ridges 214 are of variable height, ranging from about 0.25 cm to about 0.5 cm. In an alternative embodiment, the ridges 214 are all about the same height.
      <br/>
      In another alternative embodiment, some or all of the ridges 214 are greater than about 0.5 cm in height.
    </p>
    <p num="20">
      In order to determine directionality of a sound source, it is necessary to have at least two raised portions in the microphone enhancer 102, i.e., either two ridges 214 or one ridge 214 having at least two peaks or humps.
      <br/>
      In one embodiment, there are two ridges 214, each located a different distance from the microphone 104.
      <br/>
      Although the microphone enhancer 102 can contain as few as two raised portions, the use of multiple raised portions or multiple ridges 214 gives greater confidence in signal directionality because multiple notches can be detected.
      <br/>
      In one embodiment, the microphone enhancer 102 has two (2) to ten (10) ridges 214 or more.
      <br/>
      In the embodiment shown in FIGS. 2B and 3, the ridges 214 are at least slightly rounded or hyperbolic.
    </p>
    <p num="21">
      The ridges 214 must also be a sufficient distance apart from each other to provide at least a minimum level of confidence in signal interference.
      <br/>
      Otherwise, when two sound sources are the same distance from the microphone enhancer 102, but on opposite sides of the microphone 104, a "mirroring" situation or cone of confusion may result, and the system may be unable to detect which of the two sides the sound is coming from.
      <br/>
      In one embodiment, the ridges 214 are on opposing sides of the microphone hole 202, vertically centered on the major axis 206.
      <br/>
      In another embodiment, the ridges 214 are at least about one (1) cm to about 2.5 cm or more apart from each other, depending on the size of the microphone enhancer 102.
    </p>
    <p num="22">
      In order to interact properly with an incoming acoustic signal (or sound wave), the microphone enhancer 102 is placed on edge, i.e., on its rim 201, in a vertical or near-vertical plane so that the top portion of the ridges 214 are perpendicular or nearly perpendicular to the incoming signal.
      <br/>
      As the microphone enhancer 102 is tilted either forwards towards the signal or backwards away from the signal, the degree of reflection, and thus the signal strength becomes increasingly reduced.
      <br/>
      If the microphone enhancer 102 is tilted too much away from normal in either direction, a shearing effect may result, such that the ridges 214 are no longer able to produce a suitable interference pattern.
    </p>
    <p num="23">
      The microphone enhancer 102 can also be rotated to rest on any portion of the circumference of the rim 201, depending on the particular application.
      <br/>
      In one embodiment, the major axis 206 of the microphone enhancer 102 lies in a horizontal plane, i.e., landscape position.
      <br/>
      This arrangement is useful for conventional videoconferencing applications, when it is expected that all of the participants will remain seated.
      <br/>
      In an alternative embodiment, the microphone enhancer 102 is placed with the major axis 206 in a vertical direction, i.e., portrait view.
      <br/>
      This arrangement can be used when it is expected that all of the participants will be standing, such that it is desirable to detect vertical height.
      <br/>
      In another alternative embodiment, there are at least two microphone enhancers 102 which can be arranged as a "cross," such that the major axis of a first microphone enhancer is horizontal or nearly horizontal, and the major axis of a second microphone enhancer is vertical or nearly vertical.
      <br/>
      In another embodiment, one or more circular shaped microphone enhancers 102 are used.
      <br/>
      The latter two embodiments are useful when participants are both standing and sitting.
    </p>
    <p num="24">
      In human hearing, the most important cues for localizing a sound source's angular position within a given horizontal plane involves the relative difference of the wavefront at the two ears.
      <br/>
      Similarly, in the present invention, the sound source's angular position is determined based on the relative difference of the wavefront at the ridges 214 on either side of the microphone 104.
      <br/>
      Specifically, interference occurs because the same signal is traveling by more than one path, but ending at the same place, i.e., the head of the microphone 104.
      <br/>
      The combination of a portion of a signal traveling a direct path with other portions of the signal traveling along at least two different delayed or reflective paths on opposite sides of the microphone, essentially forms a triangle, with the common vertices being the sound source 116B and the microphone 104.
      <br/>
      When this happens, the delayed paths' signals begin to destructively interfere with the direct path signal only at those frequencies which corresponds with the difference in distance.
      <br/>
      In this way, the corresponding spectral notches are created.
    </p>
    <p num="25">
      FIG. 3 shows an enlarged top view of a signal, comprising three different sound waves, 302, 304 and 306, encountering the microphone system 101.
      <br/>
      The figure is not drawn to scale, and each sound wave is represented by an arrow which is at or near the center of each wave.
      <br/>
      Furthermore, only two ridges 214 are shown in FIG. 3 for simplicity, although the invention is not so limited.
      <br/>
      As shown in FIG. 3, a first reflected sound wave 302 reflects off one ridge 214, a second reflected sound wave 304 reflects off another ridge 214, and a direct path sound wave 306 goes directly into the microphone 104.
      <br/>
      As a result, reception by the microphone 104 of the reflected sound waves, 302 and 304, is delayed by an amount proportional to the distance from the center of each ridge 214 to the microphone 104.
      <br/>
      In other words, the ridge-reflected sound paths, 302 and 304, are longer and cause interference.
      <br/>
      In the embodiment shown in FIG. 3, reflection of the first reflected sound wave 302 is delayed by distance 308, and reflection of the second reflected sound wave 304 is delayed by a different distance 310.
      <br/>
      The actual amount of interference or delay is variable, depending on many factors, including, but not limited to, the relative position and size of the ridges 214, microphone 104 and sound source 116B.
      <br/>
      However, this time delay is calculable using an appropriate signal processing algorithm, which varies depending on the external conditions.
      <br/>
      In one embodiment, the time delay is equivalent to about: 1 sec/330 m, since it takes a sound wave about one (1) second to travel about 330 m at standard temperature and pressure.
      <br/>
      Using the results of the algorithm, it is possible to pinpoint the direction of the sound source 116B.
      <br/>
      In one embodiment, the location of the sound source 116B is tracked to within about +- five (5) degrees spatially.
    </p>
    <p num="26">
      As shown in FIG. 3, the microphone 104 is placed into the microphone hole 202 such that the operative end or head of the microphone 104 is surrounded by the microphone enhancer 102 on all sides.
      <br/>
      Specifically, the height of the ridges 214 are designed to be greater than the height of the microphone 104.
      <br/>
      In this way, the incoming signals from the sound source 116B are first encountered by the ridges 214 of the microphone enhancer 102, and not the microphone 104 itself.
      <br/>
      In one embodiment, the head of the microphone 104 is flush against the bottom of the microphone opening 202.
    </p>
    <p num="27">
      The microphone enhancer 102 can be made from any suitable material of any suitable thickness as long as it is relatively rigid and possesses some reflective properties.
      <br/>
      In one embodiment the material is highly reflective and spectrally flat such that it reflects all frequencies nearly equally, as opposed to absorbing some frequencies and reflecting others.
      <br/>
      In one particular embodiment, the material has an absorption coefficient of about 0.05 at frequencies of from about 100 Hz to about four (4) kHz, and has a spectral flatness of about +- one (1) decibel (dB) from frequencies of about 100 Hz to about eight (8) kHz. The types of material the microphone enhancer 102 can be made from include, but are not limited to, plastics, ceramics, metals, various types of coated materials, and so forth.
      <br/>
      In one embodiment, the microphone enhancer 102 is made from plastic using any type of injection molding process well-known in the art and is rigid enough to retain its shape over a wide range of temperatures, e.g., from about zero (0) degrees C. to about 40 degrees C. If the material is so highly reflective as to cause reverberations, such as with certain metals, it may be necessary to further process the signal so as to remove unwanted signals or noise caused by the reverberations.
    </p>
    <p num="28">
      The microphone system 101 can also be placed in any suitable position in relation to the sound source 116B as long as an adequate interference pattern is generated.
      <br/>
      In one embodiment, the microphone system 101 picks up only lateralization cues, which comprise cues only in a horizontal plane.
      <br/>
      In another embodiment, the microphone system 101 picks up localization cues, which comprise both horizontal and elevational cues.
      <br/>
      When the system 100 is only attempting to detect lateralization cues, the microphone system 101 can be placed in the same or nearly the same horizontal plane as the sound source 116B.
      <br/>
      In one embodiment, adequate interference is generated when the sound source 116B is present in a horizontal arc less than about 180 degrees in relation to the front face of the microphone system 101, such as about 15 degrees or more from either side of the microphone system 101.
      <br/>
      In this embodiment, therefore, the microphone system 101 has at least a 150 degree effective range of operation in the horizontal direction.
      <br/>
      When the system 100 is also (or only) detecting elevational cues, the microphone enhancer 102 generates its own simulated head-related transfer function (HRTF), i.e., a positional transfer function (PTF), for a particular sound source 116B.
      <br/>
      In one embodiment, adequate interference is generated when the sound source 116B is present in a vertical arc less than 180 degrees in relation to the front face of the microphone system 101, such as about 15 degrees or more from either the top or bottom of the microphone system 101.
      <br/>
      In this embodiment, the microphone system 101 has at least a 150 degree effective range of operation in the vertical direction.
      <br/>
      In most videoconferencing applications, however, the sound source 116B is located at or above the horizontal plane on which the microphone system 101 is positioned.
    </p>
    <p num="29">
      The sound source 116B can be any suitable distance away from the microphone enhancer 102 as long as an adequate interference pattern can be generated.
      <br/>
      In one embodiment, the sound source 116B is between about one (1) m and about five (5) m away from the microphone enhancer 102.
      <br/>
      If the sound source 116B is too close to the microphone enhancer 102, the associated signal becomes so large that it is difficult to accurately distinguish direction.
      <br/>
      If the sound source 116B is too far away, it becomes difficult to differentiate the sound source 116B from ongoing background noise.
      <br/>
      In one embodiment, background noise is accommodated by programming the controller 105 with a suitable algorithm.
      <br/>
      For example, the system 100 can be operated initially with only background or environmental noise present so that a baseline can be established.
      <br/>
      Once the desired sound source 116B begins, only signals above the baseline are considered by the system 100.
      <br/>
      Any signals which are occurring at the baseline or below are effectively ignored or "subtracted," i.e., only the sound waves one sine greater in proportion to the background noise are considered.
    </p>
    <p num="30">
      The microphone 104 can be any suitable type of microphone which is capable of transforming sound energy into electrical energy and of producing the desired frequency response.
      <br/>
      Microphones 104 having unusual spectral characteristics, such as notches or lowpass or highpass characteristics can also be used, provided the system 100 is programmed to compensate for these characteristics.
      <br/>
      In one embodiment, the microphone 104 is a flat or nearly flat spectrum microphone.
      <br/>
      In a particular embodiment, the microphone 104 has a spectral flatness of about +- two (2) dB from frequencies of about 100 Hz to about ten (10) kHz.
    </p>
    <p num="31">
      The microphone 104 can be any suitable size, and in one embodiment is smaller in diameter than the diameter (or major axis) of the microphone enhancer 102.
      <br/>
      In another embodiment, the microphone 104 is a small form factor microphone designed to securely fit into the microphone fitting in the microphone enhancer 102, such as the microphone hole 202.
      <br/>
      Generally, a smaller microphone 104 causes the effective size of the major axis 206 to be larger, allowing a greater delay or interference to be generated.
      <br/>
      However, if the microphone 104 is too small in relation to the microphone enhancer 102, spectral flatness may be compromised.
      <br/>
      Alternatively, if the microphone 104 is too large in relation to the microphone enhancer 102, the ability of the ridges 214 contained in the microphone enhancer 102 to generate unique interference patterns may be diminished.
      <br/>
      In one embodiment, the microphone 104 has a diameter of about 10-30% of the diameter (or major axis) of the microphone enhancer 102.
      <br/>
      In an alternative embodiment, the microphone 104 has a diameter of about 0.635 cm to about 0.95 cm. In a particular embodiment, the microphone is a Telex Claria Microphone made by Telex Communications, Inc., in Minneapolis, Minn.
    </p>
    <p num="32">
      The microphone system 101 of the present invention, comprising the microphone 104 and microphone enhancer 102, is essentially substituting for a human "listener." In order for any listener to determine the direction and location of a virtual sound source, i.e., localize the sound source, it is first necessary to determine the "angular perception." The angular perception of a virtual sound source can be described in terms of azimuth and elevational angles.
      <br/>
      Therefore, the present invention determines the azimuth angle, and if applicable, the elevational angle as well, so that the microphone system 101 can localize the sound source 116B.
      <br/>
      As shown in FIG. 4, the azimuth angle 402 refers to the relative angle of the sound source 116B on a first horizontal plane 404 parallel to groundlevel 406.
      <br/>
      The elevational angle 408 refers to the angular distance of a fixed point, such as the sound source 116B, above a horizontal plane of an object, such as above a second horizontal plane 410 of the microphone system 101.
      <br/>
      Normally, azimuth is described in terms of degrees, such that a sound source 116B located at zero (0) degrees azimuth and elevation are at a point directly ahead of the listener, in this case, the microphone system 101.
      <br/>
      Azimuth can also be described as increasing counterclockwise from zero to 360 degrees along the azimuthal circle.
      <br/>
      The azimuth angle in FIG. 4 is about 30 degrees and the elevational angle 406 is about 60 degrees.
      <br/>
      The linear distance between the sound source 116B and the microphone system 101 can be referred to as a perceived distance, although it is not necessary to directly compute this distance when localizing the sound source 116B.
    </p>
    <p num="33">
      As noted above, interference patterns are created by the microphone enhancer 102 and then received by the microphone 104.
      <br/>
      This interference is manifested in the spectrum of the signal as a "spectral notch" (or anti-resonance), which is the opposite of a "spectral peak." Processing software is used to analyze the incoming interference patterns, i.e., estimate the spectral components of a digital signal, so that the common notches associated with a specific spatial direction can be located.
      <br/>
      A suitable algorithm is used which searches in consecutive frames of input signal spectra for consistent nulls in a specified frequency range proportionate to the ridge distances used.
      <br/>
      The azimuth angle of the sound source 116B can then be estimated based on the location of the notch in the observed signal spectra through a look-up table.
      <br/>
      By also creating interference patterns in the vertical direction, the elevation angle of the sound source 116B can also be estimated using the same detection method.
      <br/>
      This searching results in a "most likely" spatial location for that position.
    </p>
    <p num="34">
      Prior to entering the spatial estimator 108, however, the signal from the microphone system 101 first enters the signal processor 106 as described above.
      <br/>
      The signal processor 106 can comprise any suitable components necessary for handling the incoming signal from the microphone system 101 and providing an appropriate signal to the spatial estimator 108.
      <br/>
      As noted above and shown in FIG. 5, the signal processor 106 can include an analog-to-digital (A/D) converter 508 and a transformer 510.
    </p>
    <p num="35">
      In the embodiment shown in FIG. 5, the A/D converter 508 includes a sampler 514, and a quantizer 515.
      <br/>
      An incoming analog signal enters the sampler 514 where sequential "samples" in time are taken.
      <br/>
      These numerical samples or frames 516 contain spectral notches, as described above.
      <br/>
      In addition to the spectral notches generated by the microphone system 101, however, the frames 516 also contain "content" spectral notches, which are present naturally in the signal from the sound source 116B.
      <br/>
      The content notches are typically transient, lasting approximately, 20 milliseconds (ms) or less and are disregarded by the system 100 as described below.
      <br/>
      The sampler 514 can operate at any suitable sampling frequency, such as about 16 kHz to about 48 kHz.
      <br/>
      In the embodiment shown in FIG., 5, the frames 516 passing through the sampler 514 are contain analog values.
      <br/>
      These frames 516 then enter the quantizer 515, where the values are adjusted or quantized into whole number increments, such that the signal is now digitized.
    </p>
    <p num="36">
      The frames 516 then exit the A/D converter 508, and are sent to the transformer 510.
      <br/>
      In the embodiment shown in FIG. 5, the transformer 510 includes a multiplier 522, a window 524, and an analyzer 526.
      <br/>
      In the transformer 510, the multiplier 522 multiplies the output signal from the A/D converter 508, using window functions generated by the window 524.
      <br/>
      The window 524 can generate any suitable window function known in the art, such as a conventional Hamming window, an exponential window, and so forth.
      <br/>
      The signals at the output of the multiplier 522 are further processed by the analyzer 526.
    </p>
    <p num="37">
      Any suitable algorithm can be used to analyze the signals, which include selecting a predetermined percentage or value for data reduction.
      <br/>
      In one embodiment, a Principal Components Analysis (PCA) or variation thereof is used, such as is described in U.S. patent application Ser.
      <br/>
      No. 08/710,270 now U.S. Pat. No. 5,928,311 to Leavy and Shen, entitled, "A Method and Apparatus for Constructing a Digital Filter." In another embodiment, the incoming digital signal is converted from a time domain to a frequency domain by performing an integral transform for each frame.
      <br/>
      Such transform can include Fourier analysis such as the inverse fast Fourier transform (IFFT) or the fast Fourier transform (FFT).
    </p>
    <p num="38">
      The specific calculations comprising the FFT are well-known in the art and will not be discussed in detail herein.
      <br/>
      Essentially, a Fourier transform mathematically decomposes a complex waveform into a series of sine waves whose amplitudes and phases are determinable.
      <br/>
      Each Fourier transform is considered to be looking at only one "slice" of time such that particular spectral anti-resonances or nulls are revealed.
      <br/>
      In one embodiment, the analyzer 526 takes a series of 512 or 1024 point FFTs of the incoming digital signal.
      <br/>
      In another embodiment, the analyzer 526 uses a modification of the algorithm described in U.S. patent application Ser.
      <br/>
      No. 08/772,016 to Shen, entitled, "Method and Apparatus for Performing Block Based Frequency Domain Filtering," (hereinafter "Shen").
      <br/>
      Since Shen describes an algorithm for "generating" three-dimensional sound, the modifications would necessarily include those which would instead incorporate parameters for "detecting" three-dimensional sound.
    </p>
    <p num="39">
      The Fourier-transformed signals of the current frame 516 may be sent directly to other components or modules for further processing.
      <br/>
      In the embodiment shown in FIG. 5, the signals are sent directly to the spatial estimator 108.
      <br/>
      Within the spatial estimator 108, the Fourier transformed signals can be used immediately by the notch detector 502 or can first be stored for subsequent processing in a memory 506 associated with each transform.
      <br/>
      The memory 506 can store up to "N" previous frames of information.
      <br/>
      Essentially, the notch detector 502 searches in the output of the analyzer 526 for notches in specific areas which correspond to spatial direction (based on reflections from the microphone enhancer 102 described above).
      <br/>
      If a prominent notch is detected, the notch detector 502 performs statistical analysis to determine if that spatial direction was previously detected, i.e., it compares a detected notch with consecutive frames of input by using a look-up table 504 containing information on previous frames 516.
      <br/>
      In this way, more recent frames 516 are compared with older frames 516 so it can be determined with sufficient confidence that a particular notch exiting the sampler 514 is a directional spectral notch introduced by the microphone enhancer 102 (described above), and not a content spectral notch.
      <br/>
      In one embodiment, sufficient confidence is achieved when a notch appears in at least about five (5) or more sequential frames 516.
      <br/>
      In one embodiment, the notch detector 502 is comprised of a horizontal notch detector and a vertical notch detector, which search for notches in a horizontal look-up table and vertical look-up table, respectively.
    </p>
    <p num="40">
      If a spatial direction was previously detected, directionality signals are then sent to the camera motor 110 which, in turn, moves the video camera so that it is pointed in the direction of the detected sound source 116B.
      <br/>
      Image information is then transmitted to the audio/video transmitter 112 from the video camera.
      <br/>
      Associated audio information is also transmitted to the audio/video transmitter from the audio compressor 111.
    </p>
    <p num="41">
      The sound steering system 100 of the present invention can be designed to accommodate many of the variable levels which characterize a sound event.
      <br/>
      These variables include frequency (or pitch), intensity (or loudness) and duration.
      <br/>
      In an alternative embodiment, spectral content (or timbre) is also detected by the system 100.
    </p>
    <p num="42">
      Specifically, the system 100 can be designed to accommodate a broad range of signal frequencies, from subsonic to supersonic bandwidths, i.e., less than 15 Hz up to greater than 20 kHz.
      <br/>
      In one embodiment, the signal processing of the sound steering system 100 is designed to focus on the bandwidth associated with audible speech, i.e., from about 300 Hz to about five (5) kHz. Through the use of spectral smoothing, the signal processor 108 can also be programmed to ignore certain sounds or noise in the spectrum.
    </p>
    <p num="43">
      The signal processor 108 can further be programmed to ignore interruptions of a second sound source for a certain period of time, such as from one (1) to five (5) seconds or more.
      <br/>
      Such interruptions can include sounds from another person and mechanical noises, such as the hum of a motor.
      <br/>
      If the sounds from the second sound source, such as the voice of another person, continue after the predetermined period, then the camera 103 can be moved in the direction of that second sound source, if desired.
    </p>
    <p num="44">
      The sensitivity of the system 100 in terms of the ability to detect a certain intensity or loudness from a given sound source 116B can also be adjusted in any suitable manner depending on the particular application.
      <br/>
      In one embodiment, the system 100 can pick up intensities associated with normal conversation, such as about 75-90 dB or more.
      <br/>
      In alternative embodiments, intensities less than about 75 dB or greater than about 90 dB can be detected.
      <br/>
      However, when the signal becomes more intense, the signal strength ratio, i.e., the ratio of the direct path signal to the reflective paths' signals may not necessarily change in the same proportion.
      <br/>
      As a result, one signal may start to hide or mask the other signal such that the reflections become difficult or nearly impossible to detect, and directionality is lost.
    </p>
    <p num="45">
      In one embodiment, the system 100 also includes an image recognition step to confirm the presence of a sound source 116B.
      <br/>
      For example, if an error has been made in determining the direction of a sound source 116B and the camera is now pointing at an empty chair or at a person whose lips do not appear to be moving, the image recognition step can be used to verify that there is no sound source 116B in that direction, and the camera can be moved accordingly.
    </p>
    <p num="46">
      Depending on particular applications, reverberations may need to be accounted for in the signal processing algorithm.
      <br/>
      In one embodiment, the system is used in a conventional conference room where the participants are not speaking in unusually close proximity to a wall.
      <br/>
      In another embodiment, a large, non-carpeted room is used having noticeable reverberations.
    </p>
    <p num="47">
      One embodiment of the above sound detection process can be described in a series of steps as shown in FIG. 6.
      <br/>
      In the acquiring step 602, a stream of acoustic data containing interference is acquired from the microphone system.
      <br/>
      This data is converted 604 into numerical sampling frames.
      <br/>
      Each frame is then multiplied 606 by a window function.
      <br/>
      A Fourier Transform is generated 608 for each frame.
      <br/>
      The transform is used 610 to search for spectral notches.
      <br/>
      Spectral notches are found 612 within each frame in specific areas corresponding to spatial directions.
      <br/>
      The frames are stored 613 in memory, and can then be correlated 614 with the previous "N" frames.
      <br/>
      A look-up table of known locations associated with different spectral notches is used 616 to determine the correct spatial direction.
      <br/>
      Lastly, the "most likely" spatial candidate is output 618 to the camera motor.
    </p>
    <p num="48">
      FIG. 7 shows one embodiment of an application of the present invention in use in a conventional videoconferencing arrangement.
      <br/>
      In this embodiment, the microphone system 101 is operatively connected to a video system comprising a video camera 103 having a lens 703 and a camera motor 110.
      <br/>
      Both the video system and microphone system 101 are installed on a conventional monitor 702.
      <br/>
      The microphone system 101 can be placed in any suitable location in relation to the monitor 702 and video camera 103.
      <br/>
      In one embodiment, the microphone system 101 is placed on the front face of the monitor 702 above a viewing screen 704, with the rim of the microphone enhancer 102 flush or nearly flush against the monitor 702.
      <br/>
      The microphone 104 itself is recessed into the base of the bowl-shaped microphone enhancer 102, as described above.
      <br/>
      In this embodiment, the presenter or sound source 116B is about directly in front of the system 100, while other participants 705 are seated nearby.
    </p>
    <p num="49">
      Any suitable type of videoconferencing system can be used.
      <br/>
      In one embodiment a video "friendly" or plug-and-play system is used.
      <br/>
      Such a system includes, but is not limited to, a Universal Serial Bus (USB) enabled computer system.
      <br/>
      Such platforms can utilize the technology of the present invention to provide more natural video phone interactions.
      <br/>
      In a particular embodiment, a Proshare 200 or Teamstation 4 system made by Intel, Inc. in Santa Clara, Calif., is used.
    </p>
    <p num="50">
      As shown in FIG. 7, the output from the audio/video transmitter contained within the controller 105 travels via the transmission medium 118 to the remote audio/video receiver 120.
      <br/>
      The transmission medium 118 can be any suitable type of wired or wireless medium using any suitable bandwidth over which information can be transmitted.
      <br/>
      The output through the transmission medium 118 can be sent to the remote audio/video receiver 120 using any suitable type of network operatively connected to the controller 105.
      <br/>
      The same transmission medium 118 and network can also be used to receive signals from any remote audio/video transmitter into a local audio/video receiver for complete videoconferencing capabilities.
      <br/>
      This includes, but is not limited to, a wide area network (WAN), a private branch exchange (PBX), any type of local area network (LAN) together with the appropriate bridges, routers and gateways, and so forth.
      <br/>
      In one embodiment any suitable type of Internet connection is used, such as a conventional dial-up Internet connection, or any type of high speed Internet connection such as an Integrated Services Digital Network (ISDN), and so forth.
      <br/>
      A set of Media Access Control (MAC) protocols, such as Ethernet or Token Ring, can also be used to allow information to flow smoothly over a network.
      <br/>
      In one embodiment, the transmission medium 118 comprises a two-way whole structure Ethernet connection using standard protocol, such as Transmission Control Protocol/Internet Protocol (TCP/IP).
      <br/>
      The signal from the receiver 120 is then output to any type of conventional monitor 124 and speakers 126 so that viewers 122 can see and hear the output.
    </p>
    <p num="51">
      Refinements to the system of the present invention can be made by testing a predetermined speaker array in an anechoic chamber to check and adjust the signal processing algorithm as necessary.
      <br/>
      Further testing can also be performed in one or more "typical" conference rooms to determine the effects of reflection, reverberation, occlusions, and so forth.
      <br/>
      Further adjustments can then be made to the algorithm, the shape of the microphone enhancer, the size and spacing of the ridges, and so forth, as needed.
    </p>
    <p num="52">
      The system of the present invention is more compact and requires less hardware than conventional videoconferencing systems.
      <br/>
      By detecting the source of three-dimensional sounds, rather than determining a difference between two microphone signals, it is likely the system will provide improved response time and accuracy.
      <br/>
      In one embodiment, the steering system 100 can be used to follow individual presenters or talkers by panning and tilting as needed.
      <br/>
      In another embodiment, the tracking system or sound tracker is used to point a camera at any sounds of interest, such as musical sounds.
      <br/>
      In yet another embodiment, the tracking system is used for security purposes and can detect not only the sound of a single voice, but also multiple voices, footsteps, and so forth.
      <br/>
      With appropriate modifications, the sound steering system can also be used in robotic guidance systems.
    </p>
    <p num="53">
      Although specific embodiments have been illustrated and described herein, it will be appreciated by those of ordinary skill in the art that any arrangement which is calculated to achieve the same purpose may be substituted for the specific embodiment shown.
      <br/>
      This application is intended to cover any adaptations or variations of the present invention.
      <br/>
      Therefore, it is manifestly intended that this invention be limited only by the claims and the equivalents thereof.
    </p>
  </description>
  <claims format="original" lang="en" id="claim_en">
    <claim num="1">
      <claim-text>What is claimed is:</claim-text>
      <claim-text>1.</claim-text>
      <claim-text>An apparatus to determine directionality of acoustic signals that arrive from an acoustic source comprising:</claim-text>
      <claim-text>a plurality of reflectors to receive and modify acoustic signals to produce modified acoustic signals, the plurality of reflectors having asymmetric ridges which modify the acoustic signals by creating interference patterns in the signals; a transducer located proximate to the reflectors to receive the modified acoustic signals; a detector operatively connected to the transducer to detect and identify spectral notches in the modified acoustic signals;</claim-text>
      <claim-text>and a device operatively connected to the detector to determine directionality of the acoustic source based upon the identified notches received.</claim-text>
    </claim>
    <claim num="2">
      <claim-text>2. An apparatus according to claim 1 wherein at least two of the plurality of reflectors are located different distances from the transducer.</claim-text>
    </claim>
    <claim num="3">
      <claim-text>3. An apparatus according to claim 2 wherein a portion of each acoustic signal encounters the asymmetric ridges prior to encountering the transducer causing a delay in time before the portion reaches the transducer.</claim-text>
    </claim>
    <claim num="4">
      <claim-text>4. An apparatus according to claim 1 wherein the plurality of reflectors are located on a finite reflecting surface having an elliptical shape, further wherein the transducer is located at the center of the ellipse.</claim-text>
    </claim>
    <claim num="5">
      <claim-text>5. An apparatus according to claim 4 wherein the reflectors are made from injection molded plastic having an absorption coefficient of at least about 0.05 at a frequency of from about 100 Hz to about four (4) kHz.</claim-text>
    </claim>
    <claim num="6">
      <claim-text>6. An apparatus according to claim 1 wherein the device for determining directionality compares the identified notches to previously detected notches to find common notches associated with a particular spatial direction.</claim-text>
    </claim>
    <claim num="7">
      <claim-text>7. An apparatus according to claim 6 further comprising a spectral smoother to eliminate unwanted noise in the acoustic signals.</claim-text>
    </claim>
    <claim num="8">
      <claim-text>8. An apparatus according to claim 7 further comprising a camera tracking system operatively connected to the apparatus wherein output from the device can activate the camera tracking system and cause a video camera to be pointed in the direction of the acoustic source.</claim-text>
    </claim>
    <claim num="9">
      <claim-text>9. A method for determining directionality of acoustic signals arriving from an acoustic source comprising: receiving and modifying acoustic signals using a plurality of reflectors, wherein the plurality of reflectors have asymmetric ridges which modify the acoustic signals by creating interference patterns in the signals; transducing the modified acoustic signals into electrical signals using a transducer located proximate to the reflectors; determining and identifying spectral notches in the modified acoustic signals using a detector operatively connected to the transducer;</claim-text>
      <claim-text>and using a device operatively connected to the detector, determining directionality of the acoustic source based upon the identified notches received.</claim-text>
    </claim>
    <claim num="10">
      <claim-text>10. A method according to claim 9 wherein at least two of the plurality of reflectors are located different distances from the transducer.</claim-text>
    </claim>
    <claim num="11">
      <claim-text>11. A method according to claim 10 wherein a portion of each acoustic signal encounters the asymmetric ridges prior to encountering the transducer causing a delay in time before the portion reaches the transducer.</claim-text>
    </claim>
    <claim num="12">
      <claim-text>12. A method according to claim 9 wherein the plurality of reflectors are located on a finite reflecting surface having an elliptical shape, further wherein the transducer is located at the center of the ellipse.</claim-text>
    </claim>
    <claim num="13">
      <claim-text>13. A method according to claim 12 wherein the reflectors are made from injection molded plastic having an absorption coefficient of at least about 0.05 at a frequency of from about 100 Hz to about four (4) kHz.</claim-text>
    </claim>
    <claim num="14">
      <claim-text>14. A method according to claim 9 wherein the determination of directionality includes comparing the identified notches to previously detected notches to find common notches associated with a particular spatial direction.</claim-text>
    </claim>
    <claim num="15">
      <claim-text>15. A method according to claim 14 further comprising subtracting estimates of unwanted noise from the acoustic signals using a spectral smoother.</claim-text>
    </claim>
    <claim num="16">
      <claim-text>16. A method according to claim 15 further comprising using a camera tracking system having a video camera, the camera tracking system operatively connected to the apparatus, wherein output from the spatial estimator activates the camera tracking system, causing the video camera to be pointed in the direction of the acoustic source.</claim-text>
    </claim>
    <claim num="17">
      <claim-text>17. A method for determining directionality of a sound source, comprising: acquiring a stream of acoustic data containing interference from the sound source; converting the acoustic data into numerical sampling frames; multiplying each frame by a window function; generating a Fourier transform for each frame; searching for spectral notches using the Fourier transform; finding spectral notches in specific areas corresponding to a specific spatial direction; looking up in a table of known locations associated with different spectral notches;</claim-text>
      <claim-text>and outputting a spatial candidate to a camera motor wherein a camera operatively connected to the camera motor is pointed in the direction of the sound source.</claim-text>
    </claim>
    <claim num="18">
      <claim-text>18. A method according to claim 17 wherein the camera records an image associated with the sound source.</claim-text>
    </claim>
    <claim num="19">
      <claim-text>19. A microphone system for use in a conference environment where a three-dimensional acoustic source emits acoustic signals from diverse and varying locations within the environment, comprising: a microphone located at the center of at least one reflecting surface having asymmetric ridges, wherein the acoustic signals which interact with the asymmetric ridges reach the microphone later in time than the acoustic signals which travel a direct path to the microphone, further wherein all acoustic signals reaching the microphone are transduced to electrical signals;</claim-text>
      <claim-text>and control circuity configured to analyze the electrical signals to determine an angular orientation of the acoustic source in relation to the microphone.</claim-text>
    </claim>
    <claim num="20">
      <claim-text>20. A system according to claim 19 wherein the control circuitry provides information on the angular orientation to a video tracking system having a camera, further wherein the video tracking system causes the camera to transmit an image associated with the acoustic source from a transmitter to a remote receiver.</claim-text>
    </claim>
    <claim num="21">
      <claim-text>21. A microphone system according to claim 20 wherein the control circuitry includes an audio compressor which provides a compressed audio signal to the transmitter.</claim-text>
    </claim>
    <claim num="22">
      <claim-text>22. A sound steering system comprising: a microphone enhancer for determining directionality of a sound source, the microphone enhancer comprising at least two reflectors having asymmetric ridges, each reflector located on opposite sides of a microphone;</claim-text>
      <claim-text>and a controller coupled to the microphone to receive and transform electrical signals from the microphone into an output signal containing directional information of the sound source.</claim-text>
    </claim>
    <claim num="23">
      <claim-text>23. A system according to claim 22 further comprising a camera tracking system having a video camera, the camera tracking system operatively connected to the controller, wherein the controller output signal activates the camera tracking system, causing the video camera to be pointed in the direction of the sound source.</claim-text>
    </claim>
    <claim num="24">
      <claim-text>24. A computer readable medium having instructions for instructing a computer to perform the method of: determining and identifying spectral notches in a stream of modified acoustic data from an acoustic source, the acoustic data modified through interaction with two or more reflectors comprised of asymmetric ridges;</claim-text>
      <claim-text>and determining directionality of the acoustic source based upon the identified notches received.</claim-text>
    </claim>
    <claim num="25">
      <claim-text>25. A method for instructing a computer using a computer readable medium having instructions, comprising the steps of: determining and identifying spectral notches in a stream of modified acoustic data from an acoustic source, the acoustic data modified through interaction with two or more reflectors comprised of asymmetric ridges;</claim-text>
      <claim-text>and determining directionality of the acoustic source based upon the identified notches received.</claim-text>
    </claim>
    <claim num="26">
      <claim-text>26. The method as recited in claim 23 further comprising a spectral smoother to subtract estimates of unwanted noise from the acoustic source.</claim-text>
    </claim>
    <claim num="27">
      <claim-text>27. A method according to claim 24 further comprising smoothing the modified acoustic signal with a spectral smoother.</claim-text>
    </claim>
    <claim num="28">
      <claim-text>28. A method according to claim 24 wherein the acoustic source is a microphone.</claim-text>
    </claim>
    <claim num="29">
      <claim-text>29. A method according to claim 25 further comprising smoothing the modified acoustic signal with a spectral smoother.</claim-text>
    </claim>
    <claim num="30">
      <claim-text>30. A method according to claim 25 wherein the acoustic source is a microphone.</claim-text>
    </claim>
  </claims>
</questel-patent-document>