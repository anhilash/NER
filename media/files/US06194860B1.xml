<?xml version="1.0" encoding="UTF-8"?>
<questel-patent-document lang="en" date-produced="20180805" produced-by="Questel" schema-version="3.23" file="US06194860B1.xml">
  <bibliographic-data lang="en">
    <publication-reference publ-desc="Granted patent as first publication">
      <document-id>
        <country>US</country>
        <doc-number>06194860</doc-number>
        <kind>B1</kind>
        <date>20010227</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>US6194860</doc-number>
      </document-id>
    </publication-reference>
    <original-publication-kind>B1</original-publication-kind>
    <application-reference is-representative="YES" family-id="23709741" extended-family-id="42115123">
      <document-id>
        <country>US</country>
        <doc-number>09430940</doc-number>
        <kind>A</kind>
        <date>19991101</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>1999US-09430940</doc-number>
      </document-id>
      <document-id data-format="questel_Uid">
        <doc-number>43173961</doc-number>
      </document-id>
    </application-reference>
    <language-of-filing>en</language-of-filing>
    <language-of-publication>en</language-of-publication>
    <priority-claims>
      <priority-claim kind="national" sequence="1">
        <country>US</country>
        <doc-number>43094099</doc-number>
        <kind>A</kind>
        <date>19991101</date>
        <priority-active-indicator>Y</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="1">
        <doc-number>1999US-09430940</doc-number>
      </priority-claim>
    </priority-claims>
    <dates-of-public-availability>
      <publication-of-grant-date>
        <date>20010227</date>
      </publication-of-grant-date>
    </dates-of-public-availability>
    <classifications-ipcr>
      <classification-ipcr sequence="1">
        <text>B25J   5/00        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>B</section>
        <class>25</class>
        <subclass>J</subclass>
        <main-group>5</main-group>
        <subgroup>00</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="2">
        <text>B25J   9/16        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>B</section>
        <class>25</class>
        <subclass>J</subclass>
        <main-group>9</main-group>
        <subgroup>16</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="3">
        <text>B25J  19/02        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>B</section>
        <class>25</class>
        <subclass>J</subclass>
        <main-group>19</main-group>
        <subgroup>02</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
    </classifications-ipcr>
    <classification-national>
      <country>US</country>
      <main-classification>
        <text>318587000</text>
        <class>318</class>
        <subclass>587000</subclass>
      </main-classification>
      <further-classification sequence="1">
        <text>318586000</text>
        <class>318</class>
        <subclass>586000</subclass>
      </further-classification>
      <further-classification sequence="2">
        <text>700254000</text>
        <class>700</class>
        <subclass>254000</subclass>
      </further-classification>
      <further-classification sequence="3">
        <text>700259000</text>
        <class>700</class>
        <subclass>259000</subclass>
      </further-classification>
      <further-classification sequence="4">
        <text>901001000</text>
        <class>901</class>
        <subclass>001000</subclass>
      </further-classification>
      <further-classification sequence="5">
        <text>901047000</text>
        <class>901</class>
        <subclass>047000</subclass>
      </further-classification>
    </classification-national>
    <classifications-ecla>
      <classification-ecla sequence="1">
        <text>B66F-009/075F</text>
        <section>B</section>
        <class>66</class>
        <subclass>F</subclass>
        <main-group>009</main-group>
        <subgroup>075F</subgroup>
      </classification-ecla>
      <classification-ecla sequence="2">
        <text>B25J-005/00W</text>
        <section>B</section>
        <class>25</class>
        <subclass>J</subclass>
        <main-group>005</main-group>
        <subgroup>00W</subgroup>
      </classification-ecla>
      <classification-ecla sequence="3">
        <text>B25J-009/16V1</text>
        <section>B</section>
        <class>25</class>
        <subclass>J</subclass>
        <main-group>009</main-group>
        <subgroup>16V1</subgroup>
      </classification-ecla>
      <classification-ecla sequence="4">
        <text>B25J-019/02B4</text>
        <section>B</section>
        <class>25</class>
        <subclass>J</subclass>
        <main-group>019</main-group>
        <subgroup>02B4</subgroup>
      </classification-ecla>
    </classifications-ecla>
    <patent-classifications>
      <patent-classification sequence="1">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>B66F-009/0755</classification-symbol>
        <section>B</section>
        <class>66</class>
        <subclass>F</subclass>
        <main-group>9</main-group>
        <subgroup>0755</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="2">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>B25J-005/007</classification-symbol>
        <section>B</section>
        <class>25</class>
        <subclass>J</subclass>
        <main-group>5</main-group>
        <subgroup>007</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="3">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>B25J-009/1697</classification-symbol>
        <section>B</section>
        <class>25</class>
        <subclass>J</subclass>
        <main-group>9</main-group>
        <subgroup>1697</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="4">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>B25J-019/023</classification-symbol>
        <section>B</section>
        <class>25</class>
        <subclass>J</subclass>
        <main-group>19</main-group>
        <subgroup>023</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="5">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>G05B-2219/39391</classification-symbol>
        <section>G</section>
        <class>05</class>
        <subclass>B</subclass>
        <main-group>2219</main-group>
        <subgroup>39391</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="6">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>G05B-2219/40513</classification-symbol>
        <section>G</section>
        <class>05</class>
        <subclass>B</subclass>
        <main-group>2219</main-group>
        <subgroup>40513</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="7">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>G05B-2219/40564</classification-symbol>
        <section>G</section>
        <class>05</class>
        <subclass>B</subclass>
        <main-group>2219</main-group>
        <subgroup>40564</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="8">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>G05B-2219/40604</classification-symbol>
        <section>G</section>
        <class>05</class>
        <subclass>B</subclass>
        <main-group>2219</main-group>
        <subgroup>40604</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="9">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>G05B-2219/40611</classification-symbol>
        <section>G</section>
        <class>05</class>
        <subclass>B</subclass>
        <main-group>2219</main-group>
        <subgroup>40611</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="10">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>G05B-2219/45049</classification-symbol>
        <section>G</section>
        <class>05</class>
        <subclass>B</subclass>
        <main-group>2219</main-group>
        <subgroup>45049</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="11">
        <classification-scheme office="EP" scheme="ICO"/>
        <classification-symbol>S05B-219/39391</classification-symbol>
      </patent-classification>
      <patent-classification sequence="12">
        <classification-scheme office="EP" scheme="ICO"/>
        <classification-symbol>S05B-219/40513</classification-symbol>
      </patent-classification>
      <patent-classification sequence="13">
        <classification-scheme office="EP" scheme="ICO"/>
        <classification-symbol>S05B-219/40564</classification-symbol>
      </patent-classification>
      <patent-classification sequence="14">
        <classification-scheme office="EP" scheme="ICO"/>
        <classification-symbol>S05B-219/40604</classification-symbol>
      </patent-classification>
      <patent-classification sequence="15">
        <classification-scheme office="EP" scheme="ICO"/>
        <classification-symbol>S05B-219/40611</classification-symbol>
      </patent-classification>
      <patent-classification sequence="16">
        <classification-scheme office="EP" scheme="ICO"/>
        <classification-symbol>S05B-219/45049</classification-symbol>
      </patent-classification>
    </patent-classifications>
    <number-of-claims>18</number-of-claims>
    <exemplary-claim>1</exemplary-claim>
    <figures>
      <number-of-drawing-sheets>4</number-of-drawing-sheets>
      <number-of-figures>4</number-of-figures>
      <image-key data-format="questel">US6194860</image-key>
    </figures>
    <invention-title format="original" lang="en" id="title_en">Mobile camera-space manipulation</invention-title>
    <references-cited>
      <citation srep-phase="examiner">
        <patcit num="1">
          <text>BIRK JOHN R, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4146924</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4146924</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="2">
          <text>NAKAGAWA YASUO, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4575304</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4575304</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="3">
          <text>CHEN RICHARD M</text>
          <document-id>
            <country>US</country>
            <doc-number>4613942</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4613942</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="4">
          <text>LEMELSON JEROME H</text>
          <document-id>
            <country>US</country>
            <doc-number>4636137</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4636137</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="5">
          <text>DAY CHIA P, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4639878</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4639878</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="6">
          <text>PERDUE TERRY A</text>
          <document-id>
            <country>US</country>
            <doc-number>4679152</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4679152</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="7">
          <text>GEORGE SATISH, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4754415</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4754415</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="8">
          <text>CHRISTIAN DONALD J</text>
          <document-id>
            <country>US</country>
            <doc-number>4789940</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4789940</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="9">
          <text>PRYOR TIMOTHY R</text>
          <document-id>
            <country>US</country>
            <doc-number>4796200</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4796200</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="10">
          <text>KADONOFF MARK B, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4815008</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4815008</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="11">
          <text>CONWAY LYNN A, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5046022</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5046022</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="12">
          <text>LANZA FABRIZIO, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5938710</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5938710</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="13">
          <text>JACKSON BERNIE FERGUS</text>
          <document-id>
            <country>US</country>
            <doc-number>5943783</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5943783</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="14">
          <text>SKARR STEVEN B, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4833383</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4833383</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="15">
          <text>SKAAR STEVEN B, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5300869</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5300869</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="16">
          <text>TSUGE HIROSHI, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5523663</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5523663</doc-number>
          </document-id>
        </patcit>
      </citation>
    </references-cited>
    <parties>
      <applicants>
        <applicant data-format="original" app-type="applicant" sequence="1">
          <addressbook lang="en">
            <orgname>Yoder Software, Inc.</orgname>
            <address>
              <address-1>Urbana, IL, US</address-1>
              <city>Urbana</city>
              <state>IL</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </applicant>
        <applicant data-format="questel" app-type="applicant" sequence="2">
          <addressbook lang="en">
            <orgname>YODER SOFTWARE</orgname>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </applicant>
      </applicants>
      <inventors>
        <inventor data-format="original" sequence="1">
          <addressbook lang="en">
            <name>Seelinger, Michael J.</name>
            <address>
              <address-1>Urbana, IL, US</address-1>
              <city>Urbana</city>
              <state>IL</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
        <inventor data-format="original" sequence="2">
          <addressbook lang="en">
            <name>Yoder, John-David S.</name>
            <address>
              <address-1>Bluffton, OH, US</address-1>
              <city>Bluffton</city>
              <state>OH</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
        <inventor data-format="original" sequence="3">
          <addressbook lang="en">
            <name>Skaar, Steven B.</name>
            <address>
              <address-1>South Bend, IN, US</address-1>
              <city>South Bend</city>
              <state>IN</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
      </inventors>
      <agents>
        <agent sequence="1" rep-type="agent">
          <addressbook lang="en">
            <orgname>Berns Law Office</orgname>
          </addressbook>
        </agent>
        <agent sequence="2" rep-type="agent">
          <addressbook lang="en">
            <name>Berns, Michael</name>
          </addressbook>
        </agent>
      </agents>
    </parties>
    <examiners>
      <primary-examiner>
        <name>Ip, Paul</name>
      </primary-examiner>
    </examiners>
    <lgst-data>
      <lgst-status>LAPSED</lgst-status>
    </lgst-data>
  </bibliographic-data>
  <abstract format="original" lang="en" id="abstr_en">
    <p id="P-EN-00001" num="00001">
      <br/>
      The invention is a method of using computer vision to control systems consisting of a combination of holonomic and nonholonomic degrees of freedom such as a wheeled rover equipped with a robotic arm, a forklift, and earth-moving equipment such as a backhoe or a front-loader.
      <br/>
      Using vision sensors mounted on the mobile system and the manipulator, the system establishes a relationship between the internal joint configuration of the holonomic degrees of freedom of the manipulator and the appearance of features on the manipulator in the reference frames of the vision sensors.
      <br/>
      Then, the system, perhaps with the assistance of an operator, identifies the locations of the target object in the reference frames of the vision sensors.
      <br/>
      Using this target information, along with the relationship described above, the system determines a suitable trajectory for the nonholonomic degrees of freedom of the base to follow towards the target object.
      <br/>
      The system also determines a suitable pose or series of poses for the holonomic degrees of freedom of the manipulator.
      <br/>
      With additional visual samples, the system automatically updates the trajectory and final pose of the manipulator so as to allow for greater precision in the overall final position of the system.
    </p>
  </abstract>
  <description format="original" lang="en" id="desc_en">
    <heading>STATEMENT OF GOVERNMENT RIGHTS</heading>
    <p num="1">The work disclosed in this application was supported in part by a grant from the NASA-SBIR program to Yoder Software, Inc., therefore, the U.S. Government may have some rights in the present invention.</p>
    <heading>BACKGROUND OF THE INVENTION</heading>
    <p num="2">
      1.
      <br/>
      Field of the Invention
    </p>
    <p num="3">
      The invention relates to a practical means of using computer vision to control systems consisting of a combination of holonomic and nonholonomic degrees of freedom in order to perform user-designated operations on stationary objects.
      <br/>
      Examples of combination holonomic/nonholonomic systems are a wheeled rover equipped with a robotic arm, a forklift, and earth-moving equipment such as a backhoe or a front-loader and even an underwater vehicle with attached robotic arm.
    </p>
    <p num="4">
      The present invention eliminates the need for direct, ongoing human participation in the control loop for completing a given task such as engaging a pallet with a forklift.
      <br/>
      Whereas, depending upon the application of the present invention, the human may supply some high-level supervision for the system such as "engage pallet," the effect of the new art is to create fully autonomous response of the system, synchronized between control of the holonomic and nonholonomic degrees of freedom, which produces effective, precise, reliable and robust direction and control of the mechanism without any subsequent human intervention.
    </p>
    <p num="5">2. References</p>
    <p num="6">The remainder of this specification refers to various individual publications listed below by number by reciting, for example, "�1�", or "�2�", and so forth.</p>
    <p num="7">�1� E. Gonzalez-Galvan and S. B. Skaar, "Application of a precision enhancing measure in 3-D rigid-body positioning using camera-space manipulation," International Journal of Robotics Research, Vol. 16, No. 2, pp. 240-257, 1997.</p>
    <p num="8">�2� B. Horn, Robot Vision, MIT Press, Cambridge, 1986.</p>
    <p num="9">�3� M. Seelinger, S. B. Skaar, and M. Robinson, "An Alternative Approach for Image-Plane Control of Robots," Lecture Notes in Control and Information Sciences, Eds. D. Kriegman, G. Hager, and S. Morse, pp. 41-65, Springer, London, 1998.</p>
    <p num="10">
      �4� E. Gonzalez-Galvan and S. B. Skaar, "Servoable cameras for three dimensional positioning with camera-space manipulation," Proc.
      <br/>
      LASTED Robotics and Manufacturing, pp. 260-265, 1995.
    </p>
    <p num="11">�5� S. B. Skaar, I. Yalda-Mooshabad, and W. H, Brockman, "Nonholonomic camera-space manipulation," IEEE Trans. on Robotics and Automation, Vol. 13, No. 3, pp. 464-479, August 1992.</p>
    <p num="12">�6� R. K. Miller, D. G. Stewart, H. Brockman, and S. B. Skaar, "A camera space control system for an automated forklift," IEEE Trans. on Robotics and Automation, Vol. 10, No. 5, pp. 710-716, October 1994.</p>
    <p num="13">�7� Y. Hwang. "Motion Planning of a Robotic Arm on a Wheeled Vehicle on a Rugged Terrain," L. A. Demsetz, ed., Robotics for Challenging Environments, Proc. of RCE II, pp. 57-63, 1996.</p>
    <p num="14">
      �8� T. Lueth, U. Nassal, U. Rembold. "Reliability and Integrated Capabilities of Locomotion and Manipulation for Autonomous Robot Assembly," Robotics and Autonomous Systems.
      <br/>
      Vol. 14, No.2-3, pp. 185-198, May 1995.
    </p>
    <p num="15">
      �9� MacKenzie, D. and Arkin, R. "Behavior-Based Mobile Manipulations for Drum Sampling," Proceedings of the 1996 IEEE Int.
      <br/>
      Conf. On Robotics and Automation, pp 2389-2395, April 1996.
    </p>
    <p num="16">
      �10� C. Perrier, P. Dauchez, F. Pierrot. "A Global Approach for Motion Generation of Non-Holonomic Mobile Manipulators, " Proc.
      <br/>
      IEEE Int. Conference on Robotics and Automation pp. 2971-2976, 1998.
    </p>
    <p num="17">�11� O. Khatib, "Mobile manipulation: The robotic assistant," Robotics and Autonomous Systems, Vol. 26, pp. 175-183, 1999.</p>
    <p num="18">3. Nomenclature</p>
    <p num="19">The following is a summary of notation used in this specification:</p>
    <p num="20">Cj =�C1j,C2j, . . . ,C6j �T view parameters for camera j</p>
    <p num="21">THETA =� THETA 1, THETA 2, . . . , THETA n �T internal joint configuration of an n-degree of freedom system</p>
    <p num="22">(xci j, yci j) camera space location of point i in camera j</p>
    <p num="23">(fx,fy) orthographic camera model</p>
    <p num="24">J1,J2,J3 scalar quantities minimized to estimate various parameters</p>
    <p num="25">ncam number of cameras in system</p>
    <p num="26">nc (j) number of visual features used in any given summation</p>
    <p num="27">p number of poses in the pre-plan trajectory</p>
    <p num="28">Wik relative weight given to each visual sample</p>
    <heading>DESCRIPTION OF THE PRIOR ART</heading>
    <p num="29">
      1.
      <br/>
      Camera-Space Manipulation
    </p>
    <p num="30">
      Camera-space manipulation, hereafter referred to as CSM, was developed as a means of achieving highly precise control of the positioning and orienting of robotic manipulators in the presence of uncertainties in the workspace of the robot.
      <br/>
      These uncertainties include such things as kinematic errors, kinematic changes due to temperature changes or dynamic loads, or workpieces in unknown or varying positions. U.S. Pat. No. 4,833,383 to Skaar et al., describes CSM.
      <br/>
      CSM uses computer vision in order to enable a manipulator's load or tool to be positioned and oriented highly accurately relative to an arbitrarily positioned and oriented workpiece.
      <br/>
      This high accuracy is extremely robust to uncertainties in the robot's workspace.
      <br/>
      CSM neither relies on the calibration of the camera(s) nor the robot.
      <br/>
      CSM works in an open-loop fashion, thus real-time image processing is not required.
    </p>
    <p num="31">
      CSM can operate in a fully autonomous fashion or with supervisory control.
      <br/>
      A graphical user interface was developed for use with CSM.
      <br/>
      Through this interface, the user can view either a live or still-frame image of the workspace of the robot.
      <br/>
      By clicking on this image, the user selects the surface, or region or juncture on the surface, upon which the operation will be performed.
      <br/>
      The user also selects the type of task to be performed by the robot from the interface program Additionally, the user sets other operating parameters such as the speed of the manipulator.
    </p>
    <p num="32">2. Description of CSM</p>
    <p num="33">
      CSM works by establishing a relationship between the appearance of image-plane visual features located on the manipulator with the internal joint configuration of the robot.
      <br/>
      If the positioning task involves more than two dimensions, then at least two cameras must be used.
      <br/>
      The relationship, described with a set of view parameters given by C=�C1,C2, . . . ,C6 �T, is determined for each of the participating cameras.
      <br/>
      This relationship is based on the orthographic camera model:
      <br/>
      xcj =((C1j)2 +(C2j)2 -(C3j)2 -(C4j)2)X+2(C2j C3j +C1j C4j)Y+2(C2j C4j -C1j C3j)Z+C5j
      <br/>
      ycj =2(C2j C3j -C1j C4j)X+((C1j)2 -(C2j)2 +(C3j)2 -(C4j))2 Y+2(C3j C4j +C1j C2j)Z+C6j   (1)
    </p>
    <p num="34">
      where (xcj, ycj) represents the estimated image-plane location of a feature on the robot's end effector in the jth participating camera.
      <br/>
      The position vector (X,Y,Z), describes the location of the manipulator feature relative to a reference frame tied to the robot.
      <br/>
      It is a function of the internal joint configuration,  THETA =� THETA 1, THETA 2, . . . , THETA n �T for an n degree-of-freedom robot, and the model of the robot's forward kinematics.
      <br/>
      For convenience, Eq. (1) is rewritten as:
      <br/>
      xcj =fx ( THETA , Cj)
      <br/>
      ycj =fy ( THETA , Cj)
    </p>
    <p num="35">
      The view parameters are initialized through a process called the pre-plan trajectory.
      <br/>
      During the pre-plan trajectory, the robot is driven to a set number of poses (between 10 and 20), spanning both a large region of the robot's joint space as well as wide regions of the camera spaces.
      <br/>
      At each of these poses, images are acquired in all participating cameras and the locations of the designated manipulator features are found in each of these images.
      <br/>
      Then the view parameters for the jth camera are estimated by minimizing over all C=�C1,C2, . . . ,C6 �T :  (Equation image '1' not included in text)
    </p>
    <p num="36">
      where p is the number of poses in the pre-plan trajectory, nc (k) is the number of features found in the image corresponding to camera j for pose number k, Wik is the relative weight given to feature number i in pose number k, and (xci j, yci j) represents the actual image-plane location of the ith cue located on the robot's end effector in the jth participating camera.
      <br/>
      The weighting factor might be chosen based on the location of a given feature with respect to the end of the manipulator.
      <br/>
      For instance, features located closer to a tool held in the manipulator might receive more relative weight than those located farther away from it.
    </p>
    <p num="37">
      Once the view parameters have been established in each of the participating CSM cameras, it is possible to use the relationship described by Eq. (1) to position the robot.
      <br/>
      In order to accomplish this, it is necessary to create or determine compatible camera-space targets. "Compatible" targets refer to how well a given target point in the reference frames of the different cameras represents the same physical three-dimensional surface point.
      <br/>
      If a sufficient number of camera-space targets are determined, then the internal joint configuration of the robot, can be found by minimizing J2 over all  THETA :  (Equation image '2' not included in text)
    </p>
    <p num="38">
      where ncam is the number of participating cameras and nc (j) is the number of camera-space target pairs, (xci j,yci j).
      <br/>
      The three-dimensional point, is measured relative to the coordinate system attached to the robot and is a function of the internal joint configuration of the robot,  THETA , and refers to the location of the ith target point in the robot reference frame that is to be aligned with the camera-space target points, (xci j,yci j), where j=1, . . . ,ncam.
      <br/>
      In order to achieve three-dimensional positioning of the manipulator, it is necessary to have one set of camera-space targets in at least 2 cameras.
      <br/>
      To achieve both three-dimensional position and orientation of the manipulator more sets of camera-space targets are necessary.
      <br/>
      Consider, for instance, the task of engaging a pallet with a forklift tool mounted on a robot.
      <br/>
      In this case, both position and orientation are critical to complete the task successfully.
      <br/>
      In light of the pallet-engaging task, one way to look at Eq. (3) is to ask the following question: Where do the cues on the manipulator have to appear in the camera-spaces in order for the forklift to engage the pallet? The camera-space locations of the cues, the answer to the preceding question, become the sets of camera-space targets, (xci j,yci j), which are used in Eq. (3) to resolve the robot joint rotations.
    </p>
    <p num="39">
      As mentioned above, Eq. (1) is based on the rather simple orthographic camera model.
      <br/>
      It is well known that the orthographic camera model is far from a perfect description of the mapping from a three-dimensional reference frame into a two-dimensional image.
      <br/>
      CSM takes advantage of estimation to compensate for an admittedly flawed camera model as well as other model imperfections, such as the robot's forward kinematics, and is thus able to achieve a high level of precision in positioning the robot.
      <br/>
      The view parameters can be updated with as many new sample pairs of visual images and robot joint poses as become available.
      <br/>
      As the manipulator moves towards its terminal position, camera samples are taken.
      <br/>
      In these images, the manipulator is located in a much smaller or local region of camera space and manipulator joint space.
      <br/>
      The view parameters are updated with this local information, which is given more relative weight than the samples obtained during the pre-plan trajectory.
      <br/>
      With the emphasis on the local information, the view parameters accurately map from three-dimensions into two-dimensions in the local region.
    </p>
    <p num="40">
      A process called flattening is another measure that has been taken in order to improve the overall precision of the system �1�. Using the parameters of the orthographic camera model and minimal additional information, it is possible to obtain identical results as would have been achieved by using the more accurate pinhole camera model.
      <br/>
      The process of flattening becomes more important as the perspective effect �2� becomes more dominant.
      <br/>
      For instance, when a camera is located at a large distance from a small workpiece, the perspective effect is minimal.
      <br/>
      However, as the workpiece is moved closer to the camera, the perspective effect becomes more noticeable thus making the flattening process more important.
    </p>
    <p num="41">3. Estimation in CSM</p>
    <p num="42">
      The use of estimation as a basis for doing CSM has many advantages over other vision guided robotic strategies.
      <br/>
      For instance, neither the cameras nor the robot need be calibrated in order for CSM to work.
      <br/>
      Problems with calibration involve the initial determination of internal camera parameters, internal robot parameters, and the parameters describing the relationship between the camera(s) and the robot.
      <br/>
      Additionally, it is extremely difficult, if not impossible, to maintain this calibrated relationship, especially when the environment itself is hostile to the system.
      <br/>
      CSM makes use of the best kinematic model available for the robot in conjunction with the video samples acquired in a normal positioning maneuver to describe the relationship between the internal joint configuration of the robot and the image-plane appearance of manipulator features.
      <br/>
      The estimation of this relationship (the view parameters) involves giving samples in the local joint-space and camera-space regions more relative weight.
      <br/>
      This skewed weighting allows the system to achieve a high level of accuracy in positioning in this region.
      <br/>
      The fact of the matter is that due to inaccuracies in the orthographic camera model, errors in the robot's kinematics, and other system errors, the view parameters do not do a perfect job of describing the relationship between the internal joint configuration of the robot and the image-plane appearance of manipulator features for a wide range of joint and camera space.
      <br/>
      Using estimation and skewing the weighting of samples to the local operating regions of camera and joint space overcome these errors by ensuring that these errors are insignificant in the region of interest.
      <br/>
      Application of the flattening process described above makes this process even more precise.
    </p>
    <p num="43">
      Another advantage that CSM has over other vision-guided control strategies is that it does not rely upon constant visual access to the target workpiece.
      <br/>
      If the target becomes obscured during the positioning trajectory, which is often caused by the robot itself, CSM still is able to complete the task �3�. During the positioning trajectory of the robot there are many factors that can cause the surface of interest to become obscured from the view of one of the cameras.
      <br/>
      When some type of visual obstruction occurs, the CSM-driven system continues the positioning trajectory using the best information available.
      <br/>
      In the event that more information becomes available through additional camera samples, this information is used to update the view parameters, and in turn, the target terminal pose.
      <br/>
      Related to this advantage is the fact that CSM does not rely upon real-time image processing.
      <br/>
      New information acquired by the system as it moves towards the target terminal pose is incorporated, as it becomes available.
      <br/>
      The time delay between when the image was acquired and when the information is used does not negatively affect system performance.
    </p>
    <p num="44">4. CSM's Limitation to Fixed-Cameras</p>
    <p num="45">
      As mentioned previously, CSM does not rely on camera calibration.
      <br/>
      The position of each camera need not be known by the system.
      <br/>
      Any of a wide variety of camera positions provides the same highly accurate results in positioning the robot.
      <br/>
      For a given camera/robot setup, the pre-plan trajectory provides sufficient information for establishing the relationship described by the view parameters.
      <br/>
      If a given camera is moved after the pre-plan trajectory, it is necessary to rerun the pre-plan trajectory.
      <br/>
      It may be possible to avoid this step if the system knows approximately how much the camera has moved.
      <br/>
      This is the case with cameras mounted on pan/tilt units where it is possible to monitor the changes in the pan and tilt angles �4�.
    </p>
    <p num="46">
      While CSM does provide considerable flexibility in camera position, it has been limited to using fixed cameras.
      <br/>
      Once the view parameters are initialized for a given configuration, traditional CSM mandates that the cameras remain in place.
      <br/>
      While this is fine for holonomic manipulators with fixed workspaces, this is a severe limitation for mobile, nonholonomic systems.
      <br/>
      CSM was previously used with cameras fixed on the wall to control a nonholonomic system �5,6�. However, since the cameras were fixed on the wall, the system could only function in a limited workspace.
      <br/>
      Accuracy was dependent on distance from the cameras.
      <br/>
      Additionally, such a system would be impractical for `exploring` robots, such as the NASA Sojourner, which is working in an environment where widely separated fixed cameras are not available.
    </p>
    <p num="47">
      As discussed in the previous section, CSM has proven to be a highly accurate, robust, and calibration-free means of controlling manipulators.
      <br/>
      It has even been used to control a nonholonomic robot �5,6�. However, it has always been limited by its need for fixed cameras.
    </p>
    <p num="48">
      U.S. Pat. No. 5,300,869 to Skaar et al., discloses a system for controlling a nonholonomic robot with fixed cameras.
      <br/>
      While this need does not present a problem for holonomic systems, it becomes a major limitation when dealing with nonholonomic systems.
      <br/>
      At best, the need for fixed cameras limits the workspace, while at worst, it makes use of the system impossible, since in many of the environments where such systems might be used, for example hazardous environments and space exploration, it is difficult or impossible to preposition fixed cameras.
    </p>
    <p num="49">
      Also, it has been noticed that most of the algorithms for controlling holonomic/nonholonomic systems had not been making the best possible use of the systems �7-11�. That is, in most cases, the problem was broken down to first position the vehicle close to the target, and then control the high degree of freedom arm to complete the task.
      <br/>
      While this does provide a means for controlling such systems, it wastes several degrees of freedom.
      <br/>
      This becomes especially important for space payloads, where extra degrees of freedom means extra cost and weight, and reduces reliability.
      <br/>
      It is also important for potential commercial retrofits.
      <br/>
      For example, existing algorithms could never be used to retrofit a standard forklift, since the design of the forklift requires that the vehicle be in a particular location to engage the pallet correctly.
      <br/>
      That is, one cannot simply move the forklift close to the pallet, and expect the lift mechanism to be able to engage the pallet.
      <br/>
      Rather, the vehicle must be maneuvered with the limited degrees of freedom of the lift mechanism in mind in order to be able to engage the pallet successfully.
    </p>
    <p num="50">
      U.S. Pat. No. 5,523,663 to Tsuge, et al., attempts to address the fixed-camera problem of controlling a nonholonomic/holonomic system.
      <br/>
      It is limited to using a single camera mounted vertically on the end of a manipulator.
      <br/>
      This system would not be useful in most applications.
    </p>
    <heading>SUMMARY OF THE INVENTION</heading>
    <p num="51">
      The present invention, known as mobile camera-space manipulation, hereinafter referred to as MCSM, overcomes the fixed-camera limitation of traditional CSM in order to make control of holonomic/nonholonomic systems possible, and efficient.
      <br/>
      It is an object of the present invention to provide an accurate, robust, calibration-free means of controlling the entire holonomic/nonholonomic system with this objective in mind.
      <br/>
      This has the benefit of increasing the workspace of the mobile system, which would then be limited only by the area through which the MCSM system could traverse.
    </p>
    <p num="52">
      Additionally, MCSM retains the benefits of standard CSM.
      <br/>
      For instance, precise calibration of the cameras, and the system kinematics is unnecessary.
      <br/>
      Such a calibration would be difficult to maintain if a mobile system were operating in a challenging environment such as in planetary surface exploration.
      <br/>
      Likewise, MCSM is able to achieve a high level of precision in positioning the system relative to an arbitrarily located target object.
      <br/>
      Finally, MCSM makes efficient use of all of the degrees of freedom of the system such that a minimal number of holonomic degrees of freedom are required to accomplish any given task.
    </p>
    <p num="53">
      There are many existing cases of human-controlled systems which are comprised of a combination of nonholonomic mobility and an on-board, holonomic manipulator, including forklifts and backhoes.
      <br/>
      The ongoing need to maintain human concentration throughout execution of a trajectory in order to engage, move or position an object or objects, and the susceptibility to human error that this requirement causes, is reason to seek a practical solution that is fully autonomous or semi-autonomous with human supervisory control only.
      <br/>
      Moreover, because of the difficulty of a human operator to produce highly accurate control of such a system a variety of tasks that might otherwise be economically performed using a nonholonomic system are not now performed.
      <br/>
      The present invention, by contrast, achieves a level of precision well in excess of what a human operator could be expected to do with such hardware.
      <br/>
      Therefore, new applications may well arise once the autonomous capability is established.
    </p>
    <p num="54">
      Additionally, MCSM could be used to remotely operate such combination holonomic/nonholonomic systems.
      <br/>
      This would be particularly useful in hazardous or challenging environments such as mine removal, hazardous waste clean up, or planetary surface exploration.
    </p>
    <p num="55">
      The invention is a method of using computer vision to control systems consisting of a combination of holonomic and nonholonomic degrees of freedom.
      <br/>
      Briefly, holonomic systems are systems, such as typical robotic arms, for which the relationship between end effector position and joint rotations can be expressed algebraically.
      <br/>
      On the other hand, in nonholonomic systems, such as typical mobile robots, this end effector/joint rotation relationship is not simply algebraic, rather it is differential.
      <br/>
      One of the ramifications of the differences between holonomic and nonholonomic systems, is that the overall position of the end effector of a nonholonomic system is dependent upon the path that the joints take in moving towards their final position.
      <br/>
      In the case of the holonomic system, given that the joints are in a certain pose, the end effector will always be in the same physical position regardless of what path the joints took in moving to this particular pose.
    </p>
    <p num="56">
      Clearly many mobile systems need a holonomic manipulator to do useful work.
      <br/>
      Examples of this are the fork manipulator on a typical forklift and the arm of a backhoe.
      <br/>
      In most instances, these combination holonomic/nonholonomic system require that the operator control all of the joints of the system directly.
      <br/>
      This is the case with the forklift, where the operator drives the vehicle and controls the height and tilt of the fork directly.
      <br/>
      The ongoing need to maintain human concentration throughout execution of a trajectory in order to engage, move or position an object or objects, and the susceptibility to human error that this requirement causes, is reason to seek a practical solution that is fully autonomous or semi-autonomous with human supervisory control only.
      <br/>
      Moreover, because of the difficulty of a human operator to produce highly accurate control of such a system a variety of tasks that might otherwise be economically performed using a nonholonomic system are not now performed.
      <br/>
      The present invention, by contrast, achieves a level of precision well in excess of what a human operator could be expected to do with such hardware.
    </p>
    <p num="57">
      There has been some research in the field of automatically controlling combination holonomic/nonholonomic systems.
      <br/>
      Much of the work done in the control of such systems treats the control of the two systems completely separately--that is, the mobile or nonholonomic base is moved into a position close to the target, and then a high degree of freedom, holonomic manipulator is used to complete the task.
      <br/>
      These systems have to deal with resolving redundant degrees of freedom, and thus, in some sense, these approaches waste the degrees of freedom of the nonholonomic base.
      <br/>
      The present invention provides a means of controlling both the nonholonomic base and the holonomic manipulator together such that only a minimal number of holonomic degrees of freedom are necessary to achieve any given task.
    </p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading>
    <p num="58">
      FIG. 1 is a block diagram of a process of the invention.
      <br/>
      FIG. 2 is a diagram of a prior art camera-space manipulation, using fixed cameras, a holonomic robot of at least one degree of freedom, and a computer equipped with suitable hardware to control the cameras as well as the robot.
      <br/>
      FIG. 3 is a diagram of a rover with mobile base controlled by  THETA 1, THETA 2 with a single holonomic degree of freedom manipulator characterized by  THETA 3 measured relative to the X,Y,Z coordinate system attached to the holonomic manipulator.
      <br/>
      FIG. 4 is a diagram of a forklift system having holonomic degrees of freedom on the fork manipulator of vertical lift (h1), tilt angle (h2), sideshift (h3), and nonholonomic degrees of freedom of the mobile base of steering angle (h4) and thrust forward or reverse (h5).
    </p>
    <heading>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT</heading>
    <p num="59">
      1.
      <br/>
      Standard CSM Part of MCSM
    </p>
    <p num="60">
      The present invention uses a form of CSM as a necessary subsystem, as part of the combination holonomic/nonholonomic system.
      <br/>
      FIG. 2 shows such a CSM system, which consists of at least two cameras and some sort of manipulator with at least one holonomic degree of freedom.
      <br/>
      In the present invention, the standard CSM system must be part of some mobile base consisting of nonholonomic degrees of freedom In one embodiment, the rover shown in FIG. 3 contains a standard CSM system consisting of two cameras and a single degree of freedom arm.
      <br/>
      This standard CSM system is mounted onto, or integrated into, the mobile base.
      <br/>
      The mobile base itself has two nonholonomic degrees of freedom in its independently driven front wheels.
      <br/>
      MCSM is not limited to this particular combination of holonomic and nonholonomic degrees of freedom.
      <br/>
      For instance, the forklift shown in FIG. 4 could be an MCSM system.
      <br/>
      This particular forklift system has three holonomic degrees of freedom: the vertical lift of the fork (h1), the tilt angle of the fork (h2), and the sideshift of the fork (h3).
      <br/>
      There are also the two nonholonomic degrees of freedom drive angle (h4) and power to the drive wheels (h5).
      <br/>
      The standard CSM subsystem of this MCSM system would consist of two cameras mounted on the forklift along with the three holonomic degrees of freedom of the fork manipulator.
    </p>
    <p num="61">
      The first step in the MCSM process is to establish the view parameters in each camera using the standard CSM subsystem.
      <br/>
      This step involves running a pre-plan trajectory in which all of the holonomic degrees of freedom of the manipulator are moved to a series of predetermined poses.
      <br/>
      At each of these poses, video samples are acquired in each of the cameras.
      <br/>
      The camera-space locations of features on the manipulator are determined using standard image processing techniques.
      <br/>
      The view parameters for each camera are found by minimizing Eq. (3).
    </p>
    <p num="62">2. Determination of Target Objective</p>
    <p num="63">
      Once the view parameters have been established it is necessary to determine some target objective.
      <br/>
      The target objective will be dependent on the type of task being performed.
      <br/>
      For instance, in the case of a forklift, the target objective might be engaging a certain pallet, which is in the field of view of one of the cameras.
      <br/>
      For the rover with attached robotic arm, the target objective might be picking up a rock or placing some scientific instrument carried by the robotic arm at a certain location relative to a given rock or object.
    </p>
    <p num="64">
      In order to carry out the designated task it is necessary to define camera-space locations of the target point(s), (xci j,yci j).
      <br/>
      An operator assists in choosing a specific camera-space target location.
      <br/>
      The operator does this by viewing one or more images from the cameras on the MCSM system.
      <br/>
      In these images, the operator selects a point that corresponds to the target.
      <br/>
      In the case of the rover engaging a rock, the operator, with the assistance of image processing software, may simply select the center of the rock.
      <br/>
      In the case of the forklift engaging a pallet, the user might select one or more of the corners as the camera-space target locations.
      <br/>
      The operator either must select the corresponding points in the other camera-spaces or use image processing software to automatically determine the corresponding points.
    </p>
    <p num="65">
      Once the camera-space location of a target point is determined in at least two cameras, it is possible to get an estimate of the three dimensional location of the ith target point relative to the MCSM system, (Xi,Yi,Zi).
      <br/>
      It is important to note that the three dimensional location is measured in the coordinate system that is attached to the holonomic manipulator.
      <br/>
      In order to determine the three dimensional location of this target point, the following equation is minimized over all (Xi,Yi,Zi):  (Equation image '3' not included in text)
    </p>
    <p num="66">
      This can be accomplished by carrying out at least squares minimization of Eq. (4).
      <br/>
      The necessary conditions for the least squares minimization give three equations:  (Equation image '4' not included in text)
    </p>
    <p num="67">After carrying out the partial derivatives listed in Eq. (5), the equations can be put into matrix form:  (Equation image '5' not included in text)</p>
    <p num="68">where  (Equation image '6' not included in text)</p>
    <p num="69">and  (Equation image '7' not included in text)</p>
    <p num="70">In order to solve for (Xi,Yi,Zi), Eq. (6) is rearranged as:  (Equation image '8' not included in text)</p>
    <p num="71">
      It is important to note that the process of determining (Xi,Yi,Zi) for a given set of camera-space target points is dependent on the view parameters, Cj =�C1j,C2j, . . . ,C6j �T,for each camera.
      <br/>
      Thus, if the view parameters change based on updated information, then the three dimensional estimate of the target location, (Xi,Yi,Zi) would also change even though the actual position of the target might not have changed.
    </p>
    <p num="72">3. Trajectory Generation</p>
    <p num="73">
      Once there is an estimate of the three dimensional position of the target, it is possible to generate a trajectory for the vehicle to follow such that upon completion of the trajectory, the MCSM system can complete its task.
      <br/>
      If the task were engaging a pallet with a forklift system, the trajectory would allow for a suitable entry of the forks into the openings of the pallet.
      <br/>
      In this case, the trajectory must control the angle of approach of the vehicle.
      <br/>
      In the case of a rover system placing a scientific instrument at some specified distance from a target rock, the angle of approach of the rock may not be important.
      <br/>
      In this case, the trajectory planned would terminate with the rover in a position such that it is possible to locate the scientific instrument in the proper place using only the holonomic degrees of freedom of the onboard robotic arm.
      <br/>
      If the rover system has only a single degree of freedom arm, as shown in FIG. 3, then successful completion of the task is dependent upon the nonholonomic degrees of freedom of the rover base moving the rover into the proper position for the arm to complete the task.
      <br/>
      If the rover had a higher degree of freedom robotic arm, it might be possible to position the rover base somewhat close to the target rock.
      <br/>
      Then, the multiple degrees of freedom of the robotic arm could do the fine positioning, compensating for inaccuracies in the position of the mobile base.
      <br/>
      This solution would require the resolution of redundant degrees of freedom.
      <br/>
      While MCSM could handle such redundancy, it is not necessary since MCSM achieves precise control of the nonholonomic degrees of freedom of the system requiring only the minimum number of holonomic degrees of freedom to complete any given task.
    </p>
    <p num="74">
      Once a trajectory has been planned, the system will begin motion following the desired path as close as possible.
      <br/>
      The means of controlling the vehicle such that it traverses the desired path is dependent on the configuration of the vehicle itself.
      <br/>
      For instance, the rover shown in FIG. 3 has independently driven front wheels.
      <br/>
      Each drive wheel has a computer controlled servomotor equipped with a gearhead that produces sufficient torque to move the rover over rough terrain.
      <br/>
      Steering the rover is accomplished by controlling the speed with which the second wheel moves relative to the first wheel.
      <br/>
      Thus the rover system can follow a trajectory by defining a series of wheel 1 to wheel 2 ratios and the distance that wheel 1 should rotate through while maintaining a given ratio of wheel 2 rotation.
    </p>
    <p num="75">
      The forklift system described above requires slightly different input in order to track a given trajectory.
      <br/>
      In this case, it would be necessary to control the steering angle, h4, as well as the thrust forward, h5.
    </p>
    <p num="76">4. Updating Trajectory</p>
    <p num="77">
      The trajectory planned for the mobile base of the MCSM system is determined based upon the three-dimensional estimates of the target location(s).
      <br/>
      It is important to note that these three-dimensional estimates are dependent upon the view parameters, Cj =�C1j,C2j, . . . ,C6j �T, for each camera, j, as well as the camera-space target locations of the given target point, (xci j,yci j).
      <br/>
      When either the view parameters or the camera-space target locations change, the estimate of the three dimensional position of the target changes.
      <br/>
      The view parameters would change if localized video information of the manipulator became available.
      <br/>
      As mentioned above, in a typical CSM system, video samples of the manipulator are acquired when the manipulator is in the local region close to the terminal position.
      <br/>
      These video samples are typically given a higher relative weight to the samples acquired in the pre-plan trajectory.
      <br/>
      Along with the pre-plan samples, and any other samples acquired, the view parameters are updated with the local information by carrying out the minimization defined in Eq. (2).
      <br/>
      The view parameters containing the localized video information will do a better job in describing the relationships between the camera-space appearances of manipulator features and the actual three-dimensional position of these features in the local region of operation.
    </p>
    <p num="78">
      The preferred practice in MCSM systems is to acquire some additional video samples of the manipulator in positions close to the terminal pose.
      <br/>
      This localized visual information is given more relative weight than the samples in the pre-plan.
      <br/>
      The view parameters are updated.
      <br/>
      Then, in turn, the estimates of the three-dimensional target locations are also updated based on the updated view parameters.
      <br/>
      The new values for the three-dimensional locations of the target point can be used to create an updated trajectory for the mobile base of the MCSM system to follow.
      <br/>
      The updating of the view parameters by acquiring additional video samples of the manipulator can occur while the mobile base is stationary or in motion.
    </p>
    <p num="79">
      As the mobile base of the MCSM system moves through the prescribed trajectory, the position of the target points change relative to the manipulator on the vehicle.
      <br/>
      Likewise, the camera-space locations of the target points also change.
      <br/>
      This is not due to the target actually moving, rather it is due to the fact that the vehicle has moved, since the target location is measured relative to the vehicle.
      <br/>
      The camera-space locations of the target point are monitored as the vehicle moves towards the target.
      <br/>
      This tracking can be done while the vehicle is in motion, or the vehicle can come to a stop periodically in order to wait for the system to find the target again.
    </p>
    <p num="80">
      Once the system finds the new camera-space locations of the target, it is possible to generate a new trajectory for the vehicle to follow.
      <br/>
      In the case of stopping and starting, the new trajectory would begin with the current position of the rover.
      <br/>
      If the rover were in motion as the new trajectory was generated, then a suitable time would be determined by the system for when it should transition from carrying out the old trajectory to following the newly updated trajectory.
    </p>
    <p num="81">
      As the mobile base of the MCSM system closes in on the target object, the holonomic degrees of freedom of the system are moved to their target location(s) as well.
      <br/>
      The method for resowing the holonomic degrees of freedom depends upon the type of MCSM system in use as well as the task at hand.
      <br/>
      For instance, in the case of the forklift system, the vertical position of the forks, h1, would be dependent on the three dimensional location of the pallet that the system is engaging.
      <br/>
      Based on FIG. 4, h1 would be determined based on the Z component of the pallet position.
      <br/>
      The tilt angle, h2, would be set to keep the forks parallel to the ground.
      <br/>
      Once the forklift finishes its trajectory, a preprogrammed move of the fork height as well as the tilt angle might be executed in order to engage the pallet.
    </p>
    <p num="82">
      Consider the task of placing a scientific instrument carried by the robotic arm of the rover depicted in FIG. 3.
      <br/>
      The single holonomic degree of freedom might be resolved using knowledge of the Z-component of the three-dimensional position of the target rock as well as the information of the length of the arm.
      <br/>
      In this case, the value for the angle of the arm would be found by:  (Equation image '9' not included in text)
    </p>
    <p num="83">where Zt refers to Z-component of the three-dimensional position of the target rock and D is the length of the robotic arm.</p>
    <p num="84">Although the invention has been shown and described with respect to certain embodiments thereof, it should be understood by those skilled in the art that other various changes and omissions in the form and detail thereof may be made therein without departing from the spirit and scope of the invention as set forth in the appended claims.</p>
  </description>
  <claims format="original" lang="en" id="claim_en">
    <claim num="1">
      <claim-text>We claim:</claim-text>
      <claim-text>1.</claim-text>
      <claim-text>A mobile camera space manipulation means comprising:</claim-text>
      <claim-text>a multiple degree of freedom movement manipulator means including;</claim-text>
      <claim-text>- a base means, - an end effector means, - connection means between the base means and the end effector means allowing multiple degrees of freedom of movement of the end effector means with respect to the base means based on known nominal kinematics, and including first sensing means for producing signals correlated to the orientation of the connection means relative to the base means, - motive means associated with the base means for allowing locomotion of the base means in a plurality of degrees of freedom of movement and including second sensing means for producing signals correlated to the distance and direction of movement of the base means along a surface; first cue means associated with the end effector means; second cue means associated with a work object; two or more camera means each having a field of view being positionable to capture at least intermittently both the first and second cue means in a field of view, each camera means being attached to the base means; camera space means associated with each camera means to convert the field of view of each camera means into a corresponding two dimensional camera space;</claim-text>
      <claim-text>and processing means including: - distinguishing means to distinguish the first and second cue means from generally all other contents of the camera spaces; - tracking means to obtain and store information relating to position and movement of the cue means in the camera spaces, monitoring means to obtain and store information relating to holonomic orientation and position of the connection means relative to the base means from the first sensing means, and nonholonomic history of movement of the base means from the second sensing means; - estimation and planning means to repetitively propose a plan of movement for one or both of the connection means and motive means to bring about a desired positional relationship between the first and second cue means in the camera spaces, the plan being based on information in the tracking means and the monitoring means taking into consideration both holonomic and nonholonomic relationships; - control means for instructing movements of one or both of the connection and motive means to follow the plan in physical space.</claim-text>
    </claim>
    <claim num="2">
      <claim-text>2. The mobile camera space manipulation means of claim 1 wherein the base means includes a plurality of wheels for allowing mobility over a surface.</claim-text>
    </claim>
    <claim num="3">
      <claim-text>3. The mobile camera space manipulation means of claim 1 wherein the base means consists of a mobile cart.</claim-text>
    </claim>
    <claim num="4">
      <claim-text>4. The mobile camera space manipulation means of claim 1 wherein the base means consists of an underwater vehicle.</claim-text>
    </claim>
    <claim num="5">
      <claim-text>5. The mobile camera space manipulation means of claim 1 wherein the end effector consists of a grasping means.</claim-text>
    </claim>
    <claim num="6">
      <claim-text>6. The mobile camera space manipulation means of claim 1 wherein the end effector consists of a fork lift.</claim-text>
    </claim>
    <claim num="7">
      <claim-text>7. The mobile camera space manipulation means of claim 1 wherein the base means includes a plurality of wheels for allowing mobility over a surface.</claim-text>
    </claim>
    <claim num="8">
      <claim-text>8. The mobile camera space manipulation means of claim 1 wherein the base means consists of a mobile cart.</claim-text>
    </claim>
    <claim num="9">
      <claim-text>9. The mobile camera space manipulation means of claim 1 wherein the base means consists of an underwater vehicle.</claim-text>
    </claim>
    <claim num="10">
      <claim-text>10. The mobile camera space manipulation means of claim 1 wherein the end effector consists of a grasping means.</claim-text>
    </claim>
    <claim num="11">
      <claim-text>11. The mobile camera space manipulation means of claim 1 wherein the end effector consists of a fork lift.</claim-text>
    </claim>
    <claim num="12">
      <claim-text>12. A camera space manipulation control means, utilizing two or more camera means for engaging an end effector means with a work object, comprising: an articulateable manipulator means of known nominal kinematics in physical space extending from a mobile base to an outward end for movement of the outward end in a predefined physical work space in the physical space relative to the mobile base, the mobile base having a nonholonomic kinematic relationship between wheel rotation and base-position response, the manipulator means including a motor means for articulating the manipulator means in said physical space, the mobile base having a motor means and a steering means to locate the mobile base in any direction over a surface or in three dimensions, and means for producing a signal identifying an approximate position and orientation of the manipulator means with respect only to the base, wherein the kinematic description of the manipulator means with base being known and the kinematic description of the mobile base being known only relative to prior movement; each camera means being positioned on the mobile base and each camera means being oriented generally towards the end effector means for providing camera vision intermittently of the end effector means and the work object in camera space; first visual cue means associated with the end effector means; second visual cue means associated with the work object, the first and second visual cue means comprising means which are distinct and identifiable in said camera space manipulation control means in any surrounding environment, the first and second visual cue means providing descriptions of three dimensional physical space maneuver objectives as admissible configurations of visual cue means in the two dimensional camera spaces of the camera means;</claim-text>
      <claim-text>and a control means operatively connected to the manipulator means and the camera means, the control means including computing means for receiving the signal from the manipulator means and identifying the approximate position and orientation of the manipulator means with respect to the base means through the use of previously known kinematics, and signal processing means which identifies and tracks the visual cue means in the camera spaces to convert such into two dimensional camera space cue position signals, the manipulator approximate position and orientation signal and the camera space cue position signals being used in the control means to estimate the relationship between the position and orientation of the manipulator means and the location in each camera space of the visual cue means placed on the manipulator means, and using the current estimations of these relationships selecting required movement and orientation of the manipulator means which will bring about admissible configurations of the visual cue means in each camera space to insure successful engagement of the object in physical space, and to control orientation commands resulting from the estimated relationship.</claim-text>
    </claim>
    <claim num="13">
      <claim-text>13. The mobile camera space manipulation means of claim 1 wherein the base means includes a plurality of wheels for allowing mobility over a surface.</claim-text>
    </claim>
    <claim num="14">
      <claim-text>14. The mobile camera space manipulation means of claim 1 wherein the base means consists of a mobile cart.</claim-text>
    </claim>
    <claim num="15">
      <claim-text>15. The mobile camera space manipulation means of claim 1 wherein the base means consists of an underwater vehicle.</claim-text>
    </claim>
    <claim num="16">
      <claim-text>16. The mobile camera space manipulation means of claim 1 wherein the end effector consists of a grasping means.</claim-text>
    </claim>
    <claim num="17">
      <claim-text>17. The mobile camera space manipulation means of claim 1 wherein the end effector consists of a fork lift.</claim-text>
    </claim>
    <claim num="18">
      <claim-text>18. A method of camera space manipulation utilizing at least two camera means for engaging an articulateable manipulator means with an object where there is not any known prior three dimensional physical space relationship between the manipulator means and the object, and there is a known three dimensional physical space relationship between the manipulator means and physical space in a two dimensional image at the focal plane of the camera means, denoted as camera space, comprising the steps: orienting each camera means to view the manipulator means which has an arm extending from a base to an outward end which is moveable in physical work space with known nominal kinematics relative to the base; the manipulator means including a motor means which articulates the manipulator means in said physical work space, and means for producing a signal identifying the approximate position and orientation of the manipulator means with respect only to the base in said physical work space; the base having motor and steering means for moving the base in any direction along a surface and including means for producing a signal identifying the approximate position and orientation of the base, each camera means being positioned and oriented to provide, at least intermittently, camera vision of at least the outward end of the manipulator means in at least part of the physical work space to view at least the outer end of the manipulator means and the work object in camera space; placing a first visual cue means in association with an outward end of the arm; placing a second visual cue means in association with the object to be engaged by the manipulator means, the first and second visual cue means comprising means which are distinct and identifiable in said camera space from the remainder of the system and any surrounding environment, the first and second visual cue means providing descriptions of three dimensional physical space maneuver objectives in terms of admissible configurations of the visual cue means in the two dimensional camera space of each camera; receiving signals from the manipulator means and base means and identifying the approximate position and orientation of the manipulator means and base means with respect to the base and surface respectively through the use of known nominal kinematics; identifying and tracking the visual cue means in the two dimensional camera space of each camera means and respectively estimating the relationship between the position and orientation of the manipulator means and the location in each camera space of the visual cue means placed on the manipulator means, and using the current estimation of these relationships to select the movement and to command the orientation of the manipulator means which will bring about the admissible configurations of the visual cue means in each camera space which insures successful engagement of the object;</claim-text>
      <claim-text>and continuously controlling movement and orientation of the manipulator means according to such autonomously selected movement and orientation commands to achieve engagement of the manipulator means with the work object in said physical work space.</claim-text>
    </claim>
  </claims>
</questel-patent-document>