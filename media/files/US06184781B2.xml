<?xml version="1.0" encoding="UTF-8"?>
<questel-patent-document lang="en" date-produced="20180805" produced-by="Questel" schema-version="3.23" file="US06184781B2.xml">
  <bibliographic-data lang="en">
    <publication-reference publ-desc="Granted patent as second publication">
      <document-id>
        <country>US</country>
        <doc-number>06184781</doc-number>
        <kind>B2</kind>
        <date>20010206</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>US6184781</doc-number>
      </document-id>
    </publication-reference>
    <original-publication-kind>B2</original-publication-kind>
    <application-reference is-representative="YES" family-id="22917128" extended-family-id="42112613">
      <document-id>
        <country>US</country>
        <doc-number>09243040</doc-number>
        <kind>A</kind>
        <date>19990202</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>1999US-09243040</doc-number>
      </document-id>
      <document-id data-format="questel_Uid">
        <doc-number>43170495</doc-number>
      </document-id>
    </application-reference>
    <language-of-filing>en</language-of-filing>
    <language-of-publication>en</language-of-publication>
    <priority-claims>
      <priority-claim kind="national" sequence="1">
        <country>US</country>
        <doc-number>24304099</doc-number>
        <kind>A</kind>
        <date>19990202</date>
        <priority-active-indicator>Y</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="1">
        <doc-number>1999US-09243040</doc-number>
      </priority-claim>
    </priority-claims>
    <dates-of-public-availability>
      <publication-of-grant-date>
        <date>20010206</date>
      </publication-of-grant-date>
    </dates-of-public-availability>
    <classifications-ipcr>
      <classification-ipcr sequence="1">
        <text>B60Q   1/52        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>B</section>
        <class>60</class>
        <subclass>Q</subclass>
        <main-group>1</main-group>
        <subgroup>52</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
    </classifications-ipcr>
    <classification-national>
      <country>US</country>
      <main-classification>
        <text>340435000</text>
        <class>340</class>
        <subclass>435000</subclass>
      </main-classification>
      <further-classification sequence="1">
        <text>340436000</text>
        <class>340</class>
        <subclass>436000</subclass>
      </further-classification>
      <further-classification sequence="2">
        <text>340461000</text>
        <class>340</class>
        <subclass>461000</subclass>
      </further-classification>
      <further-classification sequence="3">
        <text>340903000</text>
        <class>340</class>
        <subclass>903000</subclass>
      </further-classification>
      <further-classification sequence="4">
        <text>348118000</text>
        <class>348</class>
        <subclass>118000</subclass>
      </further-classification>
      <further-classification sequence="5">
        <text>348148000</text>
        <class>348</class>
        <subclass>148000</subclass>
      </further-classification>
    </classification-national>
    <classifications-ecla>
      <classification-ecla sequence="1">
        <text>B60Q-001/52</text>
        <section>B</section>
        <class>60</class>
        <subclass>Q</subclass>
        <main-group>1</main-group>
        <subgroup>52</subgroup>
      </classification-ecla>
    </classifications-ecla>
    <patent-classifications>
      <patent-classification sequence="1">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>B60Q-001/52</classification-symbol>
        <section>B</section>
        <class>60</class>
        <subclass>Q</subclass>
        <main-group>1</main-group>
        <subgroup>52</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
    </patent-classifications>
    <number-of-claims>11</number-of-claims>
    <exemplary-claim>1</exemplary-claim>
    <figures>
      <number-of-drawing-sheets>8</number-of-drawing-sheets>
      <number-of-figures>13</number-of-figures>
      <image-key data-format="questel">US6184781</image-key>
    </figures>
    <invention-title format="original" lang="en" id="title_en">Rear looking vision system</invention-title>
    <references-cited>
      <citation srep-phase="examiner">
        <patcit num="1">
          <text>NISHIMURA SHIGERU, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4713685</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4713685</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="2">
          <text>NAKANO NOBUYUKI, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5487116</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5487116</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="3">
          <text>BELL DAVID M, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5550937</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5550937</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="4">
          <text>HENLEY STUART L</text>
          <document-id>
            <country>US</country>
            <doc-number>5657073</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5657073</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="5">
          <text>SCHOFIELD KENNETH, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5670935</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5670935</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="6">
          <text>ROSINSKI ALBIN, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5793308</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5793308</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="7">
          <text>ASHIHARA JUN, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5883739</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5883739</doc-number>
          </document-id>
        </patcit>
      </citation>
    </references-cited>
    <parties>
      <applicants>
        <applicant data-format="original" app-type="applicant" sequence="1">
          <addressbook lang="en">
            <orgname>Intel Corporation</orgname>
            <address>
              <address-1>Santa Clara, CA, US</address-1>
              <city>Santa Clara</city>
              <state>CA</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </applicant>
        <applicant data-format="questel" app-type="applicant" sequence="2">
          <addressbook lang="en">
            <orgname>INTEL</orgname>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </applicant>
      </applicants>
      <inventors>
        <inventor data-format="original" sequence="1">
          <addressbook lang="en">
            <name>Ramakesavan, Sundaram</name>
            <address>
              <address-1>Chandler, AZ, US</address-1>
              <city>Chandler</city>
              <state>AZ</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
      </inventors>
      <agents>
        <agent sequence="1" rep-type="agent">
          <addressbook lang="en">
            <orgname>Trop, Pruner &amp; Hu, P.C.</orgname>
          </addressbook>
        </agent>
      </agents>
    </parties>
    <examiners>
      <primary-examiner>
        <name>Hofsass, Jeffery A.</name>
      </primary-examiner>
    </examiners>
    <lgst-data>
      <lgst-status>GRANTED</lgst-status>
    </lgst-data>
  </bibliographic-data>
  <abstract format="original" lang="en" id="abstr_en">
    <p id="P-EN-00001" num="00001">
      <br/>
      A rear looking vision system for a vehicle may produce a composite image from images produced by different imaging devices having different viewing angles of the region behind the vehicle.
      <br/>
      The composite image may combine the images so as to avoid duplicating objects within the fields of view of more than one imaging device.
    </p>
  </abstract>
  <description format="original" lang="en" id="desc_en">
    <heading>BACKGROUND</heading>
    <p num="1">The invention relates to providing a rear looking image of the region behind a vehicle such as a car, truck, boat, train or plane.</p>
    <p num="2">
      When operating an automobile, it is awkward to attempt to use the images from both the side view and rearview mirrors to check the area to the rear of the operator.
      <br/>
      The operator may look behind the vehicle, but this too is awkward and may be dangerous since the operator is no longer watching the area in front of the car.
    </p>
    <p num="3">Thus, there is a need for a system that better enables an operator to be advised about hazards to the sides and rear of a vehicle.</p>
    <heading>SUMMARY</heading>
    <p num="4">
      In accordance with one embodiment, a rearview system for a vehicle includes first and second imaging devices adapted to develop images of an area to the rear of a vehicle.
      <br/>
      A system combines the images from said first and second imaging devices together to form a composite image.
    </p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading>
    <p num="5">
      FIG. 1 is a diagrammatic top plan depiction of a vehicle with a rear looking vision system in accordance with one embodiment of the invention;
      <br/>
      FIGS. 2A-2C are depictions of exemplary frames from each of the three cameras shown in FIG. 1;
      <br/>
      FIG. 3 is a front elevational view of the cab of the vehicle, shown in FIG. 1 with a rear-looking vision system, showing the view from the operator's perspective;
      <br/>
      FIG. 4 is the resulting depiction on the rearview display device in one embodiment;
      <br/>
      FIG. 5 is a block diagram of a digital imaging system that runs an application program according to an embodiment of the invention;
      <br/>
      FIG. 6 is a flow diagram illustrating the application program of FIG. 5 that performs an image stitching process;
      <br/>
      FIG. 7 illustrates a correlation path used by the image stitching process;
      <br/>
      FIGS. 8 and 9 are charts of correlation function values calculated by comparing two images in the image stitching process;
      <br/>
      FIG. 10 is a block diagram of a computer system that may be used in the digital imaging system of FIG. 5 and in which stitching programs according to embodiments of the invention may be run; and
      <br/>
      FIG. 11 is a block diagram of a digital camera that may be used in the digital imaging system of FIG. 5.
    </p>
    <heading>DETAILED DESCRIPTION</heading>
    <p num="6">
      Referring to FIG. 1, a vehicle A (illustrated as being a flatbed truck but can also be a car, boat, train or plane or any other vehicle) is illustrated as moving down a roadway along a vehicle B (illustrated as a car), and ahead of a vehicle C (illustrated as a motorcycle) and a vehicle D (a tractor trailer truck).
      <br/>
      The vehicle A is equipped with three rear looking cameras 12a-12c which may be digital cameras.
      <br/>
      The cameras 12a and 12b are illustrated in position at the front corners of the vehicle while the camera 12c is located on the rear of the vehicle.
      <br/>
      The viewing angle of each of the cameras is illustrated by the arrows V1, V2 and V3.
    </p>
    <p num="7">
      Thus, each camera has a different field of view and images a different scene behind or on the side of the vehicle.
      <br/>
      For example, referring to FIGS. 2A-2C, the view V1 from the camera 12b may include the vehicle B and part of the vehicle C, as illustrated in FIG. 2A. The view from camera 12c, V2, includes the vehicle C and the vehicle D, as shown in FIG. 2B. The view from the camera 12a, shown in FIG. 2C, includes only the vehicle D.
    </p>
    <p num="8">
      A display 11 (shown in FIG. 3), mounted in the position normally occupied by a rearview mirror in the front of the vehicle cab 13, may display the images produced from the three illustrated cameras 12 as adjusted (as will be described hereinafter) to reduce misalignment problems and distortions arising from stitching the three images together.
      <br/>
      Namely, as shown in FIG. 4, the display 11 displays an image which corresponds to what the operator would have seen if the operator had turned around and was able to view the scene behind the vehicle fully.
      <br/>
      Namely, the three vehicles B, C and D are shown in their appropriate positions, adjusted for distance behind the vehicle and for lateral displacement with respect to one another.
      <br/>
      In this way, the operator gets a full view of what is going on behind the vehicle.
      <br/>
      Moreover, in one embodiment of the invention, the operator may see a full view of the sides of the vehicle and the entire scene is stitched together so that with one glance to the display device 11, the operator can fully appreciate everything that is going on around the vehicle, as if the cameras were effectively looking right through the vehicle A. The image of vehicle B may be caused to flash (as indicated by the arrows) to indicate it is alongside the vehicle A.
    </p>
    <p num="9">
      Referring to FIG. 5, a digital imaging system 10, which may be located in the vehicle A, includes a program 16, according to an embodiment of the invention, and one or more digital cameras or other suitable imaging devices 12 (which may be a video camera or still-shot camera) that captures multiple optical images "I".
      <br/>
      Each camera 12 transmits streams of data representing the images (stored as frames of pixel data) to a computer 14 over a communications link 15 (which may be a serial link, for example).
      <br/>
      The computer 14 may then execute the application program 16 to process the captured images.
      <br/>
      According to an embodiment of the invention, under control of the application program 16, the computer 14 can combine ("stitch") multiple frames of digitized images together to form a composite image to represent the overall scene on the side of and behind the vehicle A.
    </p>
    <p num="10">
      The captured images are then processed and stitched together by the application program 16 ("stitching program"), according to embodiments of the invention, to create a composite image that represents the entire scene behind the vehicle.
      <br/>
      Key parts of the stitching process include aligning the captured frames and matching overlapping regions of the different frames of the scene.
      <br/>
      To accurately match the regions in the multiple images, the stitching program 16 may compensate for misalignment or distortion resulting from motion of the cameras caused, for example, by movement of the vehicle.
    </p>
    <p num="11">
      There are six degrees of freedom of motion of each camera, these being the three independent axes of motion (x,y,z) and the three independent rotations about those axes.
      <br/>
      Thus, given any two overlapping images about which nothing is known regarding the relative positions of the camera when capturing two images, the six-dimensional space is searched to match the overlapping pixels of two images.
    </p>
    <p num="12">
      Because the cameras 12a-12c are relatively fixed to each other, except for perhaps vibration caused by movement of the vehicle A, the regions of overlap between images I1 and I2 (captured by cameras 12b and 12c, respectively) and between I2 and I3 (captured by cameras 12c and 12a, respectively) are roughly in the same areas.
      <br/>
      To properly combine or stitch the frames of images, all the captured frames may be aligned with respect to a reference frame to compensate for movement and rotation of the camera 12 caused by vibrations, for example, as a target scene is being captured.
      <br/>
      The reference frame may be selected as the frame captured by camera 12c.
      <br/>
      This correction process ("alignment process") is accomplished using perspective transforms (or projective mapping) to adjust the pixel positions in each frame.
      <br/>
      In one embodiment, the perspective transforms map all captured frames back into the reference frame I2.
      <br/>
      In an alternate embodiment, another frame can be chosen as the reference frame.
    </p>
    <p num="13">
      To perform the perspective transforms, mapping points in each frame are first identified.
      <br/>
      In one embodiment, four mapping points are selected in each frame.
      <br/>
      To accurately correlate these points in the frames, the mapping points are uniquely identifiable for each frame; that is, a mapping point should not be redundant.
      <br/>
      Another desired attribute of these mapping points is that they be found in "feature-rich" areas; that is, areas of high detail.
      <br/>
      Once the mapping points are identified, then the perspective transforms of all frames can be accurately performed using standard techniques known in the art.
      <br/>
      To map frame I1 to I2, the mapping points are selected from a region OV12 that roughly represents the overlap region between I1 and I2.
      <br/>
      The exact overlap region cannot be determined due to vibration of the cameras 12b and 12c.
      <br/>
      Similarly, to map I3 to I2, the mapping points are selected from a region OV23 that roughly represent the overlap region between I2 and I3.
    </p>
    <p num="14">
      Referring to FIG. 7, the stitching program 16 according to an embodiment of the invention first receives a frame from a video source, such as the digital camera 12 (step 100).
      <br/>
      The stitching program 16 may next perform pre-processing steps, including lens distortion correction (step 106) and image simplification and enhancement (step 108).
      <br/>
      An image may be distorted through lens operations and other non-linear effects.
      <br/>
      Camera lenses often have geometric distortions, including "pin cushion" distortion and "barrel" distortion.
      <br/>
      Many standard techniques exist to compensate for such distortions.
    </p>
    <p num="15">
      After distortion of the image has been corrected, image simplification and enhancement may be performed (step 108).
      <br/>
      For example, processing may be performed to remove image noise.
      <br/>
      Further, luminance distortion may be corrected to improve the image correlation process.
      <br/>
      Luminance distortion is the effect of a camera lens in changing the relative brightness of different parts of an image, for example, making the center of an image brighter than it should be relative to the edges.
      <br/>
      To correct luminance distortion, a small number of images of blank white paper may be captured and the average taken on a pixel-by-pixel basis over the multiple images.
      <br/>
      The average image may be converted to a gray scale image +pij }, from which a gray scale histogram may be calculated.
      <br/>
      From the histogram, a threshold white value wT may be calculated such that a predetermined percentage (e.g., 92%) of pixels in the image +pij } have white values less than wT.
      <br/>
      From wT and +pij }, an array +cij } is defined according to Eq. 1.  (Equation image '1' not included in text)
    </p>
    <p num="16">
      To correct for luminance distortion, all captured frames that are to be stitched together may be multiplied by the array +cij } on a pixel-by-pixel basis.
      <br/>
      Steps 106 and 108 are included in the pre-processing phase of the stitching process.
      <br/>
      In some embodiments, steps 106 and 108 may be eliminated to reduce processing requirements.
    </p>
    <p num="17">
      After the pre-processing phase, the alignment phase of the stitching process is performed to align the captured frames (I1, I2 and I3) to compensate for the unknown relative positions of the multiple images.
      <br/>
      The stitching program 16 identifies "feature-rich" image areas (step 110) in the overlap region OV12 or OV23.
      <br/>
      In the image stitching process, "good" mapping points in an image are selected to properly map one image to another.
      <br/>
      Mapping points selected in one image are correlated to corresponding mapping points in another image.
      <br/>
      Poor choices in mapping points may result in missed correlation, resulting in incorrectly stitched images.
      <br/>
      As noted above, these mapping points are found in feature-rich areas.
      <br/>
      Thus, to select these mapping points, feature-rich areas are first identified.
      <br/>
      To determine what is a feature-rich area, the number of features in an image is first determined, followed by applying the measurement to find the feature-rich area.
    </p>
    <p num="18">
      To measure feature detail, a simplified edge detection scheme may be used in one embodiment.
      <br/>
      The measurement is defined as the summation of the absolute difference of a group of closely related pixels in both the x and y directions for a given correlation area, which is a small area (such as area A in FIG. 6) over which the feature-rich determination is made.
      <br/>
      For example, the correlation area A may be selected to be a region of 30 * 30 pixels.
      <br/>
      In one embodiment, a correlation area A is initially selected to be in a corner of the overlap region OV12 or OV23.
      <br/>
      If the initial correlation area is determined not to be feature-rich, then the correlation area A is moved according to a preselected path (e.g., path R1 shown in FIG. 6) and the correlation process performed until a feature-rich area is identified.
    </p>
    <p num="19">
      The captured image is represented by a frame P and the parameters x and y represent the coordinates of pixels in the frame P. Thus, P(x, y) represents the pixel value at a certain (x, y) coordinate.
      <br/>
      To determine if the selected correlation area is a high-feature (or feature-rich) area, two variables H(x, y) and V(x, y) are defined according to Eqs. 2 and 3, in which H(x, y) represents the absolute horizontal pixel difference and V(x, y) represents the absolute vertical pixel difference.
    </p>
    <p num="20">
      H(x, y)=.vertline.P(x-1, y)-P(x+1, y).vertline.,  (Eq. 2)
      <br/>
      V(x, y)=.vertline.P(x, y-1)-P(x, y+1).vertline.,   (Eq. 3)
    </p>
    <p num="21">From the H and V values, an edge detail value D of the correlation area may be calculated using Eq. 4,  (Equation image '2' not included in text)</p>
    <p num="22">in which variables i, j are bounded by the selected correlation area.</p>
    <p num="23">
      By calculating the edge detail D, the number of features in a region may be measured.
      <br/>
      After the edge detail value D is calculated for the selected correlation area the stitching program 16 then determines if the edge detail value D exceeds the minimum feature threshold value (step 112).
      <br/>
      A feature-rich area is a region where the measured detail level is greater than the minimum feature (or detail threshold) for correlation to be successful.
      <br/>
      This minimum threshold value can be preset and depends on the sensitivity of the correlation routine to the type of features found in an image.
      <br/>
      Once this threshold value is determined, a simple search over the image can be done to find an area that meets the threshold requirement.
    </p>
    <p num="24">
      In one embodiment, the calculation of the detail value D may be improved by eliminating image noise from the calculation of D. This may allow for consistency of values across images of different quality.
      <br/>
      Noise reduction may be performed by eliminating pixel differences less than a certain threshold value.
      <br/>
      The noise threshold may be based upon the type of application and the type of images a camera can capture.
      <br/>
      For example, if the captured image includes text, the noise threshold may be set high to eliminate areas with luminance irregularity problems and poor image quality (low signal-to-noise ratios).
      <br/>
      To eliminate image noise, Eqs. 5 and 6 may be used.
    </p>
    <p num="25">
      H(x, y)=0, if H(x, y) &lt;= NThreshold,  (Eq. 5)
      <br/>
      V(x,y)=0, if V(x,y) &lt;= NThreshold,  (Eq. 6)
    </p>
    <p num="26">where Nthreshold may be the minimum pixel difference between any two pixels, which also is the noise threshold.</p>
    <p num="27">
      If the calculated detail value D is less than the minimum detail threshold (as determined in step 112), then a feature-rich area has not been found, and a new correlation area is selected along path R1 or some variation of the path shown in FIG. 6 (step 113).
      <br/>
      The feature-rich area determination of steps 110 and 112 are then performed again.
      <br/>
      If the area is determined to be feature-rich, then a mapping point may be selected in the feature-rich area (step 114).
    </p>
    <p num="28">
      Most natural scenes have a high level of detail, so selecting a set of mapping points at fixed positions in the overlap region OV may be sufficient for correct correlation.
      <br/>
      However, in the case of low detail images, the detail within the captured image may be sparse.
      <br/>
      As a result, fixed point correlation may be unsuccessful.
      <br/>
      In some embodiments, fixed point correlation may be used, but other techniques for selecting mapping points may also be used such as that described below.
    </p>
    <p num="29">
      A mapping point is an area of an image that is uniquely identifiable from one image to another.
      <br/>
      According to one embodiment, the criteria for what constitutes a good set of mapping points and how to find those points are as follows: (1) points that have non-redundant feature-rich detail; and (2) points that are as far apart as possible in the overlap region OV12 or OV23.
      <br/>
      According to one embodiment, the zigzag or spiral-shaped search pattern R1 shown in FIG. 6 may be used.
    </p>
    <p num="30">
      In one embodiment, search patterns identical to the search pattern R1 may start from each of the other corners of the overlap region OV image to identify four mapping points.
      <br/>
      The pattern R1 starts from corner C1 of the overlap region OV, while the other patterns R2, R3, and R4 would start from corners C2, C3, and C4, respectively.
    </p>
    <p num="31">
      To select the mapping point in the selected correlation area, the current frame Pk (x, y) is searched and compared to the same correlation area in a previously captured frame, Pm (x, y).
      <br/>
      A correlation function C(x, y) may be calculated according to Eq. 7, which is the sum of the squares of the pixel differences between Pk (x, y) and Pm (x, y).  (Equation image '3' not included in text)
    </p>
    <p num="32">
      where i, j are bounded by the selected correlation area.
      <br/>
      To find the correlation point, the correlation process identifies an (x, y) coordinate in the correlation area of the current frame Pk (x,y) at which the correlation function C(x, y) is a minimum.
      <br/>
      In effect, according to Eq. 7, an (x,y) coordinate is selected in the current frame Pk and frame Pk is moved with respect to the frame Pm in the correlation area A to find a coordinate (x,y) that produces a minimum for C(x,y).
    </p>
    <p num="33">Although in some embodiments the correlation function C(x,y) of Eq. 7 may be used, other correlation functions are also contemplated from which a mapping point can be selected by calculating a minimum or maximum.</p>
    <p num="34">
      The technique described may provide several benefits.
      <br/>
      First, the search from the corners that follows the zigzag path of FIG. 6 in one embodiment minimizes the search time for finding adequate mapping points and yielding maximal transform area.
      <br/>
      Further, by utilizing feature-rich mapping points, white space or ambiguous regions of the image are automatically eliminated.
    </p>
    <p num="35">
      Once a mapping point is found, a correlation error and redundancy error check may be performed (block 115).
      <br/>
      Redundancy error is a result of selecting a correlation point on one image that could be mapped to several points on another image, thereby not producing a unique point.
      <br/>
      This may result from repetitive patterns in the images, which may include camera characteristics such as interlacing.
      <br/>
      To determine if redundant detail exists, correlation data is analyzed over the correlation area A. If this analysis indicates multiple minima of the same magnitude in correlation value, then redundancy has been detected.
      <br/>
      If the selected mapping point is a redundant mapping point, then it may be discarded as a mapping point.
    </p>
    <p num="36">
      The chart shown in FIG. 8 is a simplified chart of correlation illustrating the existence of redundant detail.
      <br/>
      Although the redundancy analysis involves a two-dimensional calculation, the one-dimensional correlation illustrated in the chart of FIG. 8 provides a simple illustration of how the redundancy error detection is performed.
      <br/>
      FIG. 8 shows the result of a horizontal search (in the x direction) in which redundant detail causes two minima when two images Pk and Pm are compared by calculating the correlation function C(x, y) according to Eq. 12. The technique includes finding the minimum of the correlation function and setting a threshold that is slightly larger than the minimum value.
      <br/>
      The same concept and technique is extended to the two-dimensional case, in which a search is performed both in the x and y directions.
    </p>
    <p num="37">
      A method according to one embodiment for determining a threshold and number of minima includes selecting a predetermined threshold that is a set value (e.g. 33%) greater than the minimum value of C(x, y) found in the correlation area A. If the threshold value used results in too many minima, then the threshold can be reduced until the number is manageable.
      <br/>
      Once a good set of minimum values is determined, the stitching program 16 may confirm if each is a minimum by searching all correlation points around it to make sure it is the lowest point in the area.
      <br/>
      If it is, then a true minimum has been found.
      <br/>
      If the number of true minima is greater than one per direction (x, y directions), then redundancy has been detected and the selected mapping point cannot be used.
    </p>
    <p num="38">
      The stitching program 16 in block 115 may also check to determine if the correlation has failed.
      <br/>
      Correlation failure detection is similar to redundancy failure detection (x or y).
      <br/>
      If the correlation areas of two images are mismatched, there would be no discernable minimum, which may result in multiple, unequal minima.
      <br/>
      Thus, determining the number of minima found in the selected correlation area provides a way to validate if the correlation was successful or not.
      <br/>
      The chart shown in FIG. 9 illustrates what the correlation data could look like if the correlation was unsuccessful.
    </p>
    <p num="39">
      If the point correlation is unsuccessful due either to a detected redundancy error or a correlation error (block 116), then a new correlation area is selected (step 113) in the overlap region OV12 or OV23 to find a new mapping point (blocks 110, 112, 114, 115).
      <br/>
      However, if the point correlation is indicated as being successful (block 116), then the stitching program 16 determines if enough mapping points (e.g., 4) are selected to perform the perspective transform or projective mapping (block 118).
      <br/>
      If not, the process starts from another corner of the image to find another feature-rich area to find the next mapping point and to correlate the point (blocks 110, 112, 113, 114, 115, and 116).
    </p>
    <p num="40">
      Given a correlation point at (xM, yM) that has been calculated by moving one image (e.g., Pk) over a second image (e.g., Pm), the stitching program 16 may optionally search for a sub-pixel position (a position located between two pixels) that provides even better correlation.
      <br/>
      The pixel located at (xM, yM), which has a better correlation than its neighbors, may be referred to as the maximal pixel.
      <br/>
      Aligning the images at this pixel provides a better match then at any other pixel, but in general, alignment of the images Pk and Pm at a fractional pixel offset may provide better alignment.
    </p>
    <p num="41">
      To find this sub-pixel position, a 3 * 3 array of pixels defined by pixel positions (xL, xM, xR) and (yT, yM, yB) is identified.
      <br/>
      In the x direction (xL, xM, xR) represents the x-positions of the maximal pixel and its two neighbors.
      <br/>
      Similarly, (yT, yM, yB) represents the y-positions of the maximal pixel and its top and bottom neighbors.
      <br/>
      In the middle column of the 3 * 3 array, correlation values CT, CM, and CB are defined at positions (xM, yT), (xM, yM) and (xM, yB), respectively.
      <br/>
      Similarly, in the middle row of the 3 * 3 array, correlation values CL, CM, and CR are defined at positions (xL, yM), (xM, yM), and (xR, yM), respectively.
      <br/>
      Since CM is a minimum in the entire correlation area, it will have a value less than CL, CR, CT, and CB.
      <br/>
      Thus, to calculate a sub-pixel position that may provide better alignment than the (xM, yM) position, the mapping point (x,y) may be modified according to Eqs. 8 and 9.  (Equation image '4' not included in text)
    </p>
    <p num="42">
      where x is computed from positions (xL, yM), (xM, yM) and (xR, yM) by using the correlation values (CL, CM and CR), while y is computed from position (xM, yB), (xM, yM), (xM, yT) by using the correlation values (CB, CM and CT).
      <br/>
      After the mapping points are selected, the perspective transform is performed (step 120).
      <br/>
      The current frame Pk (representing either image I1 or I3) is mapped using the perspective transform back to a reference frame, which in some embodiments may be the image I2.
    </p>
    <p num="43">In a perspective transform, the forward mapping function may be defined by Eqs. 10 and 11.  (Equation image '5' not included in text)</p>
    <p num="44">where (x,y) is the coordinate of the current frame Pk ; (u,v) is the coordinate of the reference frame P1 ; and the nine coefficients aij (i=1-3, j=1-3) define a 3 * 3 matrix T that represents the transform.  (Equation image '6' not included in text)</p>
    <p num="45">
      Without loss of generality, the matrix T can be normalized so that the coefficient a33 is set to unity (a33 =1).
      <br/>
      This leaves eight degrees of freedom for the projective mapping.
      <br/>
      The eight coefficients can be determined by establishing correspondence between four points in the reference frame P1 and the current frame Pk.
      <br/>
      Let (un, vn) and (xn, yn), for n=0, 1, 2, 3 be these four points in the P1 and Pk frames, respectively, and assuming a33 =1, Eqs. 10 and 11 will yield
      <br/>
      x=a11 u+a21 v+a31 -a13 ux-a23 vx,  (Eq. 13)
      <br/>
      y=a12 u+a22 v+a32 -a13 uy-a23 vy,  (Eq. 14)
    </p>
    <p num="46">Applying these new x, y equations (13 and 14) to the four pairs of corresponding points in frames P1 and Pk yields an 8 * 8 system, as shown in Eq. 15.  (Equation image '7' not included in text)</p>
    <p num="47">
      where A=�a11 a21 a31 a12 a22 a32 a13 a23 � are the unknown coefficients, an X=�x0 x1 x3 y0 y1 y2 y3 � are the known coordinates of the selected mapping points in the current frame Pk.
      <br/>
      Once the coefficients aij (i=1-3, j=1-3) are determined, the matrix T is known and can be used to perform the mapping of the frame Pk to the reference frame.
      <br/>
      The mapping performed using Eqs. 13 and 14 is done on a pixel-by-pixel basis.
    </p>
    <p num="48">
      At the kth frame, a set of k-1 transforms Tj (j=1, . . . , k-1) has been created to map the frame Pk back to the coordinates of the reference frame.
      <br/>
      The transforms Tj (j-1, . . . , k-1) are multiplied together to form a resultant transform that is applied to Pk.
      <br/>
      The aligned frame Pk is then added to the composite image using known techniques.
    </p>
    <p num="49">
      Next, for the given frame, the accumulated errors of the perspective transforms may be calculated and error correction performed in the error correction phase (block 122).
      <br/>
      The next frame is then retrieved and the frame processed (block 100).
    </p>
    <p num="50">
      Thus, the captured frame P1 (representing image I1) may be first mapped to the reference frame P2 (representing Image I2), and the two frames P1 and P2 may be combined into a composite image.
      <br/>
      Next, the captured frame P3 (representing image I3) may be mapped to P2 and added to the composite image.
    </p>
    <p num="51">
      Any vehicles that are not within the field of view of the camera 12c may be identified.
      <br/>
      Their images may be caused to flash, in one example, to indicate the vehicle is along side the vehicle A. By subtracting the image from camera 12c from the composite image, a vehicle along side the vehicle A may be identified.
      <br/>
      Pattern recognition or another technique may be used to separate vehicles from stationary objects along the roadway.
    </p>
    <p num="52">
      Referring to FIG. 10, in some embodiments, the computer 14 may include a microprocessor 80 that runs the stitching program 16 according to embodiments of the invention.
      <br/>
      The system memory 88, the microprocessor 80, and the bridge/system controller circuitry 84 are all coupled to a host bus 82.
      <br/>
      The bridge circuitry 84 provides an interface from the host bus 82 to a downstream bus 99 that is coupled to an I/O controller 90 and a network interface card at 92, as examples.
      <br/>
      The computer 14 may also have, as examples, a CD-ROM or DVD-ROM drive 100, a floppy disk drive 94, and/or a hard disk drive 96 as well as the display 11.
    </p>
    <p num="53">
      According to some embodiments, the stitching program 16 may be stored on a suitable mass storage medium, such as the CD-ROM or DVD-ROM drive 100, the floppy disk drive 94, or the hard disk drive 96.
      <br/>
      During execution, the program 16 or portions of the program 16 may be loaded into the system memory 88 for execution by the microprocessor 80.
      <br/>
      In addition, the captured frames may be stored in one of the listed storage media or in the cameras 12, or alternatively, in an external location.
    </p>
    <p num="54">
      Referring to FIG. 11, the cameras 12, in some embodiments, include optical components 60 that focus an optical image to be captured onto an array of pixel sensors 69 (e.g., a CMOS pixel sensor array) which electrically captures the image.
      <br/>
      Under control of a controller 62, an analog-to-digital (A/D) converter 64 receives analog signals from the sensor array 69 and furnishes the signals to other components in the camera 12 for further processing.
      <br/>
      These other components may include a scaling unit 66 that scales the resolution of the captured frame, if desired, and a compression unit 66 to compress the size of the frame that is transmitted over the communication link 15 through a bus interface 70.
    </p>
    <p num="55">
      While the invention has been disclosed with respect to a limited number of embodiments, those skilled in the art will appreciate numerous modifications and variations therefrom.
      <br/>
      It is intended that the appended claims cover all such modifications and variations as fall within the spirit and scope of the invention.
    </p>
  </description>
  <claims format="original" lang="en" id="claim_en">
    <claim num="1">
      <claim-text>What is claimed is:</claim-text>
      <claim-text>1.</claim-text>
      <claim-text>A rearview system for a vehicle comprising:</claim-text>
      <claim-text>first and second imaging devices to develop images of an area to the rear of a vehicle, said imaging devices each having a viewing angle which overlaps with the viewing angle of the other of said imaging devices in an overlap area;</claim-text>
      <claim-text>and a system to combine the images from said first and second imaging devices together to form a composite image such that the images from said first and second imaging devices are aligned with respect to a reference frame to compensate for movement of said imaging devices so that the overlap area is not duplicated in said composite image and to correct for movement of said imaging devices.</claim-text>
    </claim>
    <claim num="2">
      <claim-text>2. The system of claim 1 including a display adapted to display said composite image.</claim-text>
    </claim>
    <claim num="3">
      <claim-text>3. The system of claim 1 including a third imaging device mounted on one side of the vehicle, said second imaging device mounted on the other side of the vehicle, and said first imaging device mounted on the rear of the vehicle, said system also adapted to combine the image from the third imaging device together with the images from the first and second imaging devices.</claim-text>
    </claim>
    <claim num="4">
      <claim-text>4. The system of claim 1 wherein the first imaging device has a viewing angle which overlaps the viewing angle of the second imaging devices.</claim-text>
    </claim>
    <claim num="5">
      <claim-text>5. The system of claim 3 wherein said first and second devices are arranged to capture an image of a different scene around a vehicle, said system adapted to combine said images so as to produce a composite image having a viewing angle greater than that of either of said imaging devices alone.</claim-text>
    </claim>
    <claim num="6">
      <claim-text>6. The system of claim 5 adapted to produce a composite image of the entire area behind and to either side of said vehicle.</claim-text>
    </claim>
    <claim num="7">
      <claim-text>7. A method comprising: receiving an image from a first vehicle mounted imaging device having a viewing angle which overlaps the viewing angle of a second vehicle mounted imaging device such that there is duplication between the images received from said first and second vehicle mounted imaging devices;</claim-text>
      <claim-text>and combining said images from said first and second devices together to form a composite image of a region to the rear of a vehicle without duplication arising from the overlap between said devices by aligning the images from said first and second devices so that the overlap area is not duplicated in said composite image and the effects of motion of said imaging devices is removed.</claim-text>
    </claim>
    <claim num="8">
      <claim-text>8. The method of claim 7 including displaying said composite image on a display available for viewing by an operator of a vehicle.</claim-text>
    </claim>
    <claim num="9">
      <claim-text>9. The method of claim 7 including receiving an image from a third imaging device and combining the image from said third imaging device together with the images from the first and second imaging devices.</claim-text>
    </claim>
    <claim num="10">
      <claim-text>10. The method of claim 7 including combining together images from said first and second devices to produce a composite image having a viewing angle greater than that of either of said imaging devices alone.</claim-text>
    </claim>
    <claim num="11">
      <claim-text>11. The method of claim 7 including producing a composite image of the entire area behind and to either side of a vehicle.</claim-text>
    </claim>
  </claims>
</questel-patent-document>