<?xml version="1.0" encoding="UTF-8"?>
<questel-patent-document lang="en" date-produced="20180805" produced-by="Questel" schema-version="3.23" file="US06182046B2.xml">
  <bibliographic-data lang="en">
    <publication-reference publ-desc="Granted patent as second publication">
      <document-id>
        <country>US</country>
        <doc-number>06182046</doc-number>
        <kind>B2</kind>
        <date>20010130</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>US6182046</doc-number>
      </document-id>
    </publication-reference>
    <original-publication-kind>B2</original-publication-kind>
    <application-reference is-representative="YES" family-id="21956056" extended-family-id="42108807">
      <document-id>
        <country>US</country>
        <doc-number>09048714</doc-number>
        <kind>A</kind>
        <date>19980326</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>1998US-09048714</doc-number>
      </document-id>
      <document-id data-format="questel_Uid">
        <doc-number>43165364</doc-number>
      </document-id>
    </application-reference>
    <language-of-filing>en</language-of-filing>
    <language-of-publication>en</language-of-publication>
    <priority-claims>
      <priority-claim kind="national" sequence="1">
        <country>US</country>
        <doc-number>4871498</doc-number>
        <kind>A</kind>
        <date>19980326</date>
        <priority-active-indicator>Y</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="1">
        <doc-number>1998US-09048714</doc-number>
      </priority-claim>
    </priority-claims>
    <dates-of-public-availability>
      <publication-of-grant-date>
        <date>20010130</date>
      </publication-of-grant-date>
    </dates-of-public-availability>
    <classifications-ipcr>
      <classification-ipcr sequence="1">
        <text>G10L  15/22        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>10</class>
        <subclass>L</subclass>
        <main-group>15</main-group>
        <subgroup>22</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
    </classifications-ipcr>
    <classification-national>
      <country>US</country>
      <main-classification>
        <text>704275000</text>
        <class>704</class>
        <subclass>275000</subclass>
      </main-classification>
      <further-classification sequence="1">
        <text>704272000</text>
        <class>704</class>
        <subclass>272000</subclass>
      </further-classification>
      <further-classification sequence="2">
        <text>704E15040</text>
        <class>704</class>
        <subclass>E15040</subclass>
      </further-classification>
    </classification-national>
    <classifications-ecla>
      <classification-ecla sequence="1">
        <text>G10L-015/22</text>
        <section>G</section>
        <class>10</class>
        <subclass>L</subclass>
        <main-group>15</main-group>
        <subgroup>22</subgroup>
      </classification-ecla>
    </classifications-ecla>
    <patent-classifications>
      <patent-classification sequence="1">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>G10L-015/22</classification-symbol>
        <section>G</section>
        <class>10</class>
        <subclass>L</subclass>
        <main-group>15</main-group>
        <subgroup>22</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20180223</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="2">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>G10L-2015/228</classification-symbol>
        <section>G</section>
        <class>10</class>
        <subclass>L</subclass>
        <main-group>2015</main-group>
        <subgroup>228</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20180221</date>
        </action-date>
      </patent-classification>
    </patent-classifications>
    <number-of-claims>11</number-of-claims>
    <exemplary-claim>1</exemplary-claim>
    <figures>
      <number-of-drawing-sheets>4</number-of-drawing-sheets>
      <number-of-figures>4</number-of-figures>
      <image-key data-format="questel">US6182046</image-key>
    </figures>
    <invention-title format="original" lang="en" id="title_en">Managing voice commands in speech applications</invention-title>
    <references-cited>
      <citation srep-phase="examiner">
        <patcit num="1">
          <text>WHITE GEORGE M</text>
          <document-id>
            <country>US</country>
            <doc-number>5386494</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5386494</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="2">
          <text>ROZAK MICHAEL J, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5748191</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5748191</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="3">
          <text>NOLAN JOSEPH C</text>
          <document-id>
            <country>US</country>
            <doc-number>5754873</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5754873</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="4">
          <text>DE ARMAS MARIO E, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5864819</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5864819</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="5">
          <text>VAN KLEECK MICHAEL HINKLEY, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5890122</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5890122</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <nplcit num="1">
          <text>IBM Technical Disclosure Bulletin. Integrated Audio Graphics User Interface, p. 368-371. Apr. 1991.</text>
        </nplcit>
      </citation>
    </references-cited>
    <parties>
      <applicants>
        <applicant data-format="original" app-type="applicant" sequence="1">
          <addressbook lang="en">
            <orgname>International Business Machines Corp.</orgname>
            <address>
              <address-1>Armonk, NY, US</address-1>
              <city>Armonk</city>
              <state>NY</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </applicant>
        <applicant data-format="questel" app-type="applicant" sequence="2">
          <addressbook lang="en">
            <orgname>IBM</orgname>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </applicant>
      </applicants>
      <inventors>
        <inventor data-format="original" sequence="1">
          <addressbook lang="en">
            <name>Ortega, Kerry A.</name>
            <address>
              <address-1>Deerfield Beach, FL, US</address-1>
              <city>Deerfield Beach</city>
              <state>FL</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
        <inventor data-format="original" sequence="2">
          <addressbook lang="en">
            <name>Boyer, Linda M.</name>
            <address>
              <address-1>Boca Raton, FL, US</address-1>
              <city>Boca Raton</city>
              <state>FL</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
        <inventor data-format="original" sequence="3">
          <addressbook lang="en">
            <name>Kist, Thomas A.</name>
            <address>
              <address-1>Boynton Beach, FL, US</address-1>
              <city>Boynton Beach</city>
              <state>FL</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
        <inventor data-format="original" sequence="4">
          <addressbook lang="en">
            <name>Lewis, James R.</name>
            <address>
              <address-1>Delray Beach, FL, US</address-1>
              <city>Delray Beach</city>
              <state>FL</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
        <inventor data-format="original" sequence="5">
          <addressbook lang="en">
            <name>Ballard, Barbara E.</name>
            <address>
              <address-1>Raleigh, NC, US</address-1>
              <city>Raleigh</city>
              <state>NC</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
        <inventor data-format="original" sequence="6">
          <addressbook lang="en">
            <name>VanBuskirk, Ronald</name>
            <address>
              <address-1>Indiantown, FL, US</address-1>
              <city>Indiantown</city>
              <state>FL</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
        <inventor data-format="original" sequence="7">
          <addressbook lang="en">
            <name>Keller, Arthur</name>
            <address>
              <address-1>Boca Raton, FL, US</address-1>
              <city>Boca Raton</city>
              <state>FL</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
      </inventors>
      <agents>
        <agent sequence="1" rep-type="agent">
          <addressbook lang="en">
            <orgname>Quarles &amp; Brady LLP</orgname>
          </addressbook>
        </agent>
      </agents>
    </parties>
    <examiners>
      <primary-examiner>
        <name>Dorvil, Richemond</name>
      </primary-examiner>
    </examiners>
    <lgst-data>
      <lgst-status>EXPIRED</lgst-status>
    </lgst-data>
  </bibliographic-data>
  <abstract format="original" lang="en" id="abstr_en">
    <p id="P-EN-00001" num="00001">
      <br/>
      A method for managing a What Can I Say (WCIS) function in an application having a plurality of commands which can be voice activated comprises the steps of: storing a set of substantially all voice activatable commands associated with the application; identifying those of the commands in the set which are displayable by the application; and, in response to a user input, displaying in a graphical user interface (GUI) a subset of the voice activatable commands which are not displayable by the application.
      <br/>
      Moreover, the method includes displaying in the GUI, in response to a user input, a list of the stored set of substantially all voice activatable commands associated with the application; displaying in the GUI a pull down menu identifying different categories by which the commands can be viewed in the list; and, displaying the GUI with a pull down menu identifying commands that can be performed against a voice command.
    </p>
  </abstract>
  <description format="original" lang="en" id="desc_en">
    <heading>BACKGROUND OF THE INVENTION</heading>
    <p num="1">
      1.
      <br/>
      Field of the Invention
    </p>
    <p num="2">This invention relates to the field of managing speech applications, and in particular, to an improved "What Can I Say" function.</p>
    <p num="3">2. Description of Related Art</p>
    <p num="4">
      The What-Can-I-Say (WCIS) function is critical to speech product usability.
      <br/>
      WCIS is important even for experienced users.
      <br/>
      Even though a user's dependence may decrease as the user gains experience, there are always instances when even the most experienced user wants to know how to issue a voice command and whether or not a particular action is voice-enabled.
      <br/>
      The use of WCIS information is especially fundamental to a speech recognition application, where its usability is of major concern.
      <br/>
      All users, regardless of experience level, find usability value in the WCIS function.
    </p>
    <p num="5">
      Previous versions of speech products had WCIS functions.
      <br/>
      The problem with these previous versions is that the functionality was too complicated for the user to find the voice commands they were looking for.
      <br/>
      In the Simply Speaking Gold application, for example, the WCIS window used tabs for the different categories of commands.
      <br/>
      When the number of tabs exceeds four or five, the name of the command becomes harder to see and the overall look is very cluttered.
      <br/>
      Subcategories were indicated by using the "+" and "-" signs.
    </p>
    <p num="6">
      In previous designs of WCIS, all voice commands were shown even if it was otherwise quite evident to the users what the commands were.
      <br/>
      Previous versions of WCIS showed the complete menu structure of an application even though to navigate a menu item was as simple as saying the menu name.
      <br/>
      The same is true of buttons.
      <br/>
      This approach resulted in a very complicated structure for users to find voice commands.
    </p>
    <heading>SUMMARY OF THE INVENTION</heading>
    <p num="7">
      Voice commands are much easier for the user to find in accordance with the inventive arrangements.
      <br/>
      The inventive arrangements are based on realization of a simple but elegant principle, namely that the user can always say what the user sees, and accordingly, a WCIS function only needs to show the commands a user cannot see.
      <br/>
      This principle enables the structure of a WCIS function to be greatly simplified by dividing voice commands into several categories and then only showing those voice commands that cannot be seen in the graphical user interface (GUI) of the speech application.
    </p>
    <p num="8">
      Functionality is further enhanced in the inventive arrangements by providing a simple alphabetic list of all the available voice commands that is easily searched.
      <br/>
      One of the reasons to provide such a list of all commands is to provide the user some way of training a command even though they can see it in the GUI.
    </p>
    <p num="9">A method for managing a What Can I Say (WCIS) function in an application having a plurality of commands which can be voice activated, in accordance with an inventive arrangement, comprises the steps of: storing a set of substantially all voice activatable commands associated with the application; identifying those of the commands in the set which are being displayed by the application; and, displaying in a graphical user interface, in response to a user input, a subset of the voice activatable commands which are not being displayed.</p>
    <p num="10">The method comprise the step of continuously tracking those of the commands in the set which are being displayed by the application or identifying those of the commands in the set which are being displayed by the application in response to the user input, prior to the displaying step.</p>
    <p num="11">The subset of the commands are advantageously arranged by category.</p>
    <p num="12">The method can further comprise the step of displaying in the GUI a list of the stored set of substantially all voice activatable commands associated with the application.</p>
    <p num="13">The method can further comprise the steps of providing the GUI with a pull down menu identifying different categories by which the commands can be viewed in the GUI and/or providing the GUI with a pull down menu identifying commands that can be performed against a voice command.</p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading>
    <p num="14">
      FIG. 1 is a GUI embodying of a WCIS Active Program view in accordance with the inventive arrangements.
      <br/>
      FIG. 2 is the GUI of FIG. 1, showing a Notes section.
      <br/>
      FIG. 3 is the GUI of FIG. 1, showing a list of commands available in the View pull-down menu.
      <br/>
      FIG. 4 is the GUI of FIG. 1, showing a list of commands available in the Action pull-down menu.
    </p>
    <heading>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
    <p num="15">
      The WCIS function can be made available to all speech aware applications on a computer system.
      <br/>
      Programmers can add their own view with their active commands to the WCIS function.
      <br/>
      Applications can also add their own view with their active commands to the WCIS function.
      <br/>
      For example, dictation applications can display inline commands, that is commands which are spoken during dictation, dictation macros and a Stop Dictation command in a WCIS GUI window, such as the WCIS GUI window shown in FIG. 1.
      <br/>
      When dictation stops, the WCIS window can be updated with a Begin Correction and other valid commands.
    </p>
    <p num="16">
      The general design for navigation by speech is to say what you can see, for example, the user can utter the name of a user interface control or command that the user sees, and the command will be executed.
      <br/>
      However, in accordance with the inventive arrangements, the WCIS accesses commands the user cannot see.
      <br/>
      Examples of Desktop commands which cannot be seen are GOTO &lt;App name&gt;, where app name is the name of a particular application the user wants to invoke, or ViaVoice commands like Microphone Off.
      <br/>
      The inventive arrangement assumes, in a sense, that the user knows about the basic rule of Say What You Can See.
      <br/>
      Therefore, the WCIS in accordance with the inventive arrangement advantageously does not show commands like menus, buttons and others that the user can currently see.
      <br/>
      The new WCIS function shows only those valid commands which the user cannot see, for example, global commands such as those that control a speech recognition application.
      <br/>
      In this way, the amount of information which must be displayed is significantly reduced, which greatly facilitates the ease with which the remaining commands can be accessed and displayed.
    </p>
    <p num="17">
      Most of the information in WCIS is divided into several categories.
      <br/>
      The user can access different views to see the different command names.
      <br/>
      The user accesses the categories via the WCIS View menu shown in FIG. 1.
      <br/>
      At the bottom of the WCIS window is an area where the user can type notes.
      <br/>
      As the user changes views the notes also change.
    </p>
    <p num="18">
      The WCIS window can be opened by the user choosing WCIS from a menu in a speech application or by using the voice command "What Can I Say." If the application had been invoked previously, the WCIS window can appear as it last did in the previous invocation of the application.
      <br/>
      If the application is being invoked for the first time, or no previous condition of the WCIS window is of record, the WCIS window advantageously defaults to a Search view.
      <br/>
      In addition, the user can also say "How Do I Say" to open the WCIS window and automatically display the Search view.
      <br/>
      The WCIS Search view has a list of all commands which can be uttered.
    </p>
    <p num="19">
      FIG. 1 shows an example of a WCIS Active Program view in the form of a GUI 10 having a window 12.
      <br/>
      The list 26 of commands displayed in area 24 the Active Program view embodied in window 10 are the menu items that contain accelerator keys, also referred to as hot keys, and the user interface controls of the active program.
      <br/>
      The active program is also referred to as the program that has focus.
      <br/>
      The program with focus has the uppermost window and responds to inputs from the keyboard or other controllers.
      <br/>
      A dictation application operating with a drawing application, denoted Paint in the Figures, for example, will have included in the list 26 such commands as Close, New, Open, Save, Print, Exit, Undo, Paste, Select All, and the like.
      <br/>
      This view is present if the user has enabled Active Program commands when such functionality is available in a speech application.
    </p>
    <p num="20">
      Any WCIS view can be provided with pull down menus in an upper area 14 of the window or view.
      <br/>
      The menus can include, for example, a View menu 16, an Action menu 18 and a Help menu 20.
      <br/>
      The View and Action menus are described below.
      <br/>
      The Help menu is conventional and need not be explained.
      <br/>
      It should be noted that each of the menus can be invoked by uttering View, Actions and Help, or the like, depending upon the particular application.
    </p>
    <p num="21">
      The View menu 16 provides the different categories in which the speech commands can be provided.
      <br/>
      For example, the menu can contain a list 32 of the following categories, as shown in FIG. 3.
      <br/>
      All Commands brings up a special page on which users can specify synonyms for the command they are looking for.
      <br/>
      This view contains an alphabetic list of all the commands.
      <br/>
      Typing or dictating into the entry field on this page will only display matching commands.
      <br/>
      VoiceCenter lists the voice commands for a speech application called ViaVoice.
      <br/>
      Active Program contains Dialog controls, tool bar names and menu items with keyboard accelerator keys.
      <br/>
      The user can say these menu items even if they are not currently visible.
      <br/>
      If the user wants to Paste something from the clipboard, for example, instead of saying Edit and then Paste, the user can just say Paste because Paste has a keyboard accelerator, namely CTRL+V (simultaneously pressing the Control and V keys).
      <br/>
      Desktop contains the list of program names and associated program control commands like Start, Open, Goto, Minimize, Maximize and the like.
      <br/>
      Text Editing contains cursor movement, text editing, text selection and clipboard commands.
      <br/>
      Keyboard contains all the keyboard commands, for example Enter, Page down, and the like.
      <br/>
      Untrained is provided in the event the speech system does not have baseforms for all the commands of an application, because the application must be trained to understand such commands.
      <br/>
      A baseform is a description of how a word sounds or is pronounced.
      <br/>
      This view lists, alphabetically, all of the controls that do not have baseforms.
      <br/>
      The user can then select these commands and train them.
      <br/>
      The commands listed are for the current active window of the application.
      <br/>
      Accordingly, if a dialog is open, the untrained commands for that dialog are listed. &lt;Name x&gt; is a generic description for active applications.
      <br/>
      The developer of an application can provides one or more pages to place in the WCIS window.
      <br/>
      These commands are generally ones the user does not see.
      <br/>
      A dictation application, for example, would list the inline commands on this page.
      <br/>
      If the application does not supply anything, the page does not appear.
      <br/>
      The application can choose to change the information based on the state of their application.
      <br/>
      Keyboard accelerators are defined by the application.
      <br/>
      Refresh refreshes the window.
    </p>
    <p num="22">
      The Action menu 18 contains commands that can be performed against a voice command.
      <br/>
      The menu might contain a list 34 as shown in FIG. 4, including for example actions to do the following.
      <br/>
      Train brings up the train word dialog to train the command selected in the WCIS window.
      <br/>
      The Active Program page might show the command Skew, for example, which has no baseform.
      <br/>
      The user can select the command and then choose Train.
      <br/>
      Double-clicking on a command can also bring up this dialog.
      <br/>
      Print prints the current page.
      <br/>
      The user can select to Print as Shown or All Commands from the print dialog.
      <br/>
      Print as shown is the default.
      <br/>
      Always on Top forces the WCIS window to be on top of the active application.
    </p>
    <p num="23">
      A Notes section 30 of the WCIS Program view is shown in FIG. 2 and provides a free form edit field for reminders and the like.
      <br/>
      The Notes section opened by activating button 36 and appears at the bottom of the WCIS window 12.
      <br/>
      The user can type and/or dictate into this area.
      <br/>
      Each page has its own notes.
      <br/>
      As the user changes pages, the notes change.
      <br/>
      For the application pages, each application page has its own note and the system saves the user's notes across invocations, that is, across multiple starts and stops of the application.
      <br/>
      As the user switches between applications, the application's pages change as well as the notes associated with the application's pages.
      <br/>
      As the user switches to another application, the notes on the Active Program page change to those for the new active application.
      <br/>
      This section can be expanded and collapsed, for example, by a push button or other activatable icon.
    </p>
    <p num="24">The WCIS function advantageously searches for programs in the Windows Start button (of Windows 95, for example) and on the desktop and automatically populates its internal program list with them.</p>
    <p num="25">
      In general, the WCIS views use the turning triangles 28 for expanding or collapsing lists.
      <br/>
      The text in bold indicates subcategory names, that is, the ones associated with the turning triangles.
      <br/>
      The How Do I Say global command launches the WCIS window on the Search view, as noted above.
    </p>
    <p num="26">
      All commands can be searched by a Search view that comes up with the entry field blank and lists all the commands available.
      <br/>
      As the user types or dictates, the display field updates with the possible commands.
      <br/>
      Once the user enters a search string, only commands matching the search string are displayed in the list.
    </p>
    <p num="27">
      It will be appreciated that the commands which need to be displayed, and those which are not to be displayed, will change as the user changes applications and windows in the applications as different sets of commands are displayed.
      <br/>
      One means by which the displayed commands can be tracked is the active accessibility function provided by the Microsoft operating system.
    </p>
    <p num="28">
      The inventive arrangements taught herein provide a significantly more usable and user-friendly WCIS GUI than previous designs.
      <br/>
      The inventive arrangements are based on the notion of only showing commands that cannot be seen in the GUI.
      <br/>
      Categories of commands improve the ease of finding a specific command among those that are shown.
      <br/>
      A Search view looks at synonyms.
      <br/>
      Finally, a notes window is provided for each application, which survives successive invocations of the applications.
    </p>
  </description>
  <claims format="original" lang="en" id="claim_en">
    <claim num="1">
      <claim-text>What is claimed is:</claim-text>
      <claim-text>1.</claim-text>
      <claim-text>A method for managing a What Can I Say (WCIS) function in a first application having a plurality of commands each of which can be voice activated by a command word, comprising the steps of:</claim-text>
      <claim-text>storing a set of substantially all voice activatable commands associated with said first application; identifying those of said commands in said set which are displayable by said first application;</claim-text>
      <claim-text>and, in response to a user input, displaying in a graphical user interface of a second application a subset of said command words not displayable by said first application.</claim-text>
    </claim>
    <claim num="2">
      <claim-text>2. The method of claim 1, comprising the step of continuously tracking those of said commands in said set which are displayable by said first application.</claim-text>
    </claim>
    <claim num="3">
      <claim-text>3. The method of claim 1, comprising the step of identifying those of said commands in said set which are displayable by said first application in response to said user input, prior to said displaying step.</claim-text>
    </claim>
    <claim num="4">
      <claim-text>4. The method of claim 1, further comprising the step of arranging said subset of said commands by category.</claim-text>
    </claim>
    <claim num="5">
      <claim-text>5. The method of claim 1, further comprising the step of displaying in said GUI a list of said stored set of substantially all voice activatable commands associated with said first application.</claim-text>
    </claim>
    <claim num="6">
      <claim-text>6. The method of claim 1, further comprising the step of providing said GUI with a pull down menu identifying different categories by which said commands can be viewed in said GUI.</claim-text>
    </claim>
    <claim num="7">
      <claim-text>7. The method of claim 1, further comprising the step of providing said GUI with a pull down menu identifying commands that can be performed against a voice command.</claim-text>
    </claim>
    <claim num="8">
      <claim-text>8. A method for managing a What Can I Say (WCIS) function in a first application having a plurality of commands each of which can be voice activated by a command word, comprising the steps of: storing a set of substantially all voice activatable commands associated with said first application; identifying those of said commands in said set which are displayable by said first application; in response to a first user input, displaying in a graphical user interface (GUI) of a second application a subset of said command words not displayable by said first application; displaying in said GUI, in response to a second user input, a list of said stored set of substantially all voice activatable commands associated with said first application; displaying in said GUI a first pull down menu identifying different categories by which said commands can be viewed in said list;</claim-text>
      <claim-text>and, displaying said GUI with a second pull down menu identifying commands that can be performed against a voice command.</claim-text>
    </claim>
    <claim num="9">
      <claim-text>9. The method of claim 8, comprising the step of continuously tracking those of said commands in said set which are displayable by said first application.</claim-text>
    </claim>
    <claim num="10">
      <claim-text>10. The method of claim 8, comprising the step of identifying those of said commands in said set which are displayable by said first application in response to said user input, prior to said displaying step.</claim-text>
    </claim>
    <claim num="11">
      <claim-text>11. The method of claim 8, further comprising the step of arranging said subset of said commands by category.</claim-text>
    </claim>
  </claims>
</questel-patent-document>