<?xml version="1.0" encoding="UTF-8"?>
<questel-patent-document lang="en" date-produced="20180805" produced-by="Questel" schema-version="3.23" file="US06188726B1.xml">
  <bibliographic-data lang="en">
    <publication-reference publ-desc="Granted patent as first publication">
      <document-id>
        <country>US</country>
        <doc-number>06188726</doc-number>
        <kind>B1</kind>
        <date>20010213</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>US6188726</doc-number>
      </document-id>
    </publication-reference>
    <original-publication-kind>B1</original-publication-kind>
    <application-reference is-representative="YES" family-id="15566942" extended-family-id="23603488">
      <document-id>
        <country>US</country>
        <doc-number>08733788</doc-number>
        <kind>A</kind>
        <date>19961018</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>1996US-08733788</doc-number>
      </document-id>
      <document-id data-format="questel_Uid">
        <doc-number>43177593</doc-number>
      </document-id>
    </application-reference>
    <language-of-filing>en</language-of-filing>
    <language-of-publication>en</language-of-publication>
    <priority-claims>
      <priority-claim kind="national" sequence="1">
        <country>US</country>
        <doc-number>73378896</doc-number>
        <kind>A</kind>
        <date>19961018</date>
        <priority-active-indicator>N</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="1">
        <doc-number>1996US-08733788</doc-number>
      </priority-claim>
      <priority-claim kind="national" sequence="2">
        <country>JP</country>
        <doc-number>15364093</doc-number>
        <kind>A</kind>
        <date>19930624</date>
        <priority-active-indicator>Y</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="2">
        <doc-number>1993JP-0153640</doc-number>
      </priority-claim>
      <priority-claim kind="national" sequence="3">
        <country>US</country>
        <doc-number>25918594</doc-number>
        <kind>A</kind>
        <date>19940613</date>
        <priority-linkage-type>B</priority-linkage-type>
        <priority-active-indicator>N</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="3">
        <doc-number>1994US-08259185</doc-number>
      </priority-claim>
    </priority-claims>
    <dates-of-public-availability>
      <publication-of-grant-date>
        <date>20010213</date>
      </publication-of-grant-date>
    </dates-of-public-availability>
    <classifications-ipcr>
      <classification-ipcr sequence="1">
        <text>H03M   7/40        20060101AFI20051220RMJP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>H</section>
        <class>03</class>
        <subclass>M</subclass>
        <main-group>7</main-group>
        <subgroup>40</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>JP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051220</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="2">
        <text>G06T   9/00        20060101ALI20051220RMJP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>06</class>
        <subclass>T</subclass>
        <main-group>9</main-group>
        <subgroup>00</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>JP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051220</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="3">
        <text>H04N   7/14        20060101ALI20051220RMJP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>7</main-group>
        <subgroup>14</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>JP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051220</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="4">
        <text>H04N   7/26        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>7</main-group>
        <subgroup>26</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="5">
        <text>H04N   7/30        20060101ALI20051220RMJP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>7</main-group>
        <subgroup>30</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>JP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051220</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="6">
        <text>H04N   7/32        20060101ALI20051220RMJP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>7</main-group>
        <subgroup>32</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>JP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051220</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="7">
        <text>H04N   7/36        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>7</main-group>
        <subgroup>36</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="8">
        <text>H04N  11/04        20060101ALI20051220RMJP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>11</main-group>
        <subgroup>04</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>JP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051220</date>
        </action-date>
      </classification-ipcr>
    </classifications-ipcr>
    <classification-national>
      <country>US</country>
      <main-classification>
        <text>375240030</text>
        <class>375</class>
        <subclass>240030</subclass>
      </main-classification>
      <further-classification sequence="1">
        <text>375E07085</text>
        <class>375</class>
        <subclass>E07085</subclass>
      </further-classification>
      <further-classification sequence="2">
        <text>375E07263</text>
        <class>375</class>
        <subclass>E07263</subclass>
      </further-classification>
    </classification-national>
    <patent-classifications>
      <patent-classification sequence="1">
        <classification-scheme office="EP" scheme="CPC">
          <date>20141101</date>
        </classification-scheme>
        <classification-symbol>H04N-019/23</classification-symbol>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>19</main-group>
        <subgroup>23</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20141108</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="2">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>H04N-019/503</classification-symbol>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>19</main-group>
        <subgroup>503</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20141108</date>
        </action-date>
      </patent-classification>
    </patent-classifications>
    <number-of-claims>13</number-of-claims>
    <exemplary-claim>1</exemplary-claim>
    <figures>
      <number-of-drawing-sheets>2</number-of-drawing-sheets>
      <number-of-figures>2</number-of-figures>
      <image-key data-format="questel">US6188726</image-key>
    </figures>
    <invention-title format="original" lang="en" id="title_en">Coding/decoding apparatus</invention-title>
    <references-cited>
      <citation srep-phase="examiner">
        <patcit num="1">
          <text>KURODA HIDEO, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4591909</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4591909</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="2">
          <text>HATORI YOSHINORI, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4636862</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4636862</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="3">
          <text>OHKI JUNICHI, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4689671</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4689671</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="4">
          <text>YASUDA AKIRA, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4703347</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4703347</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="5">
          <text>IINUMA KAZUMOTO, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4802006</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4802006</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="6">
          <text>OZEKI KAZUO, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4833535</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4833535</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="7">
          <text>MURAKAMI TOKUMICHI, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5057940</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5057940</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="8">
          <text>HASEBE ATSUSHI</text>
          <document-id>
            <country>US</country>
            <doc-number>5097327</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5097327</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="9">
          <text>KONDO TETSUJIRO</text>
          <document-id>
            <country>US</country>
            <doc-number>5193003</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5193003</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="10">
          <text>HENOT JEAN-PIERRE</text>
          <document-id>
            <country>US</country>
            <doc-number>5196933</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5196933</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="11">
          <text>INOUE IKUO</text>
          <document-id>
            <country>US</country>
            <doc-number>5237410</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5237410</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="12">
          <text>FUKUHARA TAKAHIRO, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5247590</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5247590</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="13">
          <text>HUI LUCAS Y</text>
          <document-id>
            <country>US</country>
            <doc-number>5260782</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5260782</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="14">
          <text>LIPPMAN ANDREW B, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5262856</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5262856</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="15">
          <text>AONO TOMOKO, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5267333</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5267333</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="16">
          <text>MAEDA MITSURU</text>
          <document-id>
            <country>US</country>
            <doc-number>5274453</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5274453</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="17">
          <document-id>
            <country>JP</country>
            <doc-number>36201228</doc-number>
            <kind>B</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>JP61201228</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="18">
          <document-id>
            <country>JP</country>
            <doc-number>36212383</doc-number>
            <kind>B</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>JP61212383</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="19">
          <document-id>
            <country>JP</country>
            <doc-number>36215905</doc-number>
            <kind>B</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>JP61215905</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="20">
          <document-id>
            <country>JP</country>
            <doc-number>40115469</doc-number>
            <kind>B</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>JP65115469</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="21">
          <document-id>
            <country>JP</country>
            <doc-number>40224128</doc-number>
            <kind>B</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>JP65224128</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="22">
          <document-id>
            <country>JP</country>
            <doc-number>40324199</doc-number>
            <kind>B</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>JP65324199</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="23">
          <document-id>
            <country>JP</country>
            <doc-number>40428748</doc-number>
            <kind>B</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>JP65428748</doc-number>
          </document-id>
        </patcit>
      </citation>
    </references-cited>
    <related-documents>
      <continuation>
        <relation>
          <parent-doc>
            <document-id>
              <country>US</country>
              <doc-number>25918594</doc-number>
              <kind>A</kind>
              <date>19940613</date>
            </document-id>
            <parent-status>ABANDONED</parent-status>
          </parent-doc>
        </relation>
      </continuation>
    </related-documents>
    <parties>
      <applicants>
        <applicant data-format="original" app-type="applicant" sequence="1">
          <addressbook lang="en">
            <orgname>Canon Kabushiki Kaisha</orgname>
            <address>
              <address-1>Tokyo, JP</address-1>
              <city>Tokyo</city>
              <country>JP</country>
            </address>
          </addressbook>
          <nationality>
            <country>JP</country>
          </nationality>
        </applicant>
        <applicant data-format="questel" app-type="applicant" sequence="2">
          <addressbook lang="en">
            <orgname>CANON</orgname>
          </addressbook>
          <nationality>
            <country>JP</country>
          </nationality>
        </applicant>
      </applicants>
      <inventors>
        <inventor data-format="original" sequence="1">
          <addressbook lang="en">
            <name>Hirabayashi, Yasuji</name>
            <address>
              <address-1>Kanagawa-ken, JP</address-1>
              <city>Kanagawa-ken</city>
              <country>JP</country>
            </address>
          </addressbook>
          <nationality>
            <country>JP</country>
          </nationality>
        </inventor>
      </inventors>
      <agents>
        <agent sequence="1" rep-type="agent">
          <addressbook lang="en">
            <orgname>Robin, Blecker &amp; Daley</orgname>
          </addressbook>
        </agent>
      </agents>
    </parties>
    <examiners>
      <primary-examiner>
        <name>Le, Vu</name>
      </primary-examiner>
    </examiners>
    <lgst-data>
      <lgst-status>EXPIRED</lgst-status>
    </lgst-data>
  </bibliographic-data>
  <abstract format="original" lang="en" id="abstr_en">
    <p id="P-EN-00001" num="00001">
      <br/>
      A coding apparatus is arranged to eliminate predetermined image data from input image data and code the input image data from which the predetermined image data is eliminated.
      <br/>
      A decoding apparatus is arranged to decode the coded image data from which the predetermined image data is eliminated and combine desired image data with the decoded image data.
    </p>
  </abstract>
  <description format="original" lang="en" id="desc_en">
    <p num="1">
      This is a continuation application under 37 CFR 1.62 of prior application Ser.
      <br/>
      No. 08/259,185, filed Jun. 13, 1994, now abandoned.
    </p>
    <heading>BACKGROUND OF THE INVENTION</heading>
    <p num="2">
      1.
      <br/>
      Field of the Invention
    </p>
    <p num="3">The present invention relates to a coding/decoding apparatus capable of achieving highly efficient coding of input image data.</p>
    <p num="4">2. Description of the Related Art</p>
    <p num="5">
      Moving image coding methods are classified into three representative methods: an interframe differential coding method; a motion compensation interframe differential coding method and an intraframe coding method.
      <br/>
      In practice, one picture is divided into a plurality of blocks, and such a moving image coding method is applied to each of the blocks.
    </p>
    <p num="6">
      The interframe differential coding method is intended to code the difference between a block to be coded and a block which occupies a spatially identical position in the previous frame.
      <br/>
      Since the stronger the correlation between frames, the closer to zero the difference value between the blocks, the interframe differential coding method can achieve a higher compression ratio for a lower-activity image.
    </p>
    <p num="7">
      The motion compensation interframe differential coding method is a modification of the interframe differential coding method into which motion compensation is introduced.
      <br/>
      In the motion compensation interframe differential coding method, a difference value is found between a block contained in one frame and the most approximate block selected from among neighboring blocks surrounding a block which occupies a spatially identical position in the previous frame, and the difference value is coded.
    </p>
    <p num="8">
      For example, difference data is subjected to discrete cosine transform, and after the resultant transform coefficient is quantized, Huffman coding is performed.
      <br/>
      Even if a moving object is present in a picture, the difference value obtained from the moving object can be made small.
      <br/>
      Accordingly, the motion compensation interframe differential coding method can achieve a high compression ratio even for a high-activity image.
    </p>
    <p num="9">
      The intraframe coding method is intended to perform coding within only a single picture.
      <br/>
      Specifically, an image of interest is directly subjected to discrete cosine transform, and after the resultant transform coefficient is quantized, Huffman coding is performed.
      <br/>
      If either of the aforesaid difference-value coding methods, i.e., the interframe differential coding method or the motion compensation interframe differential coding method, is employed with an interframe correlation small, the process of finding the difference value causes an increase in a dynamic range, and the amount of information to be processed increases.
      <br/>
      The intraframe coding method does not involve such a problem.
    </p>
    <p num="10">The intraframe coding method is suited to coding of an image immediately after a scene change, whereas the motion compensation interframe differential coding method is best suited to coding of ordinary low-activity moving images.</p>
    <p num="11">
      However, in the motion compensation interframe differential coding method, if a still background is contained in a block, a detection error occurs and the compression ratio is lowered.
      <br/>
      More specifically, a matching computation for detection of a motion vector is performed on the assumption that the still background is relatively moving with respect to a moving object.
      <br/>
      As a result, if the moving object and the background prevail in each block, the effect of motion compensation decreases and, in the worst case, serves as an impairment factor.
      <br/>
      In other words, if an accurate motion compensation is applied to a moving object, the motion compensation is performed on the assumption that a background image is moving, and this process of the motion compensation increases the amount of generated codes.
    </p>
    <heading>SUMMARY OF THE INVENTION</heading>
    <p num="12">An object of the present invention which has been made in the light of the above-described background is to provide a coding/decoding apparatus capable of highly efficiently coding any kind of input image data.</p>
    <p num="13">To achieve the above object, in accordance with one aspect of the present invention, there is provided a coding apparatus which comprises inputting means for inputting image data, extracting means for extracting predetermined image data from the input image data, and coding means for coding the predetermined image data extracted by the extracting means.</p>
    <p num="14">In accordance with another aspect of the present invention, there is provided a decoding apparatus which comprises decoding means for decoding, into a moving image, moving image information formed by coding only a non-background portion, background memory means for storing a background image, and combining means for combining the background image outputted from the background memory means with the moving image outputted from the decoding means.</p>
    <p num="15">
      Other objects, features and advantages of the present invention will become apparent from the following detailed description taken in conjunction with the accompanying drawings.
      <br/>
      BRIEF DESCRIPTION OF THE DRAWINGS
      <br/>
      FIG. 1 is a block diagram schematically showing a moving image transmitting apparatus according to one embodiment of the present invention; and
      <br/>
      FIG. 2 is a block diagram schematically showing a moving image receiving apparatus according to one embodiment of the present invention.
    </p>
    <heading>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT</heading>
    <p num="16">
      A preferred embodiment of the present invention will be described below with reference to the accompanying drawings.
      <br/>
      By way of example, in the following description, reference will be made to a video conference system which includes a moving image transmitting apparatus for coding a moving image in which only a person moves against a still background image into data compressed at a high compression ratio and transmitting the coded data, and a moving image receiving apparatus for decoding and reproducing the coded data transmitted from the transmitting apparatus.
    </p>
    <p num="17">FIG. 1 is a block diagram of the moving image transmitting apparatus according to one embodiment of the present invention.</p>
    <p num="18">
      Referring to FIG. 1, a camera part 1 photographs an image to be transmitted.
      <br/>
      Image data outputted from the camera part 1 is digital image data.
    </p>
    <p num="19">Incidentally, a background image is photographed in advance by the camera part 1 and is previously stored in a background memory 2 as still image data.</p>
    <p num="20">
      The still image data is luminance data represented in an 8-bit range, and use of a value of 0 is inhibited.
      <br/>
      Specifically, a pixel having a value of 0 is set to, for example, "1", and stored in the background memory 2.
    </p>
    <p num="21">The background image photographed in advance is also previously transmitted to a receiving side which will be described later.</p>
    <p num="22">A moving object extracting circuit 3 makes a comparison between the background image stored in the background memory 2 and the photographed image transmitted from the camera part 1, and extracts a portion which shows a change greater than or equal to a predetermined magnitude.</p>
    <p num="23">
      In the present embodiment, matching is performed between the background image stored in the background memory 2 and the background image contained in the photographed image transmitted from the camera part 1, and a moving object (in the present embodiment, a person image) is extracted.
      <br/>
      Thus, the moving object extracting circuit 3 provides an output consisting of only the person image, which is surrounded by values of 0s.
    </p>
    <p num="24">The image data outputted from the moving object extracting circuit 3 is divided into blocks of (8 * 8) pixels by a blocking circuit 4.</p>
    <p num="25">Image data S1 blocked by the blocking circuit 4 is inputted to a difference circuit 5.</p>
    <p num="26">Further, previous frame image data S2 relative to the previous frame is inputted from a frame memory 15 to the difference circuit 5 through a selecting switch 17.</p>
    <p num="27">The difference circuit 5 finds the difference between the input image data S1 and the previous frame image data S2 and generates difference data S3, and outputs the difference data S3 to a DCT (discrete cosine transform) circuit 6.</p>
    <p num="28">The DCT circuit 6 performs discrete cosine transform of the difference data S3 in units of minute blocks by using two-dimensional image correlations, and outputs the resultant transform data S4 to a quantizing circuit 7.</p>
    <p num="29">The quantizing circuit 7 quantizes the transform data S4 in quantizing steps controlled in a manner which will be described later, and outputs quantized data S5 to both a variable-length coding circuit 8 and an inverse quantizing circuit 12.</p>
    <p num="30">The variable-length coding circuit 8 performs variable-length coding of the quantized data S5, and outputs the resultant variable-length coded data S6 to a multiplexer circuit 9.</p>
    <p num="31">The multiplexer circuit 9 performs multiplexing of motion vector data S7 outputted from a motion vector detecting/motion compensating circuit 16, the variable-length coded data S6, and quantizing step control data S8 outputted from a quantizing step controlling circuit 11.</p>
    <p num="32">Multiplexed data S9 outputted from the multiplexer circuit 9 is outputted from a buffer circuit 10 as transmission data S10, and is transmitted over a communication line 20 to the receiving side shown in FIG. 2.</p>
    <p num="33">The buffer circuit 10 also outputs information data S11 indicative of the amount of data accumulated in the buffer circuit 10 to the quantizing step controlling circuit 11.</p>
    <p num="34">The quantizing step controlling circuit 11 controls the quantizing step of each of the quantizing circuit 7 and the inverse quantizing circuit 12 on the basis of the input information data S11.</p>
    <p num="35">Also, the quantizing step controlling circuit 11 outputs to the multiplexer circuit 9 the quantizing step control data S8 which is used to control the quantizing step of each of the quantizing circuit 7 and the inverse quantizing circuit 12.</p>
    <p num="36">The moving image transmitting apparatus has a local decoding circuit part so that the quantized data S5 to be transmitted as the transmission data S10 can be locally decoded and supplied to the frame memory 15.</p>
    <p num="37">The local decoding circuit part will be specifically described below.</p>
    <p num="38">
      The quantized data S5 is inversely quantized by the inverse quantizing circuit 12 in the quantizing steps controlled in the above-described manner.
      <br/>
      The inverse quantizing circuit 12 outputs the inversely quantized data S12 to an inverse DCT circuit 13.
    </p>
    <p num="39">The inverse DCT circuit 13 transforms the inversely quantized data S12 into decoded image data S13 by means of the completely inverse transforming processing of that performed by the DCT circuit 6, and outputs the decoded image data S13 to an adding circuit 14.</p>
    <p num="40">The adding circuit 14 adds together the previous frame image data S2 fed back by the frame memory 15 and the decoded image data S13, restores the image data outputted as the transmission data S10, and sequentially stores the transmission data S10 in the frame memory 15.</p>
    <p num="41">The frame memory 15 includes a frame memory 15A for storing local decoded data relative to the previous frame and a frame memory 15B to which to write current data.</p>
    <p num="42">
      Further, the moving image transmitting apparatus supplies the input image data S1 to the motion vector detecting/motion compensating circuit 16.
      <br/>
      The motion vector detecting/motion compensating circuit 16 reads out the image data relative to the previous frame which is stored in the frame memory 15 as the image data S1, and detects a motion vector by performing a matching computation on the read-out image data and the image data S1 inputted from the blocking circuit 4.
      <br/>
      The motion vector detecting/motion compensating circuit 16 causes the frame memory 15 to output the previous frame image data S2 which is prediction data for use in performing a motion compensation on the basis of the detected motion vector.
    </p>
    <p num="43">Further, the motion vector detecting/motion compensating circuit 16 outputs the detected motion vector data S7 to the multiplexer circuit 9.</p>
    <p num="44">
      Incidentally, if a background image or the first image is to be transmitted, since there is no image data to which reference is to be made, it is necessary to substitute "0" for the value of the previous frame image data S2.
      <br/>
      For this reason, in the present embodiment, the selecting switch 17 is provided so that either one of the image data read out from the frame memory 15 and "0" can be selected.
      <br/>
      The selecting operation of the selecting switch 17 is controlled in accordance with control data S15 outputted from a control circuit (not shown).
    </p>
    <p num="45">The moving image receiving apparatus for receiving the image data coded by and transmitted from the apparatus of FIG. 1 and reproducing an image will be described below with reference to FIG. 2.</p>
    <p num="46">FIG. 2 is a block diagram showing the moving image receiving apparatus according to the present embodiment.</p>
    <p num="47">Referring to FIG. 2, image data S21 transmitted over the communication line 20 is inputted to a demultiplexer circuit 22 through a buffer circuit 21 as reproduction image data S22.</p>
    <p num="48">The demultiplexer circuit 22 forms difference image information data S25 by separating motion vector data S23 and quantizing step control data S24 from the reproduction image data S22, and supplies the difference image information data S25 to a variable-length decoding circuit 23.</p>
    <p num="49">The variable-length decoding circuit 23 decodes the difference image information data S25 into decoded image data S26 which corresponds to the quantized data S5 coded by the variable-length coding circuit 8 (refer to FIG. 1).</p>
    <p num="50">An inverse quantizing circuit 24 inversely quantizes the decoded image data S26 in quantizing steps controlled by a quantizing step controlling circuit 25, thereby forming inversely quantized data S27.</p>
    <p num="51">Incidentally, the quantizing step controlling circuit 25 controls the quantizing step of the inverse quantizing circuit 24 on the basis of the quantizing step control data S24 outputted from the demultiplexer circuit 22.</p>
    <p num="52">The inversely quantized data S27 is transformed into decoded image data S28 by an inverse DCT circuit 26 in accordance with the completely inverse transforming process of that performed by the DCT circuit 6 (refer to FIG. 1), and the decoded image data S28 is outputted to an adding circuit 27.</p>
    <p num="53">The adding circuit 27 adds the decoded image data S28 to motion compensation data S29 read out from a frame memory 28, forms decoded image data S30, and feeds the decoded image data S30 back to the frame memory 28.</p>
    <p num="54">The demultiplexer circuit 22 also supplies to a motion compensating circuit 29 the motion vector data S23 separated from the reproduction image data S22.</p>
    <p num="55">When the motion vector data S23 is inputted to the motion compensating circuit 29, the motion compensating circuit 29 controls the reading operation of the frame memory 28 on the basis of the motion vector data S23, thereby causing the frame memory 28 to output the motion compensation data S29.</p>
    <p num="56">S1milarly to the frame memory 15 (refer to FIG. 1), the frame memory 28 includes a frame memory 28A for storing decoded values relative to the previous frame and a frame memory 28B to which to write current data.</p>
    <p num="57">
      The decoded image data S30 is also inputted to an adding circuit 30.
      <br/>
      A background image previously transmitted from the transmitting side (normally, a background image identical to that stored in the background memory 2 (refer to FIG. 1)) is stored in a background memory 31, and the background image data is outputted to the adding circuit 30.
    </p>
    <p num="58">The adding circuit 30 substitutes the background image data read out from the background memory 31 for the zero valued portion of the received image, and outputs the resultant image to a monitor 32.</p>
    <p num="59">In the above-described manner, a picture is formed in which the person image transmitted as a moving image is superimposed on the still background image, and the picture is displayed on the screen of the monitor 32.</p>
    <p num="60">
      In the above-described embodiment, it is not necessary that the background image stored in the background memory 2 on the transmitting side be identical to the background image stored in the background memory 31 on the receiving side.
      <br/>
      This feature of the above-described embodiment is useful, for example, in a case where an operator does not desire to transmit an actual background image or desires to transmit a person image superimposed on a different particular background image.
    </p>
    <p num="61">
      In such a case, by transmitting filed images or different location images to the receiving side as background images and store these background images in the background memory 31, the operator can select a desired background image from among the plurality of background images.
      <br/>
      If a file of background image information to be used is provided on the receiving side, it is preferable to adopt an arrangement for specifying a code for designating a desired background image before transmission of a moving image.
    </p>
    <p num="62">
      The background memory 2 stores image information on the basis of which a background image is eliminated from a photographed image transmitted from the camera part 1.
      <br/>
      The background within the photographic field of view of the camera part 1 varies subtly or greatly with a camera shake, panning or zooming.
      <br/>
      To cover a predictable range of variations, the background memory 2 is preferably capable of storing image data corresponding to an area wider than the area of one normal picture.
      <br/>
      If an arrangement for making reference to operation information about the camera part 1 (such as zooming, focal length, panning angle and tilting angle) is adopted, the moving object extracting circuit 3 can readily extract the portion of a moving object (in the present embodiment, the person).
    </p>
    <p num="63">
      As is readily understood from the foregoing description, in accordance with the present embodiment, it is possible to greatly reduce the number of codes required for the transmission of a moving image.
      <br/>
      Further, since an arbitrary background can be selected, the present embodiment is very useful when an operator desires to hide an actual background of the transmitting side or to use a particular background.
    </p>
    <p num="64">Incidentally, it is possible to practice the present invention in various other forms without departing from the spirit and primary features thereof.</p>
    <p num="65">For example, although the description of the present embodiment has referred to the coding method using the DCT circuit, the quantizing circuit and the variable-length coding, the present invention is not limited to such a coding method.</p>
    <p num="66">The present invention can also be applied to a case where an image which contains a stationary background image is transmitted or received, such as a videotelephone system.</p>
    <p num="67">In other words, the foregoing description of the embodiment has been given for illustrative purposes only and should not be construed as imposing any limitation in every respect.</p>
    <p num="68">The scope of the invention is, therefore, to be determined solely by the following claims and not limited by the text of the specification, and alterations made within a scope equivalent to the scope of the claims fall within the true spirit and scope of the invention.</p>
  </description>
  <claims format="original" lang="en" id="claim_en">
    <claim num="1">
      <claim-text>What is claimed is:</claim-text>
      <claim-text>1.</claim-text>
      <claim-text>A coding apparatus comprising:</claim-text>
      <claim-text>a) input means for inputting independent background image data corresponding to a predetermined background image and image data obtained by picking up an object with the predetermined background image; b) extracting means for extracting object image data corresponding to the object portion from the image data by using independent background image data;</claim-text>
      <claim-text>and c) coding means for motion compensation interpicture coding the object image data extracted by said extracting means, wherein the motion compensation interpicture coding performs motion compensation between the object image data of a present picture and the object image data of a picture other than the present picture.</claim-text>
    </claim>
    <claim num="2">
      <claim-text>2. A coding apparatus according to claim 1, further comprising image pickup means for inputting an image.</claim-text>
    </claim>
    <claim num="3">
      <claim-text>3. A coding apparatus according to claim 1, wherein said coding means includes: a) orthogonal transformation means for orthogonally transforming the input image data processed by said extracting means; b) quantizing means for quantizing the orthogonally transformed image data;</claim-text>
      <claim-text>and c) variable-length coding means for performing variable-length coding of the quantized image data.</claim-text>
    </claim>
    <claim num="4">
      <claim-text>4. A coding apparatus according to claim 3, wherein said coding means further includes a buffer memory for storing the image data coded by the variable-length coding, a quantizing step of said quantizing means being controlled in accordance with an amount of data accumulated in the buffer memory.</claim-text>
    </claim>
    <claim num="5">
      <claim-text>5. A coding apparatus according to claim 1, wherein said coding means performs intrapicture coding of the object image data.</claim-text>
    </claim>
    <claim num="6">
      <claim-text>6. A coding apparatus according to claim 1, wherein said coding means performs intrapicture coding of independent background image data.</claim-text>
    </claim>
    <claim num="7">
      <claim-text>7. A coding apparatus according to claim 1, wherein said coding means codes independent background image data.</claim-text>
    </claim>
    <claim num="8">
      <claim-text>8. A coding apparatus according to claim 7, further comprising output means for outputting the object image data and independent background image data coded by said coding means.</claim-text>
    </claim>
    <claim num="9">
      <claim-text>9. A decoding apparatus comprising: a) input means for inputting coded image data, which have been obtained by extracting object image data from image data and coding the extracted object image data by motion compensation interpicture coding, which performs motion compensation between the object image data of a present picture, and the object image data of a picture other than the present picture, the image data being data obtained by picking up an object with a predetermined background image, and the object image data being extracted from the image data by using an independent background image data corresponding to the predetermined background image obtained prior to the extraction process;</claim-text>
      <claim-text>and b) decoding means for decoding the image data inputted by said input means.</claim-text>
    </claim>
    <claim num="10">
      <claim-text>10. A decoding apparatus according to claim 9, further comprising: a) memory means for storing the background image data;</claim-text>
      <claim-text>and b) combining means for combining the image data decoded by said decoding means with the background image data stored in said memory means.</claim-text>
    </claim>
    <claim num="11">
      <claim-text>11. A decoding apparatus according to claim 9, further comprising: combining means for combining the image data decoded by said decoding means with desired image data.</claim-text>
    </claim>
    <claim num="12">
      <claim-text>12. A method for coding comprising the steps of: a) inputting independent background image data corresponding to a predetermined background image and image data obtained by picking up an object with the predetermined background image; b) extracting object image data corresponding to the objection portion from the image data by using independent background image data;</claim-text>
      <claim-text>and c) motion compensation interpicture coding the object image data extracted by said extracting step, wherein the motion compensation interpicture coding performs motion compensation between the object image data of a present picture and the object image data of a picture other than the present picture.</claim-text>
    </claim>
    <claim num="13">
      <claim-text>13. A method for decoding comprising the steps of: a) inputting coded image data, which have been obtained by extracting object image data from image data and coding the extracted object image data by motion compensation interpicture coding, which performs motion compensation between the object image data of a present picture, and the object image data of a picture other than the present picture, the image data being data obtained by picking up an object with a predetermined background image, and the object image data being extracted from the image data by using an independent background image data corresponding to the predetermined background image obtained prior to the extraction process;</claim-text>
      <claim-text>and b) decoding the image data inputted in said input step.</claim-text>
    </claim>
  </claims>
</questel-patent-document>