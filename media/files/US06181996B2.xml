<?xml version="1.0" encoding="UTF-8"?>
<questel-patent-document lang="en" date-produced="20180805" produced-by="Questel" schema-version="3.23" file="US06181996B2.xml">
  <bibliographic-data lang="en">
    <publication-reference publ-desc="Granted patent as second publication">
      <document-id>
        <country>US</country>
        <doc-number>06181996</doc-number>
        <kind>B2</kind>
        <date>20010130</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>US6181996</doc-number>
      </document-id>
    </publication-reference>
    <original-publication-kind>B2</original-publication-kind>
    <application-reference is-representative="YES" family-id="23756843" extended-family-id="42108727">
      <document-id>
        <country>US</country>
        <doc-number>09442454</doc-number>
        <kind>A</kind>
        <date>19991118</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>1999US-09442454</doc-number>
      </document-id>
      <document-id data-format="questel_Uid">
        <doc-number>43165269</doc-number>
      </document-id>
    </application-reference>
    <language-of-filing>en</language-of-filing>
    <language-of-publication>en</language-of-publication>
    <priority-claims>
      <priority-claim kind="national" sequence="1">
        <country>US</country>
        <doc-number>44245499</doc-number>
        <kind>A</kind>
        <date>19991118</date>
        <priority-active-indicator>Y</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="1">
        <doc-number>1999US-09442454</doc-number>
      </priority-claim>
    </priority-claims>
    <dates-of-public-availability>
      <publication-of-grant-date>
        <date>20010130</date>
      </publication-of-grant-date>
    </dates-of-public-availability>
    <classifications-ipcr>
      <classification-ipcr sequence="1">
        <text>G01C  21/36        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>01</class>
        <subclass>C</subclass>
        <main-group>21</main-group>
        <subgroup>36</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
    </classifications-ipcr>
    <classification-national>
      <country>US</country>
      <main-classification>
        <text>701036000</text>
        <class>701</class>
        <subclass>036000</subclass>
      </main-classification>
      <further-classification sequence="1">
        <text>340439000</text>
        <class>340</class>
        <subclass>439000</subclass>
      </further-classification>
      <further-classification sequence="2">
        <text>340990000</text>
        <class>340</class>
        <subclass>990000</subclass>
      </further-classification>
      <further-classification sequence="3">
        <text>345158000</text>
        <class>345</class>
        <subclass>158000</subclass>
      </further-classification>
      <further-classification sequence="4">
        <text>345166000</text>
        <class>345</class>
        <subclass>166000</subclass>
      </further-classification>
      <further-classification sequence="5">
        <text>345175000</text>
        <class>345</class>
        <subclass>175000</subclass>
      </further-classification>
      <further-classification sequence="6">
        <text>701532000</text>
        <class>701</class>
        <subclass>532000</subclass>
      </further-classification>
    </classification-national>
    <classifications-ecla>
      <classification-ecla sequence="1">
        <text>B60K-037/00</text>
        <section>B</section>
        <class>60</class>
        <subclass>K</subclass>
        <main-group>37</main-group>
        <subgroup>00</subgroup>
      </classification-ecla>
      <classification-ecla sequence="2">
        <text>B60K-037/06</text>
        <section>B</section>
        <class>60</class>
        <subclass>K</subclass>
        <main-group>37</main-group>
        <subgroup>06</subgroup>
      </classification-ecla>
      <classification-ecla sequence="3">
        <text>G01C-021/36</text>
        <section>G</section>
        <class>01</class>
        <subclass>C</subclass>
        <main-group>21</main-group>
        <subgroup>36</subgroup>
      </classification-ecla>
    </classifications-ecla>
    <patent-classifications>
      <patent-classification sequence="1">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>B60K-037/00</classification-symbol>
        <section>B</section>
        <class>60</class>
        <subclass>K</subclass>
        <main-group>37</main-group>
        <subgroup>00</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="2">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>B60K-037/06</classification-symbol>
        <section>B</section>
        <class>60</class>
        <subclass>K</subclass>
        <main-group>37</main-group>
        <subgroup>06</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="3">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>B60K-2350/1012</classification-symbol>
        <section>B</section>
        <class>60</class>
        <subclass>K</subclass>
        <main-group>2350</main-group>
        <subgroup>1012</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="4">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>B60K-2350/901</classification-symbol>
        <section>B</section>
        <class>60</class>
        <subclass>K</subclass>
        <main-group>2350</main-group>
        <subgroup>901</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="5">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>B60K-2350/903</classification-symbol>
        <section>B</section>
        <class>60</class>
        <subclass>K</subclass>
        <main-group>2350</main-group>
        <subgroup>903</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="6">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>G01C-021/36</classification-symbol>
        <section>G</section>
        <class>01</class>
        <subclass>C</subclass>
        <main-group>21</main-group>
        <subgroup>36</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="7">
        <classification-scheme office="EP" scheme="ICO"/>
        <classification-symbol>L60K-350/10J1</classification-symbol>
      </patent-classification>
      <patent-classification sequence="8">
        <classification-scheme office="EP" scheme="ICO"/>
        <classification-symbol>L60K-350/90A</classification-symbol>
      </patent-classification>
      <patent-classification sequence="9">
        <classification-scheme office="EP" scheme="ICO"/>
        <classification-symbol>L60K-350/90B</classification-symbol>
      </patent-classification>
    </patent-classifications>
    <number-of-claims>37</number-of-claims>
    <exemplary-claim>1</exemplary-claim>
    <figures>
      <number-of-drawing-sheets>4</number-of-drawing-sheets>
      <number-of-figures>5</number-of-figures>
      <image-key data-format="questel">US6181996</image-key>
    </figures>
    <invention-title format="original" lang="en" id="title_en">System for controlling vehicle information user interfaces</invention-title>
    <references-cited>
      <citation srep-phase="examiner">
        <patcit num="1">
          <text>FISCHER MATTHIAS, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5748748</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5748748</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="2">
          <text>WIEMER KARL-HEINZ, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5781437</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5781437</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="3">
          <text>HIRANO KAZUO</text>
          <document-id>
            <country>US</country>
            <doc-number>5839086</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5839086</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="4">
          <text>WHITE CRAIG W</text>
          <document-id>
            <country>US</country>
            <doc-number>5871232</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5871232</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="5">
          <text>ARAKAWA TAKEHARU, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5938719</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5938719</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="6">
          <text>TAKINAMI TAKASHI</text>
          <document-id>
            <country>US</country>
            <doc-number>6016110</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US6016110</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="7">
          <text>SEITA KAZUHISA</text>
          <document-id>
            <country>US</country>
            <doc-number>6023290</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US6023290</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="8">
          <text>ZUBER GARY, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>6029110</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US6029110</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="9">
          <text>KIRKHART MICHAEL O</text>
          <document-id>
            <country>US</country>
            <doc-number>6059843</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US6059843</doc-number>
          </document-id>
        </patcit>
      </citation>
    </references-cited>
    <parties>
      <applicants>
        <applicant data-format="original" app-type="applicant" sequence="1">
          <addressbook lang="en">
            <orgname>International Business Machines Corporation</orgname>
            <address>
              <address-1>Armonk, NY, US</address-1>
              <city>Armonk</city>
              <state>NY</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </applicant>
        <applicant data-format="questel" app-type="applicant" sequence="2">
          <addressbook lang="en">
            <orgname>IBM</orgname>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </applicant>
      </applicants>
      <inventors>
        <inventor data-format="original" sequence="1">
          <addressbook lang="en">
            <name>Chou, Paul Bao-Luo</name>
            <address>
              <address-1>Montvale, NJ, US</address-1>
              <city>Montvale</city>
              <state>NJ</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
        <inventor data-format="original" sequence="2">
          <addressbook lang="en">
            <name>Lai, Jennifer</name>
            <address>
              <address-1>Garrison, NY, US</address-1>
              <city>Garrison</city>
              <state>NY</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
        <inventor data-format="original" sequence="3">
          <addressbook lang="en">
            <name>Levas, Anthony</name>
            <address>
              <address-1>Yorktown Heights, NY, US</address-1>
              <city>Yorktown Heights</city>
              <state>NY</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
        <inventor data-format="original" sequence="4">
          <addressbook lang="en">
            <name>Moskowitz, Paul Andrew</name>
            <address>
              <address-1>Yorktown Heights, NY, US</address-1>
              <city>Yorktown Heights</city>
              <state>NY</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
      </inventors>
      <agents>
        <agent sequence="1" rep-type="agent">
          <addressbook lang="en">
            <orgname>McGinn &amp; Gibb, PLLC</orgname>
          </addressbook>
        </agent>
        <agent sequence="2" rep-type="agent">
          <addressbook lang="en">
            <name>Kaufman, Esq, Stephen C.</name>
          </addressbook>
        </agent>
      </agents>
    </parties>
    <examiners>
      <primary-examiner>
        <name>Cuchlinski, Jr., William A.</name>
      </primary-examiner>
    </examiners>
    <lgst-data>
      <lgst-status>GRANTED</lgst-status>
    </lgst-data>
  </bibliographic-data>
  <abstract format="original" lang="en" id="abstr_en">
    <p id="P-EN-00001" num="00001">
      <br/>
      A system for controlling a vehicle information system user interface with at least one sensor for sensing the presence of a passenger and a controller for controlling a vehicle information system user interface based upon an output signal from the sensor.
    </p>
  </abstract>
  <description format="original" lang="en" id="desc_en">
    <heading>BACKGROUND OF THE INVENTION</heading>
    <p num="1">
      1.
      <br/>
      Field of the Invention
    </p>
    <p num="2">The present invention generally relates to a vehicle, and more particularly to a vehicle including a system for detecting the presence of animate objects (e.g., passengers, pets, etc.) and for controlling the information system user interface in an appropriate manner.</p>
    <p num="3">2. Description of the Related Art</p>
    <p num="4">
      Computer technology for providing information and application functions to automotive vehicles is becoming pervasive.
      <br/>
      For example, vehicles are being outfitted with computers that contain display devices, speech synthesis, text-to-speech (TTS) interfaces, and a multitude of input devices such as speech recognition, remote control devices, keyboards, track balls, joysticks, touch-screens, etc.
    </p>
    <p num="5">
      These computerized devices are useful for controlling a vehicle or for the entertainment of the vehicle occupants (e.g., see U.S. patent application Ser.
      <br/>
      No. 09/311,277 entitled "Information System for Mobile Users", filed on May 14, 1999, and U.S. patent application Ser.
      <br/>
      No. 09/357,840, entitled "System and Method for Network Vehicle Diagnostics and Health Monitoring" filed on Jul. 20, 1999, both incorporated herein by reference).
    </p>
    <p num="6">While the above described equipment is widespread, there has been little recognition of the cognitive burden placed on a driver by these devices, particularly those which do not contribute to a driver's primary responsibility of controlling a vehicle in a safe and responsive manner.</p>
    <p num="7">
      A driver's senses may be fully occupied as a result of the large cognitive load demanded for driving a vehicle safely.
      <br/>
      To operate a vehicle safely, drivers use their hands for steering and manipulating other vehicle user interfaces such as the gear shift, turn signals, windshield wipers, heating mechanism, and parking brake.
      <br/>
      The driver also must focus attention on the road, on the traffic, and on vehicle operation devices such as rear-view mirrors, speedometer, gas gauge, and tachometer.
    </p>
    <p num="8">
      Distracting or complex output presented to the driver or requirements for complicated user input may result in accidents.
      <br/>
      These must be filtered or controlled while the driver is operating the vehicle.
      <br/>
      As a result, in-vehicle devices providing information unrelated to driving a vehicle should be easy to operate and not distracting.
    </p>
    <p num="9">
      Conversely, the same demands for attention are not placed on passengers.
      <br/>
      Therefore, a device which would be too complicated and distracting for a driver may not be rich enough information-wise for passengers who are able to focus more of their attention on a device not used to control the vehicle.
      <br/>
      Passengers are free to use computational apparatus and information delivery systems.
    </p>
    <p num="10">With the conventional systems, a problem has been the lack of tailoring of vehicle information system functions, input and output devices, and the user interface based upon composition of the passenger group.</p>
    <p num="11">
      The problem of identifying the presence of passengers in a motor vehicle and the importance of controlling vehicle safety systems has been recognized previously.
      <br/>
      Identifying passengers in a vehicle is accomplished using a variety of means.
      <br/>
      For example, force sensors, ultrasonic detectors, capacitance detectors, optical detectors, and sound detectors have been used.
      <br/>
      Passengers are detected in order to enable or advise the use of safety devices such as seat belts or air bags.
    </p>
    <p num="12">
      Further, the problem of adapting user interfaces has also been recognized previously.
      <br/>
      For instance, an example is found in U.S. Pat. No. 5,648,755 entitled "Display System" and incorporated herein by reference, which teaches the tailoring of the visual display for operating instruments as a function of the driving conditions of the vehicle.
    </p>
    <p num="13">
      However, none of the conventional systems teaches modifying an information system user interface based upon the presence or absence of passengers.
      <br/>
      Neither does any system even address the presence of detecting non-human passengers (e.g., pets, animal cargo, etc.) in a vehicle and the effect of in-vehicle devices on such non-human passengers.
    </p>
    <heading>SUMMARY OF THE INVENTION</heading>
    <p num="14">In view of the foregoing and other problems, disadvantages, and drawbacks of the conventional methods and structures, the present invention has been devised.</p>
    <p num="15">An object of the present invention is to provide a structure for detecting the presence of passengers and for selectively controlling the information system user interface appropriately.</p>
    <p num="16">It is another object of the invention to detect when a non-driving passenger is using a device (e.g., providing information or entertainment) and selectively enabling the device function to be complex and more detailed for the passenger.</p>
    <p num="17">Additionally, it is an object of the invention to control a device used by a passenger in a manner conducive to driving safety.</p>
    <p num="18">In a first aspect of the invention, a system for controlling a vehicle information system user interface includes at least one sensor for sensing the presence of a non-driving passenger in a vehicle, and a controller for controlling the vehicle information system user interface based upon an output signal from the at least one sensor.</p>
    <p num="19">
      Thus, with the invention, a vehicle's information system user interface may be selectively changed depending upon whether passengers are detected in the vehicle.
      <br/>
      Further, certain distinguishing characteristics of the passengers are detected enabling additional selective change of the vehicle user interface.
    </p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading>
    <p num="20">
      The foregoing and other objects, aspects and advantages will be better understood from the following detailed description of a preferred embodiment of the invention with reference to the drawings, in which:
      <br/>
      FIG. 1 is a schematic diagram of a vehicle including a seated person according to a preferred embodiment of the present invention;
      <br/>
      FIG. 2 is a schematic diagram of a vehicle including a seated pet and a device for detecting the presence of passengers according to a preferred embodiment of a system of the present invention;
      <br/>
      FIG. 3 is a system diagram of components according to the present invention.
      <br/>
      FIG. 4 depicts audio and video outputs in a vehicle with only a driver according to the present invention; and
      <br/>
      FIG. 5 depicts audio and video outputs in a vehicle with a driver and passengers according to the present invention.
    </p>
    <heading>DETAILED DESCRIPTION OF PREFERRED EMBODIMENTS OF THE INVENTION</heading>
    <p num="21">Referring now to the drawings, and more particularly to FIGS. 1-5, there is shown a preferred embodiment of the present invention.</p>
    <p num="22">Referring to FIG. 1, a vehicle system 100 is shown for use with a vehicle 110 and includes a passenger 200 (e.g., a person) seated within a vehicle 110 having at least one sensor (e.g., sensor circuit) and more preferably a plurality of sensors located at different positions in the vehicle 120, 130, an electronic control unit (ECU) 140 (and more preferably a plurality of ECUs) for controlling sensor circuits, a vehicle communication bus 105 for carrying data and control signals to the ECU 140, an ECU 160 interfaced with the bus 105, and at least one interface device (and more preferably a plurality of interface devices) including, for example, any one or more of a visual display 170, an audio system 180.</p>
    <p num="23">
      Further, additional devices 190 may be provided.
      <br/>
      Such additional devices may include a visual display device, an audio system, a keyboard, a telecommunications device, a fax machine, a touch screen, a speech synthesis system, a voice recognition interface, a wireless remote control device, a scent dispenser, a heating device, and a cooling device.
    </p>
    <p num="24">Further, a manually-controlled input device 135 may be provided for inputting parameters to the ECU devices including the indication of the presence of one or more passengers and for overriding automatic selections if necessary.</p>
    <p num="25">FIG. 2 shows the same vehicle system 100 for use in a vehicle 110, including a passenger 205 (e.g., in this case a pet or animal cargo), and the other preferred features of the present invention as above described in FIG. 1.</p>
    <p num="26">Referring to FIG. 3, a block diagram of the system according to the preferred embodiment is shown and described below.</p>
    <p num="27">
      The system 100 includes a sensor 120 and a sensor 130 for detecting a passenger (e.g., 200, 205 shown in FIGS. 1-2) located inside of the vehicle 110.
      <br/>
      The sensors can be formed of known and commercially available components.
      <br/>
      Many types of sensors may be employed including any one or more of infrared sensors, capacitive sensors, force/pressure sensors, etc., and are well-known in the art.
      <br/>
      Such sensor components and configurations are described in, for example, U.S. Pat. No. 4,300,116, U.S. Pat. No. 5,402,075, U.S. Pat. No. 5,313,189, U.S. Pat. No. 5,629,488, U.S. Pat. No. 5,871,063, U.S. Pat. No. 5,297,430, U.S. Pat. No. 5,305,644, U.S. Pat. No. 5,857,536, U.S. Pat. No. 5,654,615, U.S. Pat. No. Reissue 034,773, and U.S. Pat. No. 5,602,526, each incorporated herein by reference. U.S. Pat. No. 5,404,128 incorporated herein by reference describes the detection of a passenger based upon the life activity of the human body including heart beat.
      <br/>
      Such a system using the distinguishing characteristics of heart beat (e.g. rate, shape, etc.) may be used to distinguish humans from non-humans.
    </p>
    <p num="28">
      The presence of a passenger causes a change in the electrical characteristics of the sensor 120.
      <br/>
      An additional passenger may change the electrical characteristics of the sensor 130.
      <br/>
      Additional sensors may be used to detect additional passengers.
    </p>
    <p num="29">The sensor's electrical characteristics are monitored by ECU 140, which responds to a change by reporting a detection code over the car bus, such as a Society of Automotive Engineers (SAE) J1850 communication bus, or a Controller Area Network (CAN).</p>
    <p num="30">
      The car bus 105 in turn provides the code to a second ECU 160 which controls an interface 170, 180.
      <br/>
      The ECUs 160, 161, and additional ECUs 162, as necessary, may be provided as components within an embedded electronic or computing system 150 within the vehicle.
      <br/>
      The structure of the user interface is believed to be well-known to one of ordinary skill in the art taking the present invention as a whole.
      <br/>
      For example, the structure of some exemplary user interfaces are described in U.S. Pat. No. 5,648,755, U.S. Pat. No. 5,847,704, U.S. Pat. No. 5,289,184, U.S. Pat. No. 5,635,924, U.S. Pat. No. 5,821,935, U.S. Pat. No. 5,555,172, and U.S. Pat. No. 5,875,108, each incorporated herein by reference.
    </p>
    <p num="31">The interface may control visual and/or audio systems 170, 180, and additional user interface systems 190.</p>
    <p num="32">
      Also, for example, the modality of input or output may be varied.
      <br/>
      If no passenger is detected in the vehicle, voice recognition may be the only input modality used (e.g., enabled by the inventive system).
      <br/>
      Video display may be turned off and audio may be provided as the only output modality.
      <br/>
      Navigation instructions may be provided to the driver by the use of translation of computer output to audio using speech synthesis (text-to-speech or TTS) or the playing of digital recordings of human speech (one such format is the .wav file).
    </p>
    <p num="33">If a passenger is detected as being present in the vehicle (e.g., as sensed by the sensor circuits), the passenger is enabled to use any graphical user interface (and associated application program(s)) such as touch screens, mouse, keyboard, electronic remote control, joystick, track point, and so forth.</p>
    <p num="34">Additionally, the mode or modality for presenting output information may be varied.</p>
    <p num="35">For example, if there is a single display screen for displaying information (e.g., the text of incoming e-mail, updated traffic conditions, driving navigation directions, etc.), then the display could be turned off, e.g., automatically by the embedded electronic or computing system 150, or simply not enabled for a solo driver and information provided only through the auditory modality (e.g., text-to-speech, recorded sound, etc.).</p>
    <p num="36">
      Various input or output devices may be enabled (or disabled) by the detection by the sensor circuit(s) of a passenger in a seat.
      <br/>
      Visual displays such as a TV monitor or a computer display screen may be enabled or have enriched display content for the use of the passengers.
      <br/>
      For example, microphones, view screens, user input devices such as keyboards, trackballs, joysticks, mice, etc. could be enabled for used by passengers detected by the sensor circuit(s) 120, 130.
    </p>
    <p num="37">The physical orientation of input and output devices also may be changed.</p>
    <p num="38">
      For example, display monitors may rotate to face passengers (rather than a driver) or an empty seat.
      <br/>
      Another option is to employ shades (e.g., louvers) or some obscuring technique (e.g., blinds and the like) which are automatically activated upon starting the vehicle and detecting that no passengers are present.
      <br/>
      The louvers allow the passenger(s) to view enriched video content but obscure the driver's view.
      <br/>
      Along these lines, such devices could be blocked from activation unless a signal is received by the sensor circuits indicating the presence of a passenger.
    </p>
    <p num="39">
      Moreover, the acoustic characteristics of the vehicle may be changed with the detected presence of passengers.
      <br/>
      Audio inputs and outputs may be adjusted.
      <br/>
      The speaker balance of sound systems may be shifted.
      <br/>
      Audio may be enriched with more details and voice recognition may have a richer voice command set when no passenger is detected.
      <br/>
      For example, a speech recognition model could be modified and frequency distribution of audio output tuned.
      <br/>
      A further example is if pets (e.g., dogs, cats etc. as shown in FIG. 2) are detected and audio output levels adjusted so as to not frighten the animal passengers.
      <br/>
      For example, it is well-known that cats are afraid of loud noises and that dogs are startled by high frequency noises.
    </p>
    <p num="40">
      Application selection could be controlled to prevent distractions to a driver.
      <br/>
      For example, video games may be blocked from being played if no passenger is detected.
      <br/>
      However, audio input and output could be selectively enabled.
      <br/>
      Applications that could be subject to vehicle system control may include navigational aids, e-mail systems, entertainment devices, mobile communication devices, news devices, and selected transactions requiring attention (e.g., banking and securities trading).
    </p>
    <p num="41">
      The applications may be controlled according to their complexity.
      <br/>
      This would allow the passenger to engage in activities that may require a concentration level that would be dangerous for the driver.
      <br/>
      For example, when a passenger is detected in the seat next to the driver, a visual display may be enabled for use by a passenger.
      <br/>
      Such "complexity" of applications may be determined in advance.
      <br/>
      Only large abstract icons would be shown to the driver alone instead of detailed text.
      <br/>
      Programmable buttons would be made large.
      <br/>
      In this way the display is simplified and usable by the driver at a glance or a touch of the display screen.
      <br/>
      If a passenger is detected, then the interaction may be made more detailed.
    </p>
    <p num="42">
      Additionally, the function of application requests made by a lone driver may be selectively tailored by the system so that the output is appropriately constructed to their attention level.
      <br/>
      For example, summarized information may be returned, length of responses could be shortened, and/or visual representations enlarged.
    </p>
    <p num="43">
      For example, FIG. 4 illustrates a simplified video display and audio for a driver without a passenger.
      <br/>
      In the example of FIG. 4, the video display is simplified, but the audio display is more detailed.
      <br/>
      In contrast, FIG. 5 illustrates a video display and audio for a vehicle with a passenger sensed.
      <br/>
      In FIG. 5, the video display is more complex, but the audio is simplified.
      <br/>
      Manual control may be used to adjust gradations of video and audio richness.
    </p>
    <p num="44">
      Additionally, some indicia indicating the age or maturity of a passenger could be sensed and used to enable or disable functions.
      <br/>
      For example, the weight (and/or height) of a driver or passenger could be detected and used to enable or disable functions.
    </p>
    <p num="45">
      For example, adult entertainment could be disabled when children (e.g., generally and relatively light-weight people) are identified as being present.
      <br/>
      Alternatively, a height of the passenger could be gauged and those having less than a predetermined height (and thus deemed to be children who are generally and relatively shorter than adults) would be barred from such adult entertainment.
    </p>
    <p num="46">
      Thus, with the unique and un-obvious structure of the present invention, a passenger may be detected and based on such detection, the information system user interface may be selectively controlled appropriately.
      <br/>
      Thus, adaptive user interfaces may be provided based on such detection of a passenger to make vehicle operation safer.
      <br/>
      Further, when a non-driving passenger is using a device (e.g., providing information or entertainment), the device function may be selectively made more complex and more detailed for the passenger.
    </p>
    <p num="47">Thus, with the invention, a vehicle's information system user interface may be selectively changed depending upon whether passengers are detected in the vehicle, thereby to increase utility for the passenger detected, and in the absence of a detected passenger, increase vehicle driving safety.</p>
    <p num="48">While the invention has been described in terms of several preferred embodiments, those skilled in the art will recognize that the invention can be practiced with modification within the spirit and scope of the appended claims.</p>
  </description>
  <claims format="original" lang="en" id="claim_en">
    <claim num="1">
      <claim-text>What is claimed is:</claim-text>
      <claim-text>1.</claim-text>
      <claim-text>A system for controlling a vehicle information system user interface comprising:</claim-text>
      <claim-text>at least one sensor for sensing a presence of a non-driving passenger in a vehicle;</claim-text>
      <claim-text>and a controller for controlling the vehicle information system user interface to selectively provide an appropriate level of interaction therewith for a driver, based upon an output signal from said at least one sensor.</claim-text>
    </claim>
    <claim num="2">
      <claim-text>2. The system according to claim 1, wherein a plurality of ones of said at least one sensor are provided at predetermined positions in said vehicle.</claim-text>
    </claim>
    <claim num="3">
      <claim-text>3. The system according to claim 1, wherein said controller comprises an electronic control unit (ECU) for processing said output signal from said at least one sensor.</claim-text>
    </claim>
    <claim num="4">
      <claim-text>4. The system according to claim 3, wherein said controller further comprises a vehicle communication bus for carrying data to said electronic control unit.</claim-text>
    </claim>
    <claim num="5">
      <claim-text>5. The system according to claim 4, wherein said controller further comprises a plurality of electronic control circuits connected to said vehicle communication bus.</claim-text>
    </claim>
    <claim num="6">
      <claim-text>6. The system according to claim 5, wherein said plurality of electronic control circuits connected to said vehicle communication bus are selectively interfaced with a plurality of devices.</claim-text>
    </claim>
    <claim num="7">
      <claim-text>7. The system according to claim 1, wherein said controller comprises means for controlling adjustable control parameters.</claim-text>
    </claim>
    <claim num="8">
      <claim-text>8. The system according to claim 1, wherein said at least one sensor for sensing the presence of a passenger senses a weight of an occupant in a vehicle seat for a non-driving passenger.</claim-text>
    </claim>
    <claim num="9">
      <claim-text>9. The system according to claim 1, wherein said at least one sensor senses a weight of a passenger and outputs a value representing said weight to said controller.</claim-text>
    </claim>
    <claim num="10">
      <claim-text>10. The system according to claim 1, further comprising: a car bus electrically connected to said sensor, wherein said bus comprises one of a Society of Automotive Engineers (SAE) J1850 bus and a Controller Area Network bus.</claim-text>
    </claim>
    <claim num="11">
      <claim-text>11. The system according to claim 6, further comprising: a car bus electrically connected to said sensor, wherein said bus comprises one of a Society of Automotive Engineers (SAE) J1850 bus and a Controller Area Network bus.</claim-text>
    </claim>
    <claim num="12">
      <claim-text>12. The system according to claim 6, wherein said plurality of devices comprise at least one of a visual display device, an audio system, a keyboard, a telecommunications device, a fax machine, a touch screen, a speech synthesis system, a voice recognition interface, a wireless remote control device, a scent dispenser, a heating device, and a cooling device.</claim-text>
    </claim>
    <claim num="13">
      <claim-text>13. A system for detection of a vehicle occupant and control of a user interface comprising: at least one sensor for sensing a presence of a vehicle occupant;</claim-text>
      <claim-text>and a controller for selecting one of a plurality of modes of use for a vehicle system user interface to selectively provide an appropriate level of interaction therewith for a driver, based on a composition of said vehicle occupant.</claim-text>
    </claim>
    <claim num="14">
      <claim-text>14. The system according to claim 13, wherein a plurality of ones of said at least one sensor are provided at predetermined positions in said vehicle.</claim-text>
    </claim>
    <claim num="15">
      <claim-text>15. The system according to claim 13, wherein said plurality of modes comprises: a mode for a vehicle with a driver and no passengers.</claim-text>
    </claim>
    <claim num="16">
      <claim-text>16. The system according to claim 13, wherein said plurality of modes of use comprises: a plurality of modes of use for application to a vehicle with a driver and a plurality of non-driving passengers having a plurality of characteristics.</claim-text>
    </claim>
    <claim num="17">
      <claim-text>17. The system according to claim 13, wherein said plurality of modes comprises: a plurality of modes of audio presentation comprising at least one of enriched speech, simplified speech, reduced sound intensity, frequency-filtered sound, audio balance shifted to driver, and audio balance shifted to passenger.</claim-text>
    </claim>
    <claim num="18">
      <claim-text>18. The system according to claim 13, wherein said plurality of modes comprises: a plurality of modes of video presentation comprising at least one of enriched video, simplified video, video display disabled, and video display enabled.</claim-text>
    </claim>
    <claim num="19">
      <claim-text>19. The system according to claim 13, wherein said plurality of modes comprises: a plurality of modes of mechanical presentation comprising at least one of video display directed to driver, and video display directed to passenger.</claim-text>
    </claim>
    <claim num="20">
      <claim-text>20. The system according to claim 13, wherein said controller comprises means for controlling adjustable control parameters.</claim-text>
    </claim>
    <claim num="21">
      <claim-text>21. The system according to claim 20, wherein said parameters are programmable by a user.</claim-text>
    </claim>
    <claim num="22">
      <claim-text>22. The system according to claim 13, wherein said plurality of modes of use for a vehicle are selected manually by a user.</claim-text>
    </claim>
    <claim num="23">
      <claim-text>23. The system according to claim 16, wherein said plurality of characteristics includes said non-driving passenger having a predetermined measure of weight.</claim-text>
    </claim>
    <claim num="24">
      <claim-text>24. The system according to claim 23, wherein a second characteristic of said plurality of characteristics includes said non-driving passenger having a weight less than said predetermined measure of weight.</claim-text>
    </claim>
    <claim num="25">
      <claim-text>25. A system for detection of a vehicle occupant and control of a user interface comprising: at least one sensor for sensing a presence of a vehicle occupant;</claim-text>
      <claim-text>and a controller for selecting one of a plurality of modes of use for a vehicle system user interface based on a composition of the group of vehicle occupants, wherein said plurality of modes comprises a plurality of modes for a vehicle with a driver and a plurality of non-driving passengers having a plurality of characteristics, and wherein said plurality of characteristics includes a heartbeat of a non-driving passenger having a predetermined heartbeat rate.</claim-text>
    </claim>
    <claim num="26">
      <claim-text>26. A system for detection of a vehicle occupant and control of a user interface comprising: at least one sensor for sensing a presence of a vehicle occupant;</claim-text>
      <claim-text>and a controller for selecting one of a plurality of modes of use for a vehicle system user interface based on a composition of the group of vehicle occupants, wherein said plurality of modes comprises a plurality of modes for a vehicle with a driver and a plurality of non-driving passengers having a plurality of characteristics, and wherein said plurality of characteristics includes a heartbeat pattern of a non-driving passenger having a predetermined shape.</claim-text>
    </claim>
    <claim num="27">
      <claim-text>27. The system according to claim 13, wherein said sensor comprises a manually-controlled sensor.</claim-text>
    </claim>
    <claim num="28">
      <claim-text>28. The system according to claim 13, wherein said sensor comprises a user-programmable sensor.</claim-text>
    </claim>
    <claim num="29">
      <claim-text>29. A system for controlling a vehicle information system user interface comprising: at least one sensor for sensing a presence of a non-human passenger, said non-human passenger being an animal; a controller for controlling a vehicle information system user interface based upon an output signal from said sensor;</claim-text>
      <claim-text>and a scent dispenser coupled to said at least one sensor, wherein when said non-human presence is sensed, said scent dispenser is activated.</claim-text>
    </claim>
    <claim num="30">
      <claim-text>30. A system, for controlling a vehicle information system user interface comprising: at least one sensor for sensing a presence of a non-human passenger;</claim-text>
      <claim-text>and a controller for controlling a vehicle information system user interface based upon an output signal from said sensor, wherein said presence of a non-human passenger is sensed by at least one of sensing a predetermined heartbeat rate and a predetermined heartbeat shape.</claim-text>
    </claim>
    <claim num="31">
      <claim-text>31. A system for controlling a vehicle information system user interface comprising: at least one manually-controlled sensor, manually controllable by said user, for sensing a presence of a non-human passenger, said non-human passenger being an animal; a controller for controlling a vehicle information system user interface based upon an output signal from said at least one manually-controlled sensor;</claim-text>
      <claim-text>and a scent dispenser coupled to said at least one sensor, wherein when said non-human presence is sensed, said scent dispenser is activated.</claim-text>
    </claim>
    <claim num="32">
      <claim-text>32. The system, as claimed in claim 13, wherein said plurality of modes are coupled such that a change in a value of a first mode is offset by a change in a value of a second mode such that a total value of said coupled modes remains substantially the same.</claim-text>
    </claim>
    <claim num="33">
      <claim-text>33. The system of claim 13, wherein two or more modes of said plurality of modes are adjusted simultaneously and qualitatively to supplement each other.</claim-text>
    </claim>
    <claim num="34">
      <claim-text>34. The system of claim 1, wherein said system is for use with at least one of diagnostics, e-mail, news, weather, traffic, concierge, emergency help, and entertainment.</claim-text>
    </claim>
    <claim num="35">
      <claim-text>35. The system of claim 13, wherein said system is for use with at least one of diagnostics, e-mail, news, weather, traffic, concierge, emergency help, and entertainment.</claim-text>
    </claim>
    <claim num="36">
      <claim-text>36. The system of claim 1, wherein said controller controls the vehicle information system user interface in an operable mode to selectively provide an active, attenuated level of interaction therewith for the driver.</claim-text>
    </claim>
    <claim num="37">
      <claim-text>37. The system of claim 13, wherein said controller controls the vehicle information system user interface in an operable mode to selectively provide an active, attenuated level of interaction therewith for the driver.</claim-text>
    </claim>
  </claims>
</questel-patent-document>