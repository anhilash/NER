<?xml version="1.0" encoding="UTF-8"?>
<questel-patent-document lang="en" date-produced="20180805" produced-by="Questel" schema-version="3.23" file="US06182197B2.xml">
  <bibliographic-data lang="en">
    <publication-reference publ-desc="Granted patent as second publication">
      <document-id>
        <country>US</country>
        <doc-number>06182197</doc-number>
        <kind>B2</kind>
        <date>20010130</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>US6182197</doc-number>
      </document-id>
    </publication-reference>
    <original-publication-kind>B2</original-publication-kind>
    <application-reference is-representative="YES" family-id="22351290" extended-family-id="21231449">
      <document-id>
        <country>US</country>
        <doc-number>09113752</doc-number>
        <kind>A</kind>
        <date>19980710</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>1998US-09113752</doc-number>
      </document-id>
      <document-id data-format="questel_Uid">
        <doc-number>21773790</doc-number>
      </document-id>
    </application-reference>
    <language-of-filing>en</language-of-filing>
    <language-of-publication>en</language-of-publication>
    <priority-claims>
      <priority-claim kind="national" sequence="1">
        <country>US</country>
        <doc-number>11375298</doc-number>
        <kind>A</kind>
        <date>19980710</date>
        <priority-active-indicator>Y</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="1">
        <doc-number>1998US-09113752</doc-number>
      </priority-claim>
    </priority-claims>
    <dates-of-public-availability>
      <publication-of-grant-date>
        <date>20010130</date>
      </publication-of-grant-date>
    </dates-of-public-availability>
    <classifications-ipcr>
      <classification-ipcr sequence="1">
        <text>G06F  12/00        20060101AFI20060310RMJP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>06</class>
        <subclass>F</subclass>
        <main-group>12</main-group>
        <subgroup>00</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>JP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20060310</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="2">
        <text>G06F   9/48        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>06</class>
        <subclass>F</subclass>
        <main-group>9</main-group>
        <subgroup>48</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="3">
        <text>G06F   9/50        20060101ALI20060310RMJP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>06</class>
        <subclass>F</subclass>
        <main-group>9</main-group>
        <subgroup>50</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>JP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20060310</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="4">
        <text>G06F  13/00        20060101ALI20060310RMJP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>06</class>
        <subclass>F</subclass>
        <main-group>13</main-group>
        <subgroup>00</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>JP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20060310</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="5">
        <text>G06F  15/167       20060101ALI20060101BMKR</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>06</class>
        <subclass>F</subclass>
        <main-group>15</main-group>
        <subgroup>167</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>KR</country>
        </generating-office>
        <classification-status>B</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20060101</date>
        </action-date>
      </classification-ipcr>
    </classifications-ipcr>
    <classification-national>
      <country>US</country>
      <main-classification>
        <text>711151000</text>
        <class>711</class>
        <subclass>151000</subclass>
      </main-classification>
      <further-classification sequence="1">
        <text>711147000</text>
        <class>711</class>
        <subclass>147000</subclass>
      </further-classification>
      <further-classification sequence="2">
        <text>711167000</text>
        <class>711</class>
        <subclass>167000</subclass>
      </further-classification>
      <further-classification sequence="3">
        <text>711203000</text>
        <class>711</class>
        <subclass>203000</subclass>
      </further-classification>
    </classification-national>
    <classifications-ecla>
      <classification-ecla sequence="1">
        <text>G06F-009/48C4S</text>
        <section>G</section>
        <class>06</class>
        <subclass>F</subclass>
        <main-group>009</main-group>
        <subgroup>48C4S</subgroup>
      </classification-ecla>
    </classifications-ecla>
    <patent-classifications>
      <patent-classification sequence="1">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>G06F-009/4881</classification-symbol>
        <section>G</section>
        <class>06</class>
        <subclass>F</subclass>
        <main-group>9</main-group>
        <subgroup>4881</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
    </patent-classifications>
    <number-of-claims>20</number-of-claims>
    <exemplary-claim>1</exemplary-claim>
    <figures>
      <number-of-drawing-sheets>2</number-of-drawing-sheets>
      <number-of-figures>3</number-of-figures>
      <image-key data-format="questel">US6182197</image-key>
    </figures>
    <invention-title format="original" lang="en" id="title_en">Real-time shared disk system for computer clusters</invention-title>
    <references-cited>
      <citation srep-phase="examiner">
        <patcit num="1">
          <text>SHIOBARA YASHUHISA</text>
          <document-id>
            <country>US</country>
            <doc-number>4930121</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4930121</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="2">
          <text>BYRN JONATHAN WILLIAM, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5761716</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5761716</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="3">
          <text>SUMIMOTO SHINJI</text>
          <document-id>
            <country>US</country>
            <doc-number>5813016</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5813016</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="4">
          <text>BLOUNT MARION L, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5222217</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5222217</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="5">
          <text>HEIZER ISAAC J</text>
          <document-id>
            <country>US</country>
            <doc-number>5249290</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5249290</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="6">
          <text>HAMMERSLEY SCOTT D, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5392433</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5392433</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="7">
          <text>YOKOTA MASAYUKI</text>
          <document-id>
            <country>US</country>
            <doc-number>5414856</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5414856</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="8">
          <text>CIDON ISRAEL, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5446737</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5446737</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="9">
          <text>BARTON RICHARD R</text>
          <document-id>
            <country>US</country>
            <doc-number>5502840</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5502840</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="10">
          <text>SANDBERG JONATHAN</text>
          <document-id>
            <country>US</country>
            <doc-number>5592625</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5592625</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="11">
          <text>KURABAYASHI HIROAKI, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5592673</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5592673</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="12">
          <text>ATTANASIO CLEMENT RICHARD, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5668943</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5668943</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="13">
          <text>BENNETT ROBERT BRADLEY</text>
          <document-id>
            <country>US</country>
            <doc-number>5734909</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5734909</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="14">
          <text>KANAMORI ATSUSHI, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5754854</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5754854</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="1">
          <text>Publication: Bennett et al. "Munin: Distributed Shared Memory Based on Type-Specific memory Coherence," pp. 1-9. Proc. of the 2nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming. Seattle, WA. 1990.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="2">
          <text>Publication: Dias et al. "A Scalable and Highly Available Web Server." pp. 1-8. IEEE, Compcon, 1996.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="3">
          <text>Publication: Tewari et al. "Design and Performance Tradeoffs in Clustered Video Servers." pp. 1-7. Proc. of the Int'l Conference on Multimedia Computing and Systems. Japan, 1996.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="4">
          <text>Publication: Tewari et al. "High Availability in Clustered Multimedia Servers." pp. 1-10. Proc. of Int'l Conference on Data Engineering. New Orleans, 1996.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="5">
          <text>Publication: Attanasio et al. "Design and Implementation of a Recoverable Virtual Shared Disk." pp. 1-18. IBM Research Report RC 19843. T.J. Watson Research Center. New York, 1994.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="6">
          <text>Publication: Haskin et al. "The Tiger Shark File System." pp. 1-5. IBM Almaden Research Center. Undated.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="7">
          <text>Publication: John B. Carter. "Efficient Distributed Shared Memory Based on Multi-Protocol Release Consistency." Rice University. Houston, Tx. Sep., 1993.</text>
        </nplcit>
      </citation>
    </references-cited>
    <parties>
      <applicants>
        <applicant data-format="original" app-type="applicant" sequence="1">
          <addressbook lang="en">
            <orgname>International Business Machines Corporation</orgname>
            <address>
              <address-1>Armonk, NY, US</address-1>
              <city>Armonk</city>
              <state>NY</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </applicant>
        <applicant data-format="questel" app-type="applicant" sequence="2">
          <addressbook lang="en">
            <orgname>IBM</orgname>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </applicant>
      </applicants>
      <inventors>
        <inventor data-format="original" sequence="1">
          <addressbook lang="en">
            <name>Dias, Daniel Manuel</name>
            <address>
              <address-1>Mahopac, NY, US</address-1>
              <city>Mahopac</city>
              <state>NY</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
        <inventor data-format="original" sequence="2">
          <addressbook lang="en">
            <name>Mukherjee, Rajat</name>
            <address>
              <address-1>San Jose, CA, US</address-1>
              <city>San Jose</city>
              <state>CA</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
      </inventors>
      <agents>
        <agent sequence="1" rep-type="agent">
          <addressbook lang="en">
            <name>Rogitz, John L.</name>
          </addressbook>
        </agent>
      </agents>
    </parties>
    <examiners>
      <primary-examiner>
        <name>Yoo, Do Hyun</name>
      </primary-examiner>
    </examiners>
    <lgst-data>
      <lgst-status>EXPIRED</lgst-status>
    </lgst-data>
  </bibliographic-data>
  <abstract format="original" lang="en" id="abstr_en">
    <p id="P-EN-00001" num="00001">
      <br/>
      A clustered computer system includes a shared data storage system, preferably a virtual shared disk (VSD) memory system, to which the computers in the cluster write data and from which the computers read data, using data access requests.
      <br/>
      The data access requests can be associated with deadlines, and individual storage devices in the shared storage system satisfy competing requests based on the deadlines of the requests.
      <br/>
      The deadlines can be updated and requests can be killed, to facilitate real time data access for, e.g., multimedia applications such as video on demand.
    </p>
  </abstract>
  <description format="original" lang="en" id="desc_en">
    <heading>BACKGROUND OF THE INVENTION</heading>
    <p num="1">
      1.
      <br/>
      Field of the Invention
    </p>
    <p num="2">The present invention relates generally to clustered computer systems, and more particularly to systems and methods for accessing shared disk data in a clustered computer environment.</p>
    <p num="3">2. DESCRIPTION OF THE RELATED ART</p>
    <p num="4">
      Clustered computing refers to the ability of many computer processors to work in consonance with each other to undertake respective portions of large scale computing tasks, or to execute separate applications using a common data storage resource.
      <br/>
      The computers communicate with each other and/or the data storage resource via a network to, e.g., distribute computational chores among the computers, or to execute their respective independent applications.
      <br/>
      Using many computers working together to undertake large scale computations is often more cost-effective than providing a single monolithic processor to execute such tasks.
    </p>
    <p num="5">
      In many clustered computing systems, each computer might be physically connected to one or more respective data storage devices, typically computer disks.
      <br/>
      Further, to enable more than a single computer to access data on a disk, the disk might be physically connected to a plurality of computers.
      <br/>
      Such shared system disks are referred to as "physically shared" disks, because they are physically connected to the computers of the system.
      <br/>
      It will readily be appreciated that such a system, referred to as a shared storage system, not only distributes computations among the several computers, but also distributes data across the several disks.
    </p>
    <p num="6">
      While physically connecting a disk to several computers is an effective method for sharing storage, it happens that the accompanying input/output (I/O) hardware can be relatively expensive.
      <br/>
      Moreover, such a system is not readily scalable.
      <br/>
      More particularly, connecting each one of a large number of computers to each of a large number of disks is not only expensive, but requires excessive and complicated cabling.
    </p>
    <p num="7">
      Accordingly, the present assignee has introduced a system referred to as "virtual shared disk", or "VSD", in which each computer of a clustered computing system regards each one of many system disks as being physically connected all the system computers, despite the fact that each disk is physically connected only to one or a few computers.
      <br/>
      VSD achieves this essentially by providing, for each computer, a software module representing a respective system disk, with the software module appearing to the computer as a device driver for the respective disk.
      <br/>
      To read or write to a disk that is not physically connected to it, a computer invokes the device driver of the "virtual" disk as it would a physically shared disk to request a read or write operation, with the underlying VSD software module then sending the request to the computer in the system that is actually physically connected to the disk.
    </p>
    <p num="8">
      Regardless of whether the shared system disks are virtual or physical, however, it will be appreciated that two or more system computers might issue read or write requests to a single system disk virtually simultaneously with each other.
      <br/>
      When this happens, a system disk addresses the competing requests based on considerations that are internal to the disk, and not based on the order in which the requests were received or indeed on any consideration external to the disk.
      <br/>
      For example, a system disk might respond to competing requests based on the current position of the disk head relative to the sectors of the disk on which the requested data is stored (or is to be stored), with the request that is "closest" to the head being addressed first.
    </p>
    <p num="9">
      As recognized by the present invention, the above-mentioned process for addressing competing requests to a disk, whether virtually or physically, has the drawback of not considering the fact that one request might be more urgent than another.
      <br/>
      This is particularly unfortunate in multimedia applications, which typically read a large block of data bits that must be delivered in a particular temporal order.
      <br/>
      Thus, as recognized herein, read and write requests can define response deadlines (i.e., temporally-based priorities) or other, non-temporally-based priorities, beyond which the requests become stale, and satisfying them late consequently makes no sense.
      <br/>
      As discussed above, however, current shared storage systems do not respond to requests based on priorities defined by the requests, but rather based on internal disk considerations, much less do current shared storage systems consider whether a request should be terminated if it cannot be satisfied within the deadline (or other priority).
    </p>
    <p num="10">
      Moreover, the present invention understands that one multimedia application might request a first video frame bit that is to be played sooner than a second video frame bit that might be requested by another application.
      <br/>
      In such a circumstance, it would be desirable for the shared storage system to respond to the first request before the second request, regardless of whether the first request was received before the second request.
      <br/>
      Unfortunately, current shared disk systems do not contemplate considering the relative priorities of competing requests in determining how to respond to plural near-simultaneous requests to a single system disk.
    </p>
    <p num="11">Accordingly, the present invention recognizes that shared storage systems including VSD can better supply data for multimedia streams for, e.g., as video-on-demand, if the shared storage system performance can be made to model real-time multimedia data streaming.</p>
    <heading>SUMMARY OF THE INVENTION</heading>
    <p num="12">
      The invention is a general purpose computer programmed according to the inventive steps herein to update and terminate data access requests to a shared storage system in a clustered computer system.
      <br/>
      The invention can also be embodied as an article of manufacture--a machine component--that is used by a digital processing apparatus and which tangibly embodies a program of instructions that are executable by the digital processing apparatus to undertake the present invention.
      <br/>
      This invention is realized in a critical machine component that causes a digital processing apparatus to perform the inventive method steps herein.
    </p>
    <p num="13">
      In accordance with the present invention, a computer system includes plural client nodes that communicate data access requests to plural storage nodes.
      <br/>
      The system further includes logic means for associating one or more of the data access requests with respective priorities, e.g., time-based deadlines.
      <br/>
      Additionally, the system includes logic means for sending the data access requests and priorities to the storage nodes.
      <br/>
      Moreover, the system includes logic means for ordering the data access requests at the storage nodes based on the respective priorities, such that the data access requests are satisfied in consideration of their respective priorities.
    </p>
    <p num="14">
      In a preferred embodiment, logic means are provided for changing the priorities of the data access requests, prior to the requests being satisfied by a storage node, to render an updated priority.
      <br/>
      Logic means then reorder data access requests at the storage nodes, based on the updated priorities.
      <br/>
      Furthermore, logic means can be provided for terminating one or more data access requests.
    </p>
    <p num="15">
      Preferably, the computing and storage nodes are loosely synchronized with each other.
      <br/>
      In an exemplary embodiment, the system is a virtual shared disk (VSD) system, and each storage node includes at least one storage computer and at least one data storage device.
      <br/>
      Unless the invention is implemented on a system disk controller, each storage computer includes logic means for sending no more than one data access request at a time to the data storage device, such that the data storage device cannot reorder the sequence of responding to data access requests based on considerations that are internal to the data storage device.
    </p>
    <p num="16">
      In another aspect, in a computer system having plural computers communicating data access requests to a shared storage system, a computer-implemented method is disclosed for satisfying at least two contemporaneous data access requests to a single data storage device of the shared storage system.
      <br/>
      The method includes responding to the requests in an order that is defined by one or more considerations external to the data storage device.
    </p>
    <p num="17">
      In yet another aspect, a computer program device includes a computer program storage device readable by a digital processing apparatus and a program means on the program storage device.
      <br/>
      The program means includes instructions that are executable by the digital processing apparatus for performing method steps for satisfying one or more data access requests.
      <br/>
      As disclosed in detail below, the method steps embodied by the program means include associating at least some of the data access requests with respective priorities, and then sending the priorities and the data access requests to a shared storage system.
      <br/>
      With this invention, the shared storage system can respond to the requests using the priorities.
    </p>
    <p num="18">
      In another aspect, a computer program device includes a computer program storage device readable by a digital processing apparatus and a program means on the program storage device.
      <br/>
      The program means includes instructions that are executable by the digital processing apparatus for performing method steps for satisfying one or more data access requests.
      <br/>
      As disclosed in detail below, the method steps embodied by the program means include responding, with a memory system, to at least some of the data access requests in an order based on respective priorities, with the priorities and the data access requests being sent to the memory system.
    </p>
    <p num="19">The details of the present invention, both as to its structure and operation, can best be understood in reference to the accompanying drawings, in which like reference numerals refer to like parts, and in which:</p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading>
    <p num="20">
      FIG. 1 is a schematic diagram showing the system of the present invention;
      <br/>
      FIG. 2 is a schematic representation of a program storage device; and
      <br/>
      FIG. 3 is a flow chart showing the logic of the present invention.
    </p>
    <heading>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
    <p num="21">
      Referring initially to FIG. 1, a system is shown, generally designated 10, which includes plural client computers 12 (only client computers 1-4 shown for clarity of disclosure) that access, via a network 14, preferably a high bandwidth network, a shared storage system which includes plural storage nodes 16 (only four storage nodes 16 shown for clarity of disclosure).
      <br/>
      By "shared storage" is meant shared electronic data storage.
      <br/>
      As but one example of the possible functions of the client computers 12, the client computer 12 numbered "1" in FIG. 1 can be a video server, whereas the client computer numbered "3" in FIG. 1 can be a high speed World Wide Web server computer, with both client computers 12 communicating simultaneously with respective clients via a wide area network (WAN) 17, such as the Internet or other WAN.
    </p>
    <p num="22">
      Each storage node 16 includes at least one respective storage computer or storage controller 18 and at least one data storage device 20 that can be, e.g., a data storage disk which is physically connected to its associated storage computer 18.
      <br/>
      In other words, the present invention can be implemented on the element 18 regardless of whether it is a storage computer (as referred to below) or a storage controller.
      <br/>
      It is to be understood that while the storage computers 18 are shown separately from the client computers 12, if desired each client computer 12 can function as a server computer to the remaining client computers with respect to any data storage device that is physically connected to the client computer.
      <br/>
      In any case, the computers of the present invention can be desktop computers such as personal computers or laptop computers made by International Business Machines Corporation (IBM) of Armonk, N.Y. Alternatively, the computers of the present invention may be any computer, including computers sold under trademarks such as AS400, with accompanying IBM Network Stations or, Unix computers, or OS/2 servers, or Windows NT servers, or IBM RS/6000 250 workstations, or other equivalent devices.
    </p>
    <p num="23">
      In the preferred embodiment, the system 10 uses the virtual shared disk ("VSD") system disclosed in U.S. Pat. No. 5,668,943, owned by the present assignee and incorporated herein by reference.
      <br/>
      Accordingly, as set forth in the above-referenced patent, each client computer 12 communicates with four virtual disks 22 as though the virtual disks 22 were physically connected to the client computer 12.
      <br/>
      Actually, however, the client computers 12 need not be physically connected to any storage device, and the storage computer numbered "1" in FIG. 1 might be physically connected only to the disk numbered "1".
      <br/>
      Likewise, the storage computer numbered "2" might be physically connected only to the disk numbered "2", and so on, with access between the client computers 12 and their virtual disks being undertaken in accordance with the above-referenced patent.
      <br/>
      It is to be understood, however, that the principles herein apply to shared storage systems in which some or all system disks are physically shared disks, that is, are physically connected to each client computer in the system.
    </p>
    <p num="24">
      In accordance with the present invention, each client computer 12 can access a deadline module 24 (only one deadline module 24 shown in FIG. 1 for clarity of disclosure).
      <br/>
      The deadline module 24 can be executed by the respective client computer 12, or by a software-implemented module such as a file system 26 (only one file system 26 shown in FIG. 1) that is associated with the client computer 12, to undertake the inventive logic disclosed below in detail.
    </p>
    <p num="25">
      It is to be understood that the control components such as the deadline module 24 are executed by logic components such as are embodied in logic circuits or in software contained in an appropriate electronic data storage, e.g., a hard disk drive and/or optical disk drive, that are conventionally coupled to the respective client computer 12.
      <br/>
      Or, the control components can be embodied in other logical components such as a computer diskette 28 shown in FIG. 2.
      <br/>
      The diskette 28 shown in FIG. 2 has a computer usable medium 30 on which are stored computer readable code means (i.e., program code elements) A-D.
      <br/>
      In any case, execution of the particular logic components/computer software steps can be distributed among one or more of the computers 12, 18.
    </p>
    <p num="26">
      The flow charts herein illustrate the structure of the deadline module of the present invention as embodied in computer program software.
      <br/>
      Those skilled in the art will appreciate that the flow charts illustrate the structures of logic elements, such as computer program code elements or electronic logic circuits, that function according to this invention.
      <br/>
      Manifestly, the invention is practiced in its essential embodiment by a machine component that renders the logic elements in a form that instructs a digital processing apparatus (that is, a computer) to perform a sequence of function steps corresponding to those shown.
    </p>
    <p num="27">
      In other words, the deadline module 24 may be a computer program that is executed by a processor within the associated client computer 12 (or file system 26) as a series of computer-executable instructions.
      <br/>
      In addition to the drives mentioned above, these instructions may reside, for example, in RAM of the computer, or the instructions may be stored on a DASD array, magnetic tape, electronic read-only memory, or other appropriate data storage device.
      <br/>
      In an illustrative embodiment of the invention, the computer-executable instructions may be lines of compiled C+ compatible code.
    </p>
    <p num="28">
      For reasons that will become clearer after the below discussion, each storage computer 18 includes a respective request queue 32 in which data read and write requests are ordered by priority.
      <br/>
      Unless the system 10 includes a system disk controller, only a single input/output (I/O) operation is presented to the associated disk 20 at a time.
      <br/>
      Also, a software-or hardware-implemented conventional time synchronizer 34 preferably synchronizes the clocks of the computers 12, 18.
    </p>
    <p num="29">
      By "priority" is meant, in general, a data access request constraint that constrains the system 10 to respond to requests using the respective priorities, but not necessarily to satisfy the requests in absolute order of their priorities, depending on the state of the system 10.
      <br/>
      For example, although, as disclosed below, data access requests are ordered by priority in the requests queues of the disks, a request of a lower priority might be served before a request of a later-arriving but higher priority, if the particular request queue is sufficiently short such that the non-real time request can be satisfied first and the real time request then satisfied within its priority constraint (for example, the deadline).
    </p>
    <p num="30">
      Now referring to FIG. 3, the logic of the present invention can be discerned.
      <br/>
      While FIG. 3 illustrates the logic in a flow sequence for ease of disclosure, it is to be understood that one or more of the steps disclosed below, being event-driven, are undertaken asynchronously.
    </p>
    <p num="31">
      Commencing at block 36, the various nodes of the system 10 (i.e., the client computers 12 with virtual disks, if any, and the storage nodes 16) are loosely synchronized by means of, e.g., the time synchronizer 34.
      <br/>
      By "loosely" synchronized is meant that the clocks of the system nodes are synchronized to have the same time reference within the granularity of individual data access requests (typically tens of milliseconds).
      <br/>
      As recognized by the present invention, such synchronization avoids a storage node 16 favoring one or more client computers 12, effectively ignoring requests from non-favored client computers 12.
    </p>
    <p num="32">
      Moving to block 38, the logic next correlates or otherwise associates request priorities with some or all data access requests, e.g., read and write requests, from the client computers 12 to the nodes 16 of the shared storage of the system 10.
      <br/>
      These priorities need not be time-based, but in one exemplary embodiment, the priorities are established by time-based deadlines.
      <br/>
      The deadline of a data access request represents a latest desired time of response to the request.
    </p>
    <p num="33">
      In some cases, the priority of a data access request is appended to the request itself.
      <br/>
      In other circumstances, e.g., when the kernel data structure does not permit tagging a request block with a
    </p>
    <p num="34">priority, the priority is transmitted apart from the associated data access request and matched later with the request, to be transmitted with the request to the appropriate storage node 16.</p>
    <p num="35">
      In the preferred embodiment, at block 38 real-time application programming interfaces (API) are invoked to generate appropriate priorities based on, e.g., the count of real-time streams being supported, and on the file rates and file formats being used.
      <br/>
      If the request is a non-real time request, i.e., a request that does not require fulfillment within any particular time period, such as a request to access a text file, a NULL deadline is assumed, which is dealt with as more fully described below.
    </p>
    <p num="36">
      As intended by the present invention, the API can be invoked by the end user/application of the requesting client computer 12, but more preferably are invoked by an intermediate subsystem, such as the file system 26 shown in FIG. 1, that is associated with the requesting end user/application.
      <br/>
      In any case, the API of the present invention are implemented as input/output control (IOCTL) calls.
      <br/>
      An exemplary priority data structure (with comments) for the preferred VSD embodiment is as follows:
    </p>
    <p num="37">
      --
      <br/>
      --               typedef struct timestruct_t DEADLINE;
      <br/>
      --               struct vsd_deadline
      <br/>
      --               +
      <br/>
      --                char  *buf;
      <br/>
      --                 /* The user buffer for the data  */
      <br/>
      --                DEADLINE deadline;
      <br/>
      --                 /* Deadline associated with the I/O */
      <br/>
      --                int sector_offset;
      <br/>
      --                 /* Sector offset into storage device */
      <br/>
      --                int  sector_count;
      <br/>
      --                 /* Number of sectors to be read  */
      <br/>
      --               };
    </p>
    <p num="38">For read data access requests, the preferred API is</p>
    <p num="39">int ioctl(fd,GIODREAD,&amp;dead);</p>
    <p num="40">
      wherein "fd" is the file descriptor that is passed from the requesting application/file system, "GIODREAD"is a constant that identifies a global input/output request, and "&amp;dead" identifies the associated priority data structure.
      <br/>
      The above-disclosed read IOCTL reads data into a buffer pointed to by a field in the VSD_DEADLINE data structure set forth above.
      <br/>
      As mentioned previously, the data structure also contains a priority such as a real-time deadline that is used as described below.
    </p>
    <p num="41">In contrast, for write data access requests, the preferred API is</p>
    <p num="42">int ioctl(fd GIOD WRITE, &amp;dead);</p>
    <p num="43">
      The above-disclosed write IOCTL writes data from a buffer pointed to by a field in the VSD_DEADLINE data structure set forth above.
      <br/>
      As is the case for read requests, the data structure also contains a priority for write requests.
    </p>
    <p num="44">
      Moreover, the priority is associated with all subrequests (packets) that an individual data access request might be split into.
      <br/>
      More specifically, a single data access request can be arranged in multiple packets, based on a message quantum size that is determined by the protocol of the network 14, with all the packets being associated with the same priority.
      <br/>
      Accordingly, at block 40, the packetized data access requests and associated priorities are transmitted via the network 14 to the appropriate storage nodes 16 of the shared storage of the system 10.
    </p>
    <p num="45">
      If desired, the subrequests to a particular disk 20 can be reassembled at the associated storage computer 18, and then sent to the disk 20 as the complete request, to promote the efficiency with which the disk 20 responds to the request.
      <br/>
      Or, the subrequests can be ordered in the proper sequence and then sent directly to the appropriate disk 20.
    </p>
    <p num="46">
      From block 40, the logic proceeds to block 42, wherein the data access requests with priorities are received by the shared storage and wherein each storage node 16 that receives data access requests orders the requests by priority in the associated request queue 32 (FIG. 1).
      <br/>
      For a data access request that is non-real time and that consequently has been assigned a null priority in the exemplary embodiment, the request nevertheless is assigned the priority (plus one) of the last real-time access request in the queue 32 upon arrival of the non-real time request at the queue 32.
      <br/>
      This ensures that the non-real time request will not be continually surpassed by new requests with non-null deadlines.
    </p>
    <p num="47">
      Other schemes can be used, however, to address non-real time requests while ensuring non-starvation.
      <br/>
      For example, non-real time requests could be satisfied only if all the queued real-time requests can be satisfied within their respective priorities, or if a maximum latency guaranteed to non-real time requests is met.
    </p>
    <p num="48">
      In brief cross-reference to FIGS. 1 and 3, unless a system disk controller is used, each storage computer 18 permits only one request at a time from its queue 32 to be passed to the associated storage device 20.
      <br/>
      In other words, at block 42 contemporaneous data access requests are satisfied serially, using only a single input/output operation at a time with the storage device 20, when no disk controller is used to control the satisfying of requests.
      <br/>
      This avoids the device 20 controller changing the ordering between requests based on criteria that are internal to the particular device 20.
      <br/>
      Thus, the data access requests are responded to in an order that is defined by one or more considerations that are external to the data storage device 20.
      <br/>
      In one embodiment, the queue 32 uses only a single buffer for device 20 access.
    </p>
    <p num="49">
      It is to be understood that when a single storage computer 18 controls more than one disk 20, a request to one disk 20 can be made simultaneously with a request to another disk 20 controlled by the computer 18.
      <br/>
      In such a circumstance, the storage computer 18 will include a respective request queue for each disk 20 with which it is associated.
    </p>
    <p num="50">
      Moving to decision diamond 44, the logic asynchronously determines whether a priority of a request should be updated, or whether the request should be terminated ("killed").
      <br/>
      As recognized by the present invention, updating a request's priority is useful when the requesting application/file system must dynamically shift priorities of a data stream on demand, or when a certain time threshold has been reached, or to offset for additional real-time streams or rescheduling of stream invocation (as when video cassette recorder functionality such as "stop", "resume", etc. are supported).
      <br/>
      Moreover, terminating an outstanding request when, e.g., a storage device 20 stops responding to requests, frees the requesting application to request replica data blocks on other devices 20 in the shared storage of the system 10.
    </p>
    <p num="51">
      If no request is to be updated or terminated, the process ends at state 46.
      <br/>
      Otherwise, the logic moves to block 48 to update the priority of a request or to terminate a request as follows, for the exemplary VSD embodiment.
      <br/>
      When a VSD client computer 12 transmits a data access request to the shared storage of the system 10, the client computer 12 maintains a copy of the outstanding request in a local pending queue, with the copy being associated with a timer.
      <br/>
      To update the priority of a request, or to terminate a request, the timer that is associated with the request at the client computer 12 is forced to zero at block 48, if the request has not yet been satisfied.
      <br/>
      Then, at block 50 the request is retransmitted to the shared storage of the system 10 along with a new (updated) priority or, in the case of termination, a termination request is transmitted.
    </p>
    <p num="52">In the preferred embodiment, the IOCTLs for respectively updating a request priority and terminating the request are as follows:</p>
    <p num="53">int ioctl(fd,GIODUPDT,&amp;dead);</p>
    <p num="54">int ioctl(fd,GIODKILL,&amp;dead), wherein "GIODUPDT" and "GIODKILL" are constants that identify global input/output requests, and "&amp;dead" identifies the associated priority data structures.</p>
    <p num="55">
      For the termination ("kill") operation, the IOCTL determines whether any pending requests use the same buffer that is pointed to by a field in the above-disclosed VSD_DEADLINE data structure.
      <br/>
      If so, the process that made the request is freed and the request is removed at least from the requesting client computer's pending queue, using the shared memory semantics of the operating system.
    </p>
    <p num="56">
      Similarly, for the updating operation, the IOCTL determines whether any pending requests use the same buffer that is pointed to by a field in the above-disclosed VSD_DEADLINE data structure.
      <br/>
      If so, the above-described retransmission of the request, with new priority, is forced as described above.
    </p>
    <p num="57">
      When the appropriate storage device 20 receives the updated request (or termination request, if sent) it determines, at decision diamond 52, whether the old request is still in the associated queue 32.
      <br/>
      That is, the appropriate storage device 20 determines whether the old request has already been satisfied, and if so, the logic moves to block 54 to ignore the updated request or termination request.
      <br/>
      Otherwise, the logic proceeds to block 56 to reorder the requests in the appropriate queue 32 in accordance with the updated priority of the request (or in accordance with the termination and removal of the request from the queue 32), and then the process ends at state 46.
      <br/>
      If desired, as mentioned above for "kill" requests, it may be necessary to terminate the request only at the requesting client computer 12 to free the requesting application/file system.
      <br/>
      It might not be necessary to actually transmit the termination request to the remote storage device 20.
    </p>
    <p num="58">While the particular REAL-TIME SHARED DISK SYSTEM FOR COMPUTER CLUSTERS as herein shown and described in detail is fully capable of attaining the above-described objects of the invention, it is to be understood that it is the presently preferred embodiment of the present invention and is thus representative of the subject matter which is broadly contemplated by the present invention, that the scope of the present invention fully encompasses other embodiments which may become obvious to those skilled in the art, and that the scope of the present invention is accordingly to be limited by nothing other than the appended claims, in which reference to an element in the singular means "at least one" unless otherwise recited.</p>
  </description>
  <claims format="original" lang="en" id="claim_en">
    <claim num="1">
      <claim-text>We claim:</claim-text>
      <claim-text>1.</claim-text>
      <claim-text>A computer system including plural client nodes communicating data access requests to one or more storage nodes, comprising:</claim-text>
      <claim-text>logic means for associating one or more of the data access requests with respective priorities; logic means for sending the data access requests and priorities to the storage nodes; logic means for ordering the data access requests at the storage nodes based on the respective priorities, such that the data access requests are satisfied in consideration of their respective priorities; logic means for changing a priority of at least one data access request, prior to the request being satisfied by a storage node, to render an updated priority;</claim-text>
      <claim-text>and logic means for reordering data access requests at the storage nodes, based on the updated priority.</claim-text>
    </claim>
    <claim num="2">
      <claim-text>2. The system of claim 1, further comprising: logic means for terminating at least one data access request.</claim-text>
    </claim>
    <claim num="3">
      <claim-text>3. The system of claim 1, further comprising means for loosely synchronizing the computing and storage nodes with each other.</claim-text>
    </claim>
    <claim num="4">
      <claim-text>4. The system of claim 1, wherein each storage node includes at least one storage computer and at least one data storage device, and the storage computer includes logic means for sending no more than one data access request at a time to the data storage device, such that the data storage device cannot reorder the sequence of responding to data access requests based on considerations internal to the data storage device.</claim-text>
    </claim>
    <claim num="5">
      <claim-text>5. The system of claim 1, wherein the system is a virtual shared disk system.</claim-text>
    </claim>
    <claim num="6">
      <claim-text>6. The system of claim 1, wherein the priorities include time-based deadlines.</claim-text>
    </claim>
    <claim num="7">
      <claim-text>7. In a computer system having plural processors communicating data access requests to a shared storage system, a computer-implemented method for satisfying at least two contemporaneous data access requests to a single data storage device of the shared storage system, comprising the steps of: responding to the requests in an order defined at least in part by one or more considerations external to the data storage device associated with at least one storage computer, wherein the storage computer sends no more than one data access request at a time to the data storage device, such that the data storage device cannot reorder a sequence of responding to data access requests based on considerations internal to the data storage device.</claim-text>
    </claim>
    <claim num="8">
      <claim-text>8. The method of claim 7, wherein the one or more considerations external to the data storage device include a data request priority.</claim-text>
    </claim>
    <claim num="9">
      <claim-text>9. The method of claim 8, wherein the priority includes a time-based deadline.</claim-text>
    </claim>
    <claim num="10">
      <claim-text>10. The method of claim 9, further comprising: associating one or more of the data access requests with respective priorities; sending the data access requests and priorities to storage nodes in the shared storage system, each storage node including at least one data storage device;</claim-text>
      <claim-text>and ordering the data access requests at the storage nodes based on the respective priorities, such that the data access requests are satisfied in accordance with their respective priorities.</claim-text>
    </claim>
    <claim num="11">
      <claim-text>11. The method of claim 10, further comprising: changing a priority of at least one data access request, prior to the request being satisfied by a storage node, to render an updated priority;</claim-text>
      <claim-text>and reordering data access requests at the storage nodes, based on the updated priority.</claim-text>
    </claim>
    <claim num="12">
      <claim-text>12. The method of claim 9, further comprising: terminating at least one data access request, prior to the request being satisfied by a storage node.</claim-text>
    </claim>
    <claim num="13">
      <claim-text>13. The method of claim 9, further comprising loosely synchronizing the computing and storage nodes with each other.</claim-text>
    </claim>
    <claim num="14">
      <claim-text>14. The method of claim 7, wherein the system is a virtual shared disk system.</claim-text>
    </claim>
    <claim num="15">
      <claim-text>15. A computer program device comprising: a computer program storage device readable by a digital processing apparatus;</claim-text>
      <claim-text>and a program means on the program storage device and including instructions executable by the digital processing apparatus for performing method steps for satisfying one or more data access requests, the method steps comprising:</claim-text>
      <claim-text>- associating at least some of the data access requests with respective priorities; - sending the priorities and the data access requests to a shared storage system, such that the shared storage system can respond to the requests in consideration of the priorities, and - changing a priority of at least one data access request, prior to the request being satisfied by the shared storage system, to render an updated priority such that data access requests can be reordered in the shared storage system, based on the updated priority.</claim-text>
    </claim>
    <claim num="16">
      <claim-text>16. The computer program device of claim 15, wherein the shared storage system is a virtual shared disk system and at least some of the priorities are time-based deadlines.</claim-text>
    </claim>
    <claim num="17">
      <claim-text>17. The computer program device of claim 15, wherein the method steps further comprise: terminating at least one data access request, prior to the request being satisfied by the shared storage system.</claim-text>
    </claim>
    <claim num="18">
      <claim-text>18. The computer program device of claim 15, wherein the method steps further comprise loosely synchronizing the data access requests with each other.</claim-text>
    </claim>
    <claim num="19">
      <claim-text>19. A computer program device comprising: a computer program storage device readable by a digital processing apparatus;</claim-text>
      <claim-text>and a program means on the program storage device and including instructions executable by the digital processing apparatus for performing method steps for satisfying one or more data access requests, the method steps comprising: - responding, with a memory system including at one data storage device, to at least some of the data access requests based on respective priorities, the priorities and the data access requests being sent to the memory system;</claim-text>
      <claim-text>and - sending no more than one data access request at a time to the data storage device, such that the data storage device cannot reorder the sequence of responding to data access requests based on considerations internal to the data storage device.</claim-text>
    </claim>
    <claim num="20">
      <claim-text>20. The computer program device of claim 19, wherein the method steps comprise reordering data access requests at a storage computer in response to a changed priority message, prior to the request being satisfied by the system.</claim-text>
    </claim>
  </claims>
</questel-patent-document>