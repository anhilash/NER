<?xml version="1.0" encoding="UTF-8"?>
<questel-patent-document lang="en" date-produced="20180805" produced-by="Questel" schema-version="3.23" file="US06184792B2.xml">
  <bibliographic-data lang="en">
    <publication-reference publ-desc="Granted patent as second publication">
      <document-id>
        <country>US</country>
        <doc-number>06184792</doc-number>
        <kind>B2</kind>
        <date>20010206</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>US6184792</doc-number>
      </document-id>
    </publication-reference>
    <original-publication-kind>B2</original-publication-kind>
    <application-reference is-representative="YES" family-id="24206370" extended-family-id="694149">
      <document-id>
        <country>US</country>
        <doc-number>09552688</doc-number>
        <kind>A</kind>
        <date>20000419</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>2000US-09552688</doc-number>
      </document-id>
      <document-id data-format="questel_Uid">
        <doc-number>711408</doc-number>
      </document-id>
    </application-reference>
    <language-of-filing>en</language-of-filing>
    <language-of-publication>en</language-of-publication>
    <priority-claims>
      <priority-claim kind="national" sequence="1">
        <country>US</country>
        <doc-number>55268800</doc-number>
        <kind>A</kind>
        <date>20000419</date>
        <priority-active-indicator>Y</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="1">
        <doc-number>2000US-09552688</doc-number>
      </priority-claim>
    </priority-claims>
    <dates-of-public-availability>
      <publication-of-grant-date>
        <date>20010206</date>
      </publication-of-grant-date>
    </dates-of-public-availability>
    <classifications-ipcr>
      <classification-ipcr sequence="1">
        <text>G08B  17/12        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>08</class>
        <subclass>B</subclass>
        <main-group>17</main-group>
        <subgroup>12</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="2">
        <text>F23N   5/08        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>F</section>
        <class>23</class>
        <subclass>N</subclass>
        <main-group>5</main-group>
        <subgroup>08</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="3">
        <text>G08B  13/194       20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>08</class>
        <subclass>B</subclass>
        <main-group>13</main-group>
        <subgroup>194</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
    </classifications-ipcr>
    <classification-national>
      <country>US</country>
      <main-classification>
        <text>340578000</text>
        <class>340</class>
        <subclass>578000</subclass>
      </main-classification>
      <further-classification sequence="1">
        <text>250336100</text>
        <class>250</class>
        <subclass>336100</subclass>
      </further-classification>
      <further-classification sequence="2">
        <text>250339150</text>
        <class>250</class>
        <subclass>339150</subclass>
      </further-classification>
      <further-classification sequence="3">
        <text>340577000</text>
        <class>340</class>
        <subclass>577000</subclass>
      </further-classification>
      <further-classification sequence="4">
        <text>348082000</text>
        <class>348</class>
        <subclass>082000</subclass>
      </further-classification>
      <further-classification sequence="5">
        <text>382100000</text>
        <class>382</class>
        <subclass>100000</subclass>
      </further-classification>
      <further-classification sequence="6">
        <text>382195000</text>
        <class>382</class>
        <subclass>195000</subclass>
      </further-classification>
      <further-classification sequence="7">
        <text>382203000</text>
        <class>382</class>
        <subclass>203000</subclass>
      </further-classification>
      <further-classification sequence="8">
        <text>382225000</text>
        <class>382</class>
        <subclass>225000</subclass>
      </further-classification>
      <further-classification sequence="9">
        <text>700274000</text>
        <class>700</class>
        <subclass>274000</subclass>
      </further-classification>
    </classification-national>
    <patent-classifications>
      <patent-classification sequence="1">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>G08B-017/125</classification-symbol>
        <section>G</section>
        <class>08</class>
        <subclass>B</subclass>
        <main-group>17</main-group>
        <subgroup>125</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20160519</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="2">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>F23N-005/082</classification-symbol>
        <section>F</section>
        <class>23</class>
        <subclass>N</subclass>
        <main-group>5</main-group>
        <subgroup>082</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20160519</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="3">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>F23N-2029/08</classification-symbol>
        <section>F</section>
        <class>23</class>
        <subclass>N</subclass>
        <main-group>2029</main-group>
        <subgroup>08</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20160519</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="4">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>F23N-2029/20</classification-symbol>
        <section>F</section>
        <class>23</class>
        <subclass>N</subclass>
        <main-group>2029</main-group>
        <subgroup>20</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20160519</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="5">
        <classification-scheme office="EP" scheme="ICO"/>
        <classification-symbol>R23N-029/08</classification-symbol>
      </patent-classification>
      <patent-classification sequence="6">
        <classification-scheme office="EP" scheme="ICO"/>
        <classification-symbol>R23N-029/20</classification-symbol>
      </patent-classification>
    </patent-classifications>
    <number-of-claims>20</number-of-claims>
    <exemplary-claim>1</exemplary-claim>
    <figures>
      <number-of-drawing-sheets>6</number-of-drawing-sheets>
      <number-of-figures>6</number-of-figures>
      <image-key data-format="questel">US6184792</image-key>
    </figures>
    <invention-title format="original" lang="en" id="title_en">Early fire detection method and apparatus</invention-title>
    <references-cited>
      <citation srep-phase="examiner">
        <patcit num="1">
          <text>LAYCOCK JOHN</text>
          <document-id>
            <country>US</country>
            <doc-number>5202759</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5202759</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="2">
          <text>ALLEN MARK G, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5249954</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5249954</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="3">
          <text>ISHII HIROMITSU, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5289275</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5289275</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="4">
          <text>MURAKAMI YOSHISHIGE, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5777548</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5777548</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="5">
          <text>KHESIN MARK</text>
          <document-id>
            <country>US</country>
            <doc-number>5798946</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5798946</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="6">
          <text>CHAN WILLIAM S, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5937077</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5937077</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="7">
          <text>LEMELSON JEROME H, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5971747</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5971747</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="8">
          <text>THUILLARD MARC PIERRE</text>
          <document-id>
            <country>US</country>
            <doc-number>6011464</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US6011464</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="9">
          <text>SIVATHANU YUDAYA, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>6111511</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US6111511</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="10">
          <text>GOEDEKE A DONALD, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5153722</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5153722</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="11">
          <text>INNES DAVID C K G</text>
          <document-id>
            <country>US</country>
            <doc-number>5191220</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5191220</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="12">
          <text>LASENBY JOAN</text>
          <document-id>
            <country>US</country>
            <doc-number>5510772</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5510772</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="13">
          <text>THUILLARD MARC P</text>
          <document-id>
            <country>US</country>
            <doc-number>5594421</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5594421</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="14">
          <text>HALL GREGORY H, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5625342</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5625342</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="15">
          <text>BARNES HEIDI L, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5726632</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5726632</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="16">
          <text>WERNER JUERG, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5751209</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5751209</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="17">
          <text>PANOV YURI S</text>
          <document-id>
            <country>US</country>
            <doc-number>5796342</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5796342</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="18">
          <text>PEDERSEN ROBERT D, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5832187</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5832187</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="19">
          <text>MARSDEN GARY C</text>
          <document-id>
            <country>US</country>
            <doc-number>5838242</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5838242</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="20">
          <text>SCHULER FRED</text>
          <document-id>
            <country>US</country>
            <doc-number>5850182</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5850182</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="21">
          <text>YAMAGISHI TAKATOSHI, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5926280</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5926280</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="22">
          <text>KING JOHN D, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5995008</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5995008</doc-number>
          </document-id>
        </patcit>
      </citation>
    </references-cited>
    <parties>
      <applicants>
        <applicant app-type="applicant" sequence="1">
          <addressbook lang="en">
            <name>PRIVALOV GEORGE</name>
          </addressbook>
        </applicant>
        <applicant app-type="applicant" sequence="2">
          <addressbook lang="en">
            <name>PRIVALOV DIMITRI</name>
          </addressbook>
        </applicant>
      </applicants>
      <inventors>
        <inventor data-format="original" sequence="1">
          <addressbook lang="en">
            <name>Privalov, George</name>
            <address>
              <address-1>Baltimore, MD, 21210, US</address-1>
              <city>Baltimore</city>
              <state>MD</state>
              <postcode>21210</postcode>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
        <inventor data-format="original" sequence="2">
          <addressbook lang="en">
            <name>Privalov, Dimitri</name>
            <address>
              <address-1>117334 Moscow, RU</address-1>
              <city>117334 Moscow</city>
              <country>RU</country>
            </address>
          </addressbook>
          <nationality>
            <country>RU</country>
          </nationality>
        </inventor>
      </inventors>
      <agents>
        <agent sequence="1" rep-type="agent">
          <addressbook lang="en">
            <name>Guffey, Larry J.</name>
          </addressbook>
        </agent>
      </agents>
    </parties>
    <examiners>
      <primary-examiner>
        <name>Lee, Benjamin C.</name>
      </primary-examiner>
    </examiners>
    <lgst-data>
      <lgst-status>GRANTED</lgst-status>
    </lgst-data>
  </bibliographic-data>
  <abstract format="original" lang="en" id="abstr_en">
    <p id="P-EN-00001" num="00001">
      <br/>
      The present invention provides a method and apparatus for detecting fire in a monitored area.
      <br/>
      In a preferred embodiment, this method is seen to comprise the steps of: (1) capturing video images of the monitored area in the form of two-dimensional bitmaps whose spatial resolution is determined by the number of pixels comprising the bitmaps, (2) cyclically accumulating a sequential set of these captured bitmaps for analysis of the temporal variations being experienced in the pixel brightness values, (3) examining these sets of bitmaps to identify clusters of contiguous pixels having either a specified static component or a specified dynamic component of their temporally varying brightness values, (4) comparing the patterns of the shapes of these identified, static and dynamic clusters to identify those exhibiting patterns which are similar to those exhibited by the comparable bright static core and the dynamic crown regions of flickering open flames, and (5) signaling the detection of a fire in the monitored area when the degree of match between these identified, static and dynamic clusters and the comparable regions of flickering open flames exceeds a prescribed matching threshold value.
    </p>
  </abstract>
  <description format="original" lang="en" id="desc_en">
    <heading>BACKGROUND OF THE INVENTION</heading>
    <p num="1">
      1.
      <br/>
      Field of the Invention
    </p>
    <p num="2">
      The present invention generally relates to electrical, condition responsive systems.
      <br/>
      More particularly, this invention relates to a method and apparatus for detecting a fire in a monitored area.
    </p>
    <p num="3">2. Description of the Related Art</p>
    <p num="4">
      It is important that an optical fire detector be able to detect the presence of various types of flames in as reliable a manner as possible.
      <br/>
      This requires that a flame detector be able to discriminate between flames and other light sources.
      <br/>
      Commonly, such optical flame detection is carried out in the infrared (IR) portion of the light spectrum at around 4.5 microns, a wavelength that is characteristic of an emission peak for carbon dioxide.
    </p>
    <p num="5">
      Simple flame detectors employ a single sensor, and a warning is provided whenever the signal sensed by the detectors exceeds a particular threshold value.
      <br/>
      However, this simple approach suffers from false triggering, because it is unable to discriminate between flames and other bright objects, such as incandescent light bulbs, hot industrial processes such as welding, and sometimes even sunlight and warm hands waved in front of the detector.
    </p>
    <p num="6">
      Attempts have been made to overcome this problem by sensing radiation at two or more wavelengths.
      <br/>
      For example, see U.S. Pat. No. 5,625,342. Such comparisons of the relative strengths of the signals sensed at each wavelength have been found to permit greater discrimination regarding false sources than when sensing at only a single wavelength.
      <br/>
      However, such detectors can still be subject to high rates of false alarms.
    </p>
    <p num="7">Another technique for minimizing the occurrence of such false alarms is to use flicker detection circuitry which monitors radiation intensity variations over time, and thereby discriminate between a flickering flame source and a relatively constant intensity source such as a hot object.</p>
    <p num="8">
      Meanwhile, U.S. Pat. No. 5,510,772 attempts to minimize such false fire alarms by using a camera operating in the near infrared range to capture a succession of images of the space to be monitored.
      <br/>
      The brightness or intensity of the pixels comprising these images is converted to a binary value by comparing it with the average intensity value for the image (e.g., 1 if greater than the average).
      <br/>
      Computing for each pixel a crossing frequency, v (defined as the number of times that its binary value changes divided by the II number of images captured) and an average pixel binary value, C (defined as the average over all the images for a specific pixel).
      <br/>
      Testing the values of v and C against the relationship: v=KC(1-C), where K is a constant; and signaling the existence of a fire for any cluster of adjacent pixels for which the respective values of v and C fit this relationship within predetermined limits.
    </p>
    <p num="9">
      Despite such improvement efforts, these fire detectors can still be subject to high rates of false alarms, and misdiagnosis of true fires.
      <br/>
      For example, there can still be significant difficulties in producing true alarms when monitoring fires at a long distance from the detector, say up to approximately two hundred feet, when the signal to noise ratio is small.
      <br/>
      This may present even higher challenge when other active or passive light sources are present, such as spot welding, reflecting surfaces of water, flickering luminescent light fixtures etc.
    </p>
    <p num="10">
      Also, fire detectors suffer from an inconsistency in fire detection characteristics under different fire conditions, such as with different levels of fire temperature, size, position relative to the detector, fuel and interfering background radiation.
      <br/>
      Additionally, such detectors have little ability to pinpoint the exact location of a fire in a monitored area; information which can greatly aid the effective use of installed suppression systems.
      <br/>
      Consequently, there is still a need for a fire detector with exact fire location capabilities and whose ability to detect fires is less dependent on the various factors listed above.
    </p>
    <heading>SUMMARY OF THE INVENTION</heading>
    <p num="11">The present invention is generally directed to satisfying the needs set forth above and the problems identified with prior fire detection systems and methods.</p>
    <p num="12">In accordance with one preferred embodiment of the present invention, the foregoing needs can be satisfied by providing a method for detecting fire in a monitored area that comprises the steps of. (1) capturing video images of the monitored area in the form of two-dimensional bitmaps whose spatial resolution is determined by the number of pixels comprising the bitmaps, (2) cyclically accumulating a sequential set of these captured bitmaps for analysis of the temporal variations being experienced in the pixel brightness values, (3) examining these sets of bitmaps to identify clusters of contiguous pixels having either a specified static component or a specified dynamic component of their temporally varying brightness values, (4) comparing the patterns of the shapes of these identified, static and dynamic clusters to identify those exhibiting patterns which are similar to those exhibited by the comparable bright static core and the dynamic crown regions of flickering open flames, and (5) signaling the detection of a fire in the monitored area when the degree of match between these identified, static and dynamic clusters and the comparable regions of flickering open flames exceeds a prescribed matching threshold value.</p>
    <p num="13">
      In another preferred embodiment, the present invention is seen to take the form of an apparatus for detecting a fire in a monitored area.
      <br/>
      This apparatus incorporates a CCD-based, video camera preferably operating in the near IR region of spectra with built-in video processing circuitry that is commercially available.
      <br/>
      For example, an accumulation buffer may provide the necessary storage to allow for the further digital filtering of the camera's video signal, which may be accomplished using microcontroller-based, electronic components, such as video decoders and digital signal processor (DSP) chips.
    </p>
    <p num="14">It is therefore an object of the present invention to provide a fire detection method and apparatus that minimizes the occurrences of high rates of false alarms, and the misdiagnosis of true fires.</p>
    <p num="15">It is another object of the present invention to provide a fire detection method and apparatus that can accurately monitor fires at a long distance from the detector, say up to approximately two hundred feet, when the signal to noise ratio for the prior art detectors would be small.</p>
    <p num="16">It is a yet another object of the present invention to provide a fire detection method and apparatus whose ability to detect fires is less dependent on different fire conditions, such as with different levels of fire temperature, size, position relative to the detector, fuel and interfering background radiation.</p>
    <p num="17">It is a further object of the present invention to provide a fire detection method and apparatus based on distinguishing the flickering crown and static core regions of an open flame.</p>
    <p num="18">These and other objects and advantages of the present invention will become readily apparent as the invention is better understood by reference to the accompanying drawings and the detailed description that follows.</p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading>
    <p num="19">
      FIG. 1 illustrates the various forms of data that are encountered and analyzed using a preferred embodiment of the present invention.
      <br/>
      FIG. 2 is a flow chart showing the various process steps carried out in one embodiment of the present invention.
      <br/>
      FIG. 2a illustrates a typical bitmap pattern of the present invention, where the dynamic and static component pixels have been filled, respectively, with diagonal hatching and cross hatching.
      <br/>
      FIG. 3 illustrates how data flows through the various elements comprising an embodiment of the present invention in the form of a fire detecting apparatus.
      <br/>
      FIG. 4 illustrates the details of the memory organization within a data accumulation buffer of the apparatus referenced in FIG. 3.
      <br/>
      FIG. 5 illustrates the computational, hardware architecture for the apparatus referenced in FIG. 3.
    </p>
    <heading>DESCRIPTION OF THE PREFERRED EMBODIMENT</heading>
    <p num="20">Referring now to the drawings wherein are shown preferred embodiments and wherein like reference numerals designate like elements throughout, there is shown in FIG. 2 an embodiment of the present invention in the form of a method for detecting fire in a monitored area.</p>
    <p num="21">This method is generally seen to comprise the steps of: (a) detecting and capturing, at a prescribed frequency, video images of the monitored area in the form of two-dimensional bitmaps whose spatial resolution is determined by the number of pixels comprising said bitmaps, (b) cyclically accumulating a sequential set of these captured bitmaps for analysis of the temporal variations in the brightness values observed at each of the pixels, wherein these temporal variations are expressible in terms of a static and a dynamic component of the variations in pixel brightness values, (c) examining these set of bitmaps to identify a static cluster and a dynamic cluster of contiguous pixels having brightness values that, respectively, exceed prescribed static and dynamic threshold magnitudes, (d) comparing the patterns of the shapes of said identified, static and dynamic clusters to identify those exhibiting patterns which match to a predetermined matching level those exhibited by the comparable static core and dynamic, flickering coronal regions of a turbulent, open flame, and (e) signaling the detection of a fire in the monitored area when the degree of match, between the identified, static and dynamic clusters and the comparable regions of an open flame, exceeds the predetermined matching level.</p>
    <p num="22">
      FIG. 1 further illustrates this method by generally illustrating the various forms of data that are encountered and analyzed using this method.
      <br/>
      In this embodiment, a digital video camera provides a means for detecting and capturing, at a prescribed frequency (e.g., 16 frames per second) and spatial resolution (e.g., 160 * 120 pixels), video frames or bitmap images of an area that is to be temporally monitored for the outbreak of an open flame fire.
      <br/>
      These frames, F1, F2, . . . Fi, are stored in an accumulation buffer, the storage capacity of which determines the size of the sequential data sets that are cyclically analyzed to identify the presence of an open flame (e.g., an accumulation buffer providing storage for 16 frames, with the analysis cycle being of one second duration).
    </p>
    <p num="23">
      This analysis process involves an examination of the temporal variations in the intensity or brightness at each of the pixels that comprise the respective video frames or bitmaps.
      <br/>
      These temporal variations for the various pixels may be quite complex.
      <br/>
      However, for the purpose of this analysis, it proves satisfactory to describe these variations only in terms of the amplitudes of their steady-state or static component and a specific dynamic component.
      <br/>
      This is defined to be the dynamic component that is centered around five cycles per second (i.e., 5 hertz, Hz), since this has been found to be the characteristic frequency component of the intensity fluctuations observed in the flickering, coronal regions of open, turbulent flames.
    </p>
    <p num="24">
      For the purpose of the present embodiment, these measures are computed by performing a Fast Fourier Transform (FFT) on the temporally varying, pixel intensities.
      <br/>
      The measure of the static component is taken to be the zero FFT term, (i.e., mean brightness value), while the sum of the three FFT terms centered around 5 Hz are taken as the measure of the dynamic component.
      <br/>
      However, similar end results were obtained when using Digital Signal Processing techniques with Humming windows (that is not to suggest that Humming window is the only technique possible).
      <br/>
      In addition, the dynamic component can be determined by simply counting how many times the intensity signal crosses its mean value within each analysis cycle.
    </p>
    <p num="25">Thus, an intermediate result of each cycle of this analysis are two calculated bitmaps in which each pixel is assigned the calculated values of the prescribed static and dynamic components.</p>
    <p num="26">
      The analysis continues, as shown in FIG. 2, by identifying whether any of the calculated bitmap's contiguous pixels have either static or dynamic components that exceed prescribed threshold values.
      <br/>
      If so, the extent and comparative shapes of such calculated bitmap regions, denoted as clusters, are noted for still further analysis.
    </p>
    <p num="27">
      This further analysis is predicated upon the finding that the comparative shapes of such clusters lie within clearly distinguishable bounds when such clusters are due to the existence of an open flame within a monitored area.
      <br/>
      Thus, an analysis of the comparative shapes of such clusters can be used as a means for identifying the existence of an open flame within a monitored area.
    </p>
    <p num="28">
      If the area defined by a specific cluster exceeds a prescribed magnitude, this area is copied and scaled onto a standard 12 * 12 size bitmap for specific pattern matching.
      <br/>
      FIG. 2a shows such a typical bitmap pattern for an open flame, where the dynamic component pixels have been filled with diagonal hatching while the static component pixels have been filled with cross hatching.
      <br/>
      For pattern matching, any one of a number of standard and well-known techniques may be employed.
    </p>
    <p num="29">
      For example, to calculate a degree of match, one may compute the correlation factors between each bitmap pattern (D dynamic matrix and S static matrix component) and known matrix patterns D.about. and S.about. that have been previously determined by averaging over a large sample of bitmap patterns produced by video images of real, open flame fires.
      <br/>
      Examples of such known matrix patterns for these 12 * 12 bitmaps are shown below:
    </p>
    <p num="30">
      --
      <br/>
      --      For the static component, S.about.  For the dynamic component,
      <br/>
      --  D.about.
      <br/>
      --      000000000000              005559955500
      <br/>
      --      000000000000              058999999850
      <br/>
      --      000005500000              599999999995
      <br/>
      --      000567765000              799975579997
      <br/>
      --      005678876500              799753357997
      <br/>
      --      056789987650              897530035798
      <br/>
      --      068999999860              765000000567
      <br/>
      --      068999999860              765000000567
      <br/>
      --      056789987650              765000000567
      <br/>
      --      005678876500              592000000295
      <br/>
      --      000567765000              023455554520
      <br/>
      --      000567765000              002333333200
    </p>
    <p num="31">where the matrix's values have been scaled to the range of 0-9.</p>
    <p num="32">
      The product of the two correlation factors for the dynamic and static components can then be defined as the degree of confidence, C, of the identified clusters being a fire:
      <br/>
      C=D * D.about. * S * S.about.
    </p>
    <p num="33">
      The product of this value and angular size of the original cluster, S (degree) , can then be used to determine the degree of danger that particular clusters represent in terms of being a fire during a specific analysis cycle i:
      <br/>
      Fi =C * S (degree)
    </p>
    <p num="34">
      For values F that are higher then prescribed threshold value, FIG. 2 indicates that at step 15 the analysis procedure proceeds with the initiation of a positive identification response, as shown in step 17.
      <br/>
      If the value Fi is below the threshold, but still significant, the position of the respective cluster is, as shown in step 16 of FIG. 2, compared to the results of analysis from previous cycle Fi-1.
      <br/>
      If the cluster overlaps with position of another cluster that produced Fi-1 value, the cluster is promoted, as shown at step 19 of FIG. 2 (i.e., its Fi value is increased proportionally to Fi-1 Sovl, where Sovl is the angular area of the overlap of clusters Fi and Fi-1).
      <br/>
      This insures that smaller but consistent fire clusters still produce positive identification within several analysis cycles.
    </p>
    <p num="35">This analysis cycle concludes with the storing of the attributes of identified clusters for later comparison with the attributes (e.g., cluster angular position, fire danger levels, Fi) of subsequently identified clusters.</p>
    <p num="36">
      In another embodiment, the present invention takes the form of an apparatus (1) for detecting fire in a monitored area.
      <br/>
      FIG. 3 illustrates how data flows through such an embodiment.
      <br/>
      It can be seen that the nature of these data flows and their required computational procedures may be distributed among relatively inexpensive, microcontroller-based, electronic components, such as video decoders, digital signal processor (DSP) chips and an embedded microcontroller.
      <br/>
      In one embodiment of present invention, a 330 MHz, Pentium-based, personal computer running under the Microsoft Windows operating system was used with a USB TV camera, which was manufactured by 3Com.
      <br/>
      Video capture was achieved via standard Windows multimedia services.
      <br/>
      The process algorithm shown in FIG. 2 was implemented using a Visual C++ compiler.
      <br/>
      It provided the monitoring window that displayed the video information captured by the camera.
    </p>
    <p num="37">
      FIG. 3 shows that a charge coupled device (CCD) digital video camera (10), preferably operating in the near infrared range, is used to generate a video signal in form of consecutive bitmap images that are stored in a first-in, first-out (FIFO) accumulation buffer (12) that provides the necessary storage to allow for further digital filtering of the camera's video signal.
      <br/>
      An important detail of this apparatus is the organization of the video data in the accumulation buffer (12) so that it is possible to use a standard digital signal processor (DSP) chip (14) to produce the dynamic and static components of the video image.
    </p>
    <p num="38">
      FIG. 4 illustrates the details of the memory organization within this buffer.
      <br/>
      The entire buffer memory (12) is seen to be broken into paragraphs containing as many paragraphs as there are pixels in each frame.
      <br/>
      Every paragraph contains sixteen brightness values from consecutive frames that belong to a given pixel.
    </p>
    <p num="39">
      Once the buffer is filled, the entire buffer is passed through one or more DSP chips.
      <br/>
      For simplicity, two DSP chips are shown in FIG. 4, a low-pass DSP for the static image component and a band-pass DSP for the dynamic image component.
      <br/>
      At the output of each DSP, every 16-th value in the sequence is selected and, using an internal index counter, dispatched to the address of a specific pixel position in the bitmaps.
      <br/>
      These bitmaps should be allocated in the shared memory accessible by a microcontroller (16) that is responsible for identifying the occurrence of a fire (i.e., steps 7-20 of FIG. 2) and the actuation of a fire alarm.
    </p>
    <p num="40">
      The computational hardware architecture for such an embodiment of the present invention is shown in FIG. 5.
      <br/>
      It is based on a commercially, under-development Video DSP chip (A336) from Oxford Micro Devices, Inc.
      <br/>
      Such a chip incorporates a powerful parallel arithmetic unit optimized for image processing and a standard scalar processor.
      <br/>
      In addition, it includes 512K of fast, on-chip RAM and a DMA port that directly interfaces with a CCD image sensor.
      <br/>
      The control software can be loaded at startup, via a ROM/Packet DMA port, from programmed external EEPROM.
      <br/>
      Activation of fire alarm and fire suppression systems can be achieved via built-in RS232 or other interfaces.
    </p>
    <p num="41">
      This parallel arithmetic unit will be able to perform DSP filtering to separate the static and dynamic component of images having resolutions of up to 640 * 480 pixels.
      <br/>
      The clusters can be identified and analyzed in accordance to the algorithm of FIG. 2 using the scalar processor of the A336 chip.
      <br/>
      In case of the positive identification of an open flame, a signal will be issued via one of the standard interfaces, such as RS232, to a fire suppression controller, which in turn can activate fire extinguishers and/or other possible fire-response hardware.
    </p>
    <p num="42">
      Although the foregoing disclosure relates to preferred embodiments of the present invention, it is understood that these details have been given for the purposes of clarification only.
      <br/>
      Various changes and modifications of the invention will be apparent, to one having ordinary skill in the art, without departing from the spirit and scope of the invention as hereinafter set forth in the claims.
    </p>
  </description>
  <claims format="original" lang="en" id="claim_en">
    <claim num="1">
      <claim-text>We claim:</claim-text>
      <claim-text>1.</claim-text>
      <claim-text>A method of detecting fire in a monitored area, said method comprising the steps of:</claim-text>
      <claim-text>detecting and capturing, at a prescribed frequency, video images of said monitored area in the form of two-dimensional bitmaps whose spatial resolution is determined by the number of pixels comprising said bitmaps, cyclically accumulating a sequential set of said captured bitmaps for analysis of the temporal variations in the brightness values observed at each of said pixels, said temporal variations being expressible in terms of a static and a dynamic component of said variations in pixel brightness values, examining said set of bitmaps to identify a static cluster of contiguous pixels having a static component of said brightness values that exceed a prescribed static threshold magnitude, examining said set of bitmaps to identify a dynamic cluster of contiguous pixels having a dynamic component of said brightness values that exceed a prescribed dynamic threshold magnitude, and comparing the patterns of the shapes of said identified, static and dynamic clusters to identify those exhibiting patterns which match to a predetermined matching level those exhibited by the comparable static and dynamic regions of the type of fire for which said area is being monitored.</claim-text>
    </claim>
    <claim num="2">
      <claim-text>2. A method of detecting fire as recited in claim 1, wherein said dynamic component is chosen as the magnitude of the brightness values being experienced at a frequency that is approximately equal to that of the main frequency exhibited in the turbulent flickering, coronal region of an open flame.</claim-text>
    </claim>
    <claim num="3">
      <claim-text>3. A method of detecting fire as recited in claim 2, further comprising the step of: signaling the detection of a fire in said monitored area when the degree of match, between said identified, static and dynamic clusters and said comparable regions of the type of fire for which said area is being monitored, exceeds said predetermined matching level, wherein said identified, static and dynamic clusters are compared with the patterns exhibited by the comparable bright, static core and the dynamic coronal regions of flickering open flames.</claim-text>
    </claim>
    <claim num="4">
      <claim-text>4. A method of detecting fire as recited in claim 1, further comprising the step of signaling the detection of a fire in said monitored area when the degree of match, between said identified, static and dynamic clusters and said comparable regions of the type of fire for which said area is being monitored, exceeds said predetermined matching level.</claim-text>
    </claim>
    <claim num="5">
      <claim-text>5. A method of detecting fire as recited in claim 4, wherein said matching comprises the steps of scaling said patterns to a bitmap having a specified area, and processing said scaled bitmaps with a Neural network, pattern recognition algorithm to determine said level of matching.</claim-text>
    </claim>
    <claim num="6">
      <claim-text>6. A method of detecting fire as recited in claim 4, wherein said video images being formed by a plurality of video sensors operating in a spectral range that is characteristic of the type of fire for which said area is being monitored.</claim-text>
    </claim>
    <claim num="7">
      <claim-text>7. A method of detecting fire as recited in claim 4, wherein said signaling includes information regarding the severity of said fire and its position within said monitored area based on the geometric size and position of said clusters within said bitmaps.</claim-text>
    </claim>
    <claim num="8">
      <claim-text>8. A method of detecting fire as recited in claim 5, wherein said signaling includes information regarding the severity of said fire and its position within said monitored area based on the geometric size and position of said clusters within said bitmaps.</claim-text>
    </claim>
    <claim num="9">
      <claim-text>9. A method of detecting fire as recited in claim 1, wherein said matching comprises the steps of: scaling said patterns to a bitmap having a specified area, and processing said scaled bitmaps with a Neural network, pattern recognition algorithm to determine said level of matching.</claim-text>
    </claim>
    <claim num="10">
      <claim-text>10. A method of detecting fire as recited in claim 1, wherein said video images being formed by a plurality of video sensors operating in a spectral range that is characteristic of the type of fire for which said area is being monitored.</claim-text>
    </claim>
    <claim num="11">
      <claim-text>11. An apparatus for detecting fire in a monitored area, said apparatus comprising: means for detecting and capturing, at a prescribed frequency, video images of said monitored area in the form of two-dimensional bitmaps whose spatial resolution is determined by the number of pixels comprising said bitmaps, means for cyclically accumulating a sequential set of said captured bitmaps for analysis of the temporal variations in the brightness values observed at each of said pixels, said temporal variations being expressible in terms of a static and a dynamic component of said variations in pixel brightness values, means for examining said set of bitmaps to identify a static cluster of contiguous pixels having a static component of said brightness values that exceed a prescribed static threshold magnitude, means for examining said set of bitmaps to identify a dynamic cluster of contiguous pixels having a dynamic component of said brightness values that exceed a prescribed dynamic threshold magnitude, and means for comparing the patterns of the shapes of said identified, static and dynamic clusters to identify those exhibiting patterns which match to a predetermined matching level those exhibited by the comparable static and dynamic regions of the type of fire for which said area is being monitored.</claim-text>
    </claim>
    <claim num="12">
      <claim-text>12. An apparatus for detecting fire as recited in claim 11, wherein said dynamic component is chosen as the magnitude of the brightness values being experienced at a frequency that is approximately equal to that of the main frequency exhibited in the turbulent flickering, coronal region of an open flame.</claim-text>
    </claim>
    <claim num="13">
      <claim-text>13. An apparatus for detecting fire as recited in claim 12, further comprising: means for signaling the detection of a fire in said monitored area when the degree of match, between said identified, static and dynamic clusters and said comparable regions of the type of fire for which said area is being monitored, exceeds said predetermined matching level, wherein said identified, static and dynamic clusters are compared with the patterns exhibited by the comparable bright, static core and the dynamic coronal regions of flickering open flames.</claim-text>
    </claim>
    <claim num="14">
      <claim-text>14. An apparatus for detecting fire as recited in claim 11, further comprising: means for signaling the detection of a fire in said monitored area when the degree of match, between said identified, static and dynamic clusters and said comparable regions of the type of fire for which said area is being monitored, exceeds said predetermined matching level.</claim-text>
    </claim>
    <claim num="15">
      <claim-text>15. An apparatus for detecting fire as recited in claim 14, wherein said matching comprises the steps of: scaling said patterns to a bitmap having a specified area, and processing said scaled bitmaps with a Neural network, pattern recognition algorithm to determine said level of matching.</claim-text>
    </claim>
    <claim num="16">
      <claim-text>16. An apparatus for detecting fire as recited in claim 14, wherein said video images being formed by a plurality of video sensors operating in a spectral range that is characteristic of the type of fire for which said area is being monitored.</claim-text>
    </claim>
    <claim num="17">
      <claim-text>17. An apparatus for detecting fire as recited in claim 14, wherein said signaling includes information regarding the severity of said fire and its position within said monitored area based on the geometric size and position of said clusters within said bitmaps.</claim-text>
    </claim>
    <claim num="18">
      <claim-text>18. An apparatus for detecting fire as recited in claim 15, wherein said signaling 8 includes information regarding the severity of said fire and its position within said monitored area based on the geometric size and position of said clusters within said bitmaps.</claim-text>
    </claim>
    <claim num="19">
      <claim-text>19. An apparatus for detecting fire as recited in claim 11, wherein said matching comprises the steps of: scaling said patterns to a bitmap having a specified area, and processing said scaled bitmaps with a Neural network, pattern recognition algorithm to determine said level of matching.</claim-text>
    </claim>
    <claim num="20">
      <claim-text>20. An apparatus for detecting fire as recited in claim 11, wherein said video images being formed by a plurality of video sensors operating in a spectral range that is characteristic of the type of fire for which said area is being monitored.</claim-text>
    </claim>
  </claims>
</questel-patent-document>