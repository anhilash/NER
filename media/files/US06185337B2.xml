<?xml version="1.0" encoding="UTF-8"?>
<questel-patent-document lang="en" date-produced="20180805" produced-by="Questel" schema-version="3.23" file="US06185337B2.xml">
  <bibliographic-data lang="en">
    <publication-reference publ-desc="Granted patent as second publication">
      <document-id>
        <country>US</country>
        <doc-number>06185337</doc-number>
        <kind>B2</kind>
        <date>20010206</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>US6185337</doc-number>
      </document-id>
    </publication-reference>
    <original-publication-kind>B2</original-publication-kind>
    <application-reference family-id="18301797" extended-family-id="13727503">
      <document-id>
        <country>US</country>
        <doc-number>08989416</doc-number>
        <kind>A</kind>
        <date>19971212</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>1997US-08989416</doc-number>
      </document-id>
      <document-id data-format="questel_Uid">
        <doc-number>14023620</doc-number>
      </document-id>
    </application-reference>
    <language-of-filing>en</language-of-filing>
    <language-of-publication>en</language-of-publication>
    <priority-claims>
      <priority-claim kind="national" sequence="1">
        <country>JP</country>
        <doc-number>33668996</doc-number>
        <kind>A</kind>
        <date>19961217</date>
        <priority-active-indicator>Y</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="1">
        <doc-number>1996JP-0336689</doc-number>
      </priority-claim>
    </priority-claims>
    <dates-of-public-availability>
      <publication-of-grant-date>
        <date>20010206</date>
      </publication-of-grant-date>
    </dates-of-public-availability>
    <classifications-ipcr>
      <classification-ipcr sequence="1">
        <text>G06K   9/00        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>06</class>
        <subclass>K</subclass>
        <main-group>9</main-group>
        <subgroup>00</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
    </classifications-ipcr>
    <classification-national>
      <country>US</country>
      <main-classification>
        <text>382227000</text>
        <class>382</class>
        <subclass>227000</subclass>
      </main-classification>
      <further-classification sequence="1">
        <text>382206000</text>
        <class>382</class>
        <subclass>206000</subclass>
      </further-classification>
    </classification-national>
    <classifications-ecla>
      <classification-ecla sequence="1">
        <text>G06K-009/00F</text>
        <section>G</section>
        <class>06</class>
        <subclass>K</subclass>
        <main-group>009</main-group>
        <subgroup>00F</subgroup>
      </classification-ecla>
    </classifications-ecla>
    <patent-classifications>
      <patent-classification sequence="1">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>G06K-009/00221</classification-symbol>
        <section>G</section>
        <class>06</class>
        <subclass>K</subclass>
        <main-group>9</main-group>
        <subgroup>00221</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
    </patent-classifications>
    <number-of-claims>13</number-of-claims>
    <exemplary-claim>1</exemplary-claim>
    <figures>
      <number-of-drawing-sheets>5</number-of-drawing-sheets>
      <number-of-figures>5</number-of-figures>
      <image-key data-format="questel">US6185337</image-key>
    </figures>
    <invention-title format="original" lang="en" id="title_en">System and method for image recognition</invention-title>
    <references-cited>
      <citation srep-phase="examiner">
        <patcit num="1">
          <text>FUKUSHIMA KUNIHIKO</text>
          <document-id>
            <country>US</country>
            <doc-number>5058184</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5058184</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="2">
          <text>LU DAOZHENG, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5331544</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5331544</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="3">
          <text>LECUN YANN A, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5337372</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5337372</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="4">
          <text>LU DAOZHENG, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5432864</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5432864</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="5">
          <text>SADOVNIK LEV S, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5497430</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5497430</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="6">
          <text>LU DAOZHENG, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5550928</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5550928</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="7">
          <text>KINJO NAOTO</text>
          <document-id>
            <country>US</country>
            <doc-number>5629752</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5629752</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="8">
          <text>MOGHADDAM BABACK, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5710833</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5710833</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="9">
          <text>SHACKLETON MARK ANDREW, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5719951</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5719951</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="10">
          <text>SOUMA MASAKI, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5901244</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5901244</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="11">
          <text>TURK MATTHEW, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5164992</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5164992</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="12">
          <text>HUANG HSIN-HAO, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5566273</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5566273</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="13">
          <text>KOERNER EDGAR, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5675663</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5675663</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="14">
          <text>HONDA MOTOR CO LTD</text>
          <document-id>
            <country>EP</country>
            <doc-number>0733989</doc-number>
            <kind>A2</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>EP-733989</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <nplcit num="1">
          <text>Pentland et al., "View-based and modular eignespaces for face recognition", Proceedings CVPR '94, Jun. 23, 1994.</text>
        </nplcit>
      </citation>
      <citation srep-phase="examiner">
        <nplcit num="2">
          <text>Turk et al., "Face recognition using eigenfaces", Proceedings CVPR '91, Jun. 6, 1991.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="3">
          <text>Koerner et al., "A Cortical-type Modular Neural Network for Hypothetical Reasoning," Neural Networks, vol. 10, No. 5, pp. 791-814, 1997.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="4">
          <text>Olshausen, B.A. et al., "A Neurobiological Model of Visual Attention and Invariant Pattern Recognition based on Dynamic Routing of Information," Journal of Neuroscience, 13(11), Nov. 11, 1993, pp. 4700-4719.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="5">
          <text>Baron, R.J., "Mechanisms of Human Facial Recognition," International Journal of Man-Machine Studies, vol. 15, No. 2, Aug. 1981, pp. 137-178.</text>
        </nplcit>
      </citation>
    </references-cited>
    <parties>
      <applicants>
        <applicant data-format="original" app-type="applicant" sequence="1">
          <addressbook lang="en">
            <orgname>Honda Giken Kogyo Kabushiki Kaisha</orgname>
            <address>
              <address-1>Tokyo, JP</address-1>
              <city>Tokyo</city>
              <country>JP</country>
            </address>
          </addressbook>
          <nationality>
            <country>JP</country>
          </nationality>
        </applicant>
        <applicant data-format="questel" app-type="applicant" sequence="2">
          <addressbook lang="en">
            <orgname>HONDA MOTOR</orgname>
          </addressbook>
          <nationality>
            <country>JP</country>
          </nationality>
        </applicant>
      </applicants>
      <inventors>
        <inventor data-format="original" sequence="1">
          <addressbook lang="en">
            <name>Tsujino, Hiroshi</name>
            <address>
              <address-1>Wako, JP</address-1>
              <city>Wako</city>
              <country>JP</country>
            </address>
          </addressbook>
          <nationality>
            <country>JP</country>
          </nationality>
        </inventor>
        <inventor data-format="original" sequence="2">
          <addressbook lang="en">
            <name>Koerner, Edgar</name>
            <address>
              <address-1>Wako, JP</address-1>
              <city>Wako</city>
              <country>JP</country>
            </address>
          </addressbook>
          <nationality>
            <country>JP</country>
          </nationality>
        </inventor>
        <inventor data-format="original" sequence="3">
          <addressbook lang="en">
            <name>Masutani, Tomohiko</name>
            <address>
              <address-1>Wako, JP</address-1>
              <city>Wako</city>
              <country>JP</country>
            </address>
          </addressbook>
          <nationality>
            <country>JP</country>
          </nationality>
        </inventor>
      </inventors>
      <agents>
        <agent sequence="1" rep-type="agent">
          <addressbook lang="en">
            <name>Guss, Paul A.</name>
          </addressbook>
        </agent>
      </agents>
    </parties>
    <examiners>
      <primary-examiner>
        <name>Au, Amelia</name>
      </primary-examiner>
    </examiners>
    <lgst-data>
      <lgst-status>EXPIRED</lgst-status>
    </lgst-data>
  </bibliographic-data>
  <abstract format="original" lang="en" id="abstr_en">
    <p id="P-EN-00001" num="00001">
      <br/>
      In an artificial intelligence system for image recognition, a global image of an object is input from a camera or other optical pick-up device, and is processed in a global image processing means, which performs analytical processing on the global image by extracting global characteristics of the input image and evaluating consistency of the extracted global characteristics.
      <br/>
      Simultaneously, the image data is processed in a local image processing means which undertakes analytical processing on a plurality of local images defining local portions of the image to be recognized.
      <br/>
      The local image processing means is constructed by plural modules, each further defined by sub-modules, which conduct respective analyses corresponding to local images having characteristics useful in recognizing the global image, wherein each local processor extracts characteristics of an input local image and evaluates consistency of the extracted characteristic with the object to be recognized.
      <br/>
      Importantly, the global image processing means receives inputs from the local modules, and deactivates functions of local modules which are inconsistent with the global characteristics, while activating and promoting functions of local modules which are consistent with the global characteristics.
      <br/>
      Through top-down control from the global image processor, as well as inter-module signals between respective local processing modules, since inconsistent processes are quickly discovered,
    </p>
  </abstract>
  <description format="original" lang="en" id="desc_en">
    <heading>BACKGROUND OF THE INVENTION</heading>
    <p num="1">
      1.
      <br/>
      Field of the Invention
    </p>
    <p num="2">
      The present invention relates to artificial intelligence systems for image recognition.
      <br/>
      More specifically, the present invention is an image information processing system in which, based on a hypothesis regarding the basic scheme of visual information processing which approximates an actual biological vision system, various image information input through a camera or similar optical device is quantified, and subjected to calculational processing in a computer for enabling recognition of objects in the image.
      <br/>
      The system further employs automatic self-learning of object models for efficiently recognizing images.
    </p>
    <p num="3">2. Description of the Related Art</p>
    <p num="4">
      As the physiological functions of biological image recognition become further elucidated in the prior art, models have been constructed using computers and the like for approximating such recognition functions, and efforts have come about to construct artificial intelligence based visual learning systems.
      <br/>
      In such visual learning systems, a visual scene which is input via a video camera, for example, is expressed as a numerical value, and based on analysis thereof, an image object from within the visual scene is recognized, specified and/or classified.
      <br/>
      More specifically, in such systems, analytical processing is undertaken for recognizing correspondence between an input image pattern and an image pattern of a recognized object accumulated through learning.
    </p>
    <p num="5">
      The input scene is converted into a numerical value, such as a voltage value, which corresponds to the intensity of pixels making up the image, and expressed as a vector.
      <br/>
      For example, in the case of an image size occupying a 27 * 27 pixel region, a 729 dimensional vector is expressed in the form of a variance (dispersion) in an orthogonal axial space.
      <br/>
      As such, analytical processing of such a large amount of data, and distinguishing a target image pattern therefrom, is nearly impossible even with the capabilities of present computers.
    </p>
    <p num="6">Accordingly, to facilitate this analysis, there exists a demand for processing to be performed for converting the input image pattern which is the object of recognition into compressed data expressing the characteristics thereof, whereby comparison with accumulated learned patterns can then be relatively easily undertaken.</p>
    <p num="7">For efficiently analyzing the data, it is desirable to compartmentalize the data space into a so-called subspace, so as to limit the data space to its most characteristic regions.</p>
    <p num="8">
      A known method for satisfying this demand is principal component analysis (PCA).
      <br/>
      According to this method, the distribution of the object image data of a multidimensional image space is converted into feature space, and the principal components of eigenvectors which serve to characterize such space are used.
      <br/>
      More specifically, the eigenvectors are caused respectively by the amount of change in pixel intensity corresponding to changes within the image group, and can thus be thought of as characteristic axes for explaining the image.
    </p>
    <p num="9">
      The respective vectors corresponding to the object image include those which contribute greatly to the eigenvectors as well as those which do not contribute so much.
      <br/>
      The object image is caused by a large change of the image group, and can be closely expressed, for example, based on the collection of principal components of eigenvectors having large eigenvalues.
    </p>
    <p num="10">
      Stated in different terms, in order to very accurately reproduce a target image, a large number of eigenvectors are required.
      <br/>
      However, if one only desires to express the characteristics of the outward appearance of an object image, it can be sufficiently expressed using a smaller number of eigenvectors.
      <br/>
      A system utilizing the above-described eigenspace method, for recognizing human faces, is disclosed by U.S. Pat. No. 5,164,992, the disclosure of which is explicitly incorporated into the present specification by reference.
      <br/>
      This technology shall now be summarized below.
    </p>
    <p num="11">
      First, the facial images of a plurality of previously known persons are learned.
      <br/>
      Letting N be the number of pixels making up the facial image, M facial images are then expressed by respective vectors  GAMMA 1,  GAMMA 2,  GAMMA 3 . . .  GAMMA n each of length N2.
    </p>
    <p num="12">
      Taking the difference between the vector of each person's face and the average value ( PHI i = GAMMA i -average vector), this results in M vector groups.
      <br/>
      If a vector group A is defined by A=( PHI i . . .  PHI M), by calculating a vector  UPSILON k and a scalar quantity  LAMBDA k as eigenvectors and eigenvalues, respectively, of the covariant matrix C=AAT of A, an eigenspace of the face is determined.
    </p>
    <p num="13">
      In the case of an image made up of N * N pixels, the matrix C has N2 eigenvectors and eigenvalues.
      <br/>
      However, when the facial data amount M is less than the N2 dimensions of the overall image space (i.e. M&lt;&lt;N2), which includes not only facial data but background data as well, in order to recognize the facial image it is acceptable to calculate only the eigenvectors of an M * M dimensional matrix AT A. The vector space ut =A UPSILON i can be determined from the M eigenvectors  UPSILON i of the matrix L.
    </p>
    <p num="14">Hence, the data according to the above analysis is compressed whereby the number of required calculations is reduced considerably.</p>
    <p num="15">
      The input facial image is converted into the components of facial space (i.e. projected into an eigenspace of the face) through a simple operation, as follows,
      <br/>
      OMEGA k =ukT ( GAMMA - PSI )
      <br/>
      PSI : Average Vector
    </p>
    <p num="16">which is conducted in an image processing apparatus.</p>
    <p num="17">
      Secondly, a vector  OMEGA T =( OMEGA 1,  OMEGA 2 . . .  OMEGA M) expresses as a weighting the degree at which each facial eigenspace contributes to the input image pattern.
      <br/>
      The vector  OMEGA  is utilized as a standard pattern recognition.
    </p>
    <p num="18">
      The Euclidean distance  XI  between the input image  PHI = GAMMA - PSI  and the facial eigenspace  PHI f defined by equation (1) is determined from equation (2), both equations being shown below.
      <br/>
      If  XI  is within a given threshold value, the input image is recognized as belonging to  PHI f.  (Equation image '1' not included in text)
    </p>
    <p num="19">
      Stated otherwise, from within an overall image scene, by determining a vector which best evaluates the distribution of the facial images therein, the data can be limited to the partial space of the facial image.
      <br/>
      Accordingly, the amount of data is considerably reduced, and one is able to focus on a single set of data which is limited to that making up the facial characteristics.
    </p>
    <p num="20">
      Once the evaluation vector has been determined, the input images can be classified as having faces therein or not, and if it is judged that a face is present, a particular individual's face can be recognized by comparison with the accumulated data of facial patterns from previously known individuals.
      <br/>
      Turk et al., the inventors in the above-identified U.S. Pat. No., performed principal component analysis on learned images of 128 facial images, and in a facial recognition test undertaken in actual practice using 20 essential eigenvectors, the inventors were able to achieve a 95% rate of recognition with respect to 200 facial images.
    </p>
    <p num="21">
      The eigenspace method of image recognition is more effective than standard recognition techniques using template matching or standardized correlation relationships.
      <br/>
      However, in the case of images expressed by high multidimensional vectors, the parts of image features which are not explained well must be surmised, and if there are no inferential techniques for omitting the image processing calculations, it then becomes necessary to perform expanded calculations concerning all vectors, which is impossible in actual practice.
    </p>
    <p num="22">
      Additionally, the structural descriptions of knowledge concerning image information using only the eigenspace method are complex, and it is problematic when adapted to understanding of images in general.
      <br/>
      When applied to recognition of images which exist in reality, methods have to be established for correcting the mistaken processing results which invariably occur.
      <br/>
      Accordingly, new systems logic is indispensable for expanding the applicability of the eigenspace method to various kinds of image recognition.
    </p>
    <heading>SUMMARY OF THE INVENTION</heading>
    <p num="23">
      The present invention has the object of providing an image recognition system which is suitable for performing recognition of object images, including a global image processor for performing image information processing of a global image of an object of recognition which is input via an optical means, and a local image processor for performing image information processing of a local image, which is a partial image expressing the characteristics of the global image, wherein each of the aforementioned image processors has functions for extracting characteristics of the input image and for evaluating the consistency (matching) of the image.
      <br/>
      Based on the evaluation of such consistency, the processing functions of other image information processing devices are activated or deactivated, whereby recognition of the object image is performed by developing and discerning authentic image recognition from out of non-authentic image recognition.
    </p>
    <p num="24">A further object is to provide a system capable of reducing the computational burden on a computer, as well as enabling image recognition in a shorter time than capable with presently known image recognition system, by employing a model of the visual information processing mechanisms of the cerebral cortex of biological beings, as described in "A processing control scheme derived from a bottom up flow of image information in which final recognized images are arrived at from recognition of simple forms, and a top down flow of image information in which, conversely, simple initial form recognition is reached from final recognized images." (Koerner, Tsujino, Masutani, "A Cortical-Type Modular Network for Hypothetical Reasoning," Neural Network, vol. 10, no. 5, pp. 791-814, 1997.) The disclosure of this document is expressly incorporated by reference into the present specification.</p>
    <p num="25">A still further object of the present invention is to provide an image recognition system employing a system in which a global image, which is the object of recognition, and a characteristic local image making up a portion of the object are subjected to parallel image processing, and automatically compared while deactivating inconsistent assumptions, and activating assumptions which are consistent, wherein by means of self-learning, the degree of certainty in recognizing images is improved.</p>
    <p num="26">
      In order to achieve the above-stated objects, the image recognition system of the present invention provides a system for performing image recognition of an object from a global image of the object and characteristic local images, and is constructed by a global image processor for conducting analytical processing on the global image, and a local image processor for conducting analytical processing on the local images.
      <br/>
      Each of the image processors has functions for extracting characteristics of an input image, as well as evaluating consistency between the extracted characteristics and the image which is to be recognized.
      <br/>
      The global image processor is characterized by receiving an input from the local image processor, wherein the functions of the local image processor which are inconsistent with the global characteristics are deactivated, and the functions of the local image processor which are consistent with the global characteristics are activated.
    </p>
    <p num="27">The image recognition system of the present invention if further characterized by a local image processor made up of a local module comprising three sub-modules, wherein the first sub-module has a function for extracting characteristics of the local image, the second sub-module contains knowledge of the eigenspace of the local image, and the third sub-module has a function for evaluating consistency.</p>
    <p num="28">The local module of the local image processor is further characterized wherein each characteristic locality corresponds to and is arranged with an input topology.</p>
    <p num="29">The local image processor further comprises plural local modules in which the characteristic localities thereof are the same, wherein the functions of each local module are mutually activated and controlled by a control signal based on evaluations performed in each local module.</p>
    <p num="30">The local image processor further comprises a plurality of local modules having differing characteristic localities, wherein the functions of each local module are mutually activated and controlled by a control signal based on evaluations performed in each local module.</p>
    <p num="31">The image recognition system of the present invention is further characterized by a global image processor which is made up of a module comprising three sub-modules, wherein the first sub-module has a function for extracting characteristics of the global object image, the second sub-module contains knowledge of the eigenspace, and the third sub-module has a function for evaluating consistency.</p>
    <p num="32">The image recognition system of the present invention is further characterized wherein the global image processor receives an input from the local image processor, and based thereon the functions of local modules which are inconsistent with the global characteristics are deactivated, and further comprising means for outputting control signals for activating the functions of the local image processor which are consistent with the global characteristics.</p>
    <p num="33">
      The image recognition system of the present invention is one in which recognition of an object is performed based on recognition of a global image of the object, together with recognition of characteristic local images making up part of the global image.
      <br/>
      Accordingly, in this system, accurate image recognition can be performed, and plural local modules have the capability of performing sequence-seeking operations in parallel.
      <br/>
      Also, based on consistency with global characteristics in the global image processor, and deactivation signals based on the evaluations of the local image processor, functions of the local image processor which are determined to be inconsistent are quickly deactivated, so that the time required for reaching final image recognition can be shortened, and the burden on the computer can be reduced.
    </p>
    <p num="34">
      Further, in the image processing system of the present invention, simultaneously with initiating analysis of local image processing in the local image processor, the global image processor likewise receives an input global image and undertakes analytical processing thereof.
      <br/>
      As the analytical processing in the global image processor progresses swiftly, it becomes possible to control the functions of each sub-module of the local image processor.
      <br/>
      Further, based on the evaluation result, in order to further increase the degree of accuracy of recognition, each sub-module of the local module is activated, and can be confirmed from the evaluation result.
    </p>
    <p num="35">
      The feature extraction function of the local module making up the local image processor in the present invention can be based on an average vector of learned local images of the object and/or principal component analysis (PCA).
      <br/>
      For initial analytical processing, features can be extracted in the form of simple shapes, outlines or blobs.
    </p>
    <p num="36">
      The functions for data processing of images in the present invention are shared between three sub-modules, and the apparatus is structured such that the functions are deactivated or activated at the level of each sub-module.
      <br/>
      Thus, unnecessary calculations are suppressed in a short time period, enabling a sequential-seeking image recognition method to be effected.
    </p>
    <p num="37">Further, the functions of each of the sub-modules effect computational processing based on the eigenspace method, thus providing the capability for calculations from compressed data and lessening computational burden on the computer.</p>
    <p num="38">
      In the present invention, local modules which together correspond to locations having similar forms or in which the input topology thereof are close to each other, are grouped together as neighboring local modules.
      <br/>
      Further, with respect to an input local image, only those local modules of a given local module group are simultaneously operated, thereby enabling rapid image recognition while lessening computational load.
    </p>
    <p num="39">The above and other objects, features, and advantages of the visual recognition system of the present invention will become apparent from the following description when taken in conjunction with the accompanying drawings which illustrate preferred embodiments of the present invention by way of example.</p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading>
    <p num="40">
      FIG. 1 is a view for describing the overall system structure of the present invention.
      <br/>
      FIG. 2 is a view specifically showing control signals input/output by the sub-modules making up the local module, and further explaining the higher order module (AIT) and lower order module (input/output control means).
      <br/>
      FIG. 3 illustrates overall image processing flow in the local module.
      <br/>
      FIG. 4 illustrates image analysis processing flow of the overall system.
      <br/>
      FIG. 5 and FIG. 6 are both views for showing a simulation result of the present invention, performed using a human face as an object for image recognition.
    </p>
    <heading>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
    <p num="41">
      As shown in FIG. 1, visual image information containing an image of an object to be recognized is input via a suitable optical means, such as video camera 10, and is stored digitally as a pixel frame according to known prior art techniques.
      <br/>
      A controller 12 is attached to the camera 10, so that the camera can be considered as having functions analogous to the functioning of the human eye, wherein a focus thereof is adjusted on a portion of the image, and the camera 10 is operated so as to be separately directed to a specified fixed portion only of the image.
    </p>
    <p num="42">
      In order to selectively limit the image of the object to be recognized from the overall image information, the input image information is processed in a pre-attentive processing means 13.
      <br/>
      The pre-attentive processing means 13 can comprise means for conducting filtering, for example a low transmission filter or color filter, or depending on the circumstances, can include motion detection means for specifying an object from inside the overall image, or other processing means for normalizing the scale of the object.
    </p>
    <p num="43">The pre-attentive processing means further includes means for conducting processing for normalizing contrast to compensate for changes in the input image brought about by changes in illumination during camera input, or to compensate for changes in the input image brought about by linear response characteristics of an employed CCD camera.</p>
    <p num="44">
      Although not shown in the figures, the overall structure of the system additionally comprises a graphics or image processing board, along with a keyboard and/or mouse, for enabling user access to the system.
      <br/>
      Such devices, and their use, of course are well known in the relevant technical field.
    </p>
    <p num="45">Image information, which is pre-processed and digitized as described above, is input into a memory area 16 of a frontal module 11, having therein a memory means, as well as processing means for conducting principal component analysis.</p>
    <p num="46">The image information input to the memory of the frontal module 11 can be limited to image information of the object to be recognized, as limited by pre-processing, or if the background makes up important information for recognizing the object, the overall image information may also be input.</p>
    <p num="47">By means of operational commands, the image information input to the memory means of the frontal module 11 is delivered to an analysis system, wherein the analysis system, as conceptually illustrated in FIG. 1, is made up of two subsystems 14 and 15, which shall be explained in further detail below.</p>
    <p num="48">
      The first subsystem is a local image processing means 15 (hereinafter abbreviated as the PIT, in reference to the Posterior Inferior Temporal area of the human brain), and the second subsystem is a global image procession means 14 (hereinafter abbreviated as the AIT, in reference to the Anterior Inferior Temporal area of the human brain).
      <br/>
      The AIT and PIT are respective processing areas making up part of the visual pathways responsible for visual perception in the human brain.
      <br/>
      The relation of the AIT and PIT to biological visual processing is further explained in U.S. Pat. No. 5,675,663 by Koerner et al., the full disclosure of which is also explicitly incorporated into the present specification by reference.
    </p>
    <p num="49">
      The PIT subsystem 15 is responsible for analytical processing of localized images (local icons) which make up portions of the image to be recognized, and comprises plural modules for conducting analytical processing corresponding to local image possessing characteristics useful in recognizing the overall image (global icon).
      <br/>
      These analytical processing modules are hereinafter referred to as local modules 21.
    </p>
    <p num="50">The local modules 21 are configured for correspondence with a local topology with respect to the global image, wherein topology-corresponding image information from the memory means of the frontal module is delivered to respective corresponding local modules 21.</p>
    <p num="51">
      It is preferable that the image information input to the local modules 21 be extremely compressed information due to PCA processing, and the like, in the frontal module 11.
      <br/>
      More specifically, the information input to each local module 21 is information representing simple forms or blobs, wherein based on such information, all or specified neighboring local modules 21 extract characteristics of the input information simultaneously in parallel.
      <br/>
      The extracted characteristics associate modules 21 which are most closely consistent with modules 21 having specified learned images, and further, work in concert with the AIT subsystem 14, wherein ultimately a consistent local image is specified.
    </p>
    <p num="52">
      The AIT subsystem 14 is input with global image information from the frontal module 11, and extracts global characteristics of the global image to be recognized, performing analytical processing for evaluating consistency thereof.
      <br/>
      The AIT subsystem 14 is further input with local images from the PIT subsystem 15, and evaluates consistency thereof with the global image, thereby ultimately performing recognition of the input image from all characteristics which are mutually consistent.
    </p>
    <p num="53">For example, if the object of recognition from within the input camera scene is a face or an automobile, in the case of a face a locus is defined by a limited part from inside the face, for example the eyes, nose, mouth, or facial outline can be taken as a locus, and in the case of an automobile a front panel, side shape or wheel shape can be so defined.</p>
    <p num="54">In the present invention, based on the above-described system structure, at the initiation of image processing, feature information of input images generated from image information of simple forms or blobs which are not yet fully evaluated are presumed by hypothesis, wherein by means of multi-agent calculations which gradually form an interpretation of overall consistency in an analog manner, between modules 21 and/or during communication with the AIT subsystem 14, image recognition is performed based on hypothetical deductions.</p>
    <p num="55">The PIT subsystem 15 is made up of an arrangement of plural local modules 21 corresponding to locations of the image to be recognized, wherein each local module 21 is arranged under a condition in which the position of its concerned local image region is associated with an input topology as a principal measurement thereof.</p>
    <p num="56">
      The AIT subsystem 14 is constructed as a module for processing the global image.
      <br/>
      Because it handles global characteristics of the image, there is no relationship with input topology as in the case of the PIT subsystem 15.
    </p>
    <p num="57">
      The structure and functions of each local module 21 of the PIT subsystem 15 shall now be explained with reference to FIG. 2.
      <br/>
      In order to facilitate explanation, elements of the system, along with other relevant terminology, shall be defined as follows.
    </p>
    <p num="58">
      Each local module 21 is made up from three sub-modules R0, R1and R2.
      <br/>
      More specifically, the following image processing route, as shown below, results:
      <br/>
      Camera --&gt; Frontal Module --&gt; R0 --&gt; R1 --&gt; AIT
    </p>
    <p num="59">
      Each module acts as an agent for image processing, each respectively interpreting input data, and thus can be expressed by the term "agents." An agent which is close in input to a first agent and which serves as a standard is referred to as a "lower order" agent.
      <br/>
      Further, an agent which is closer to the AIT than the standard agent is referred to as a "higher order" agent.
    </p>
    <p num="60">
      An agent of a another local module having the same position as the standard agent is referred to as a "same order" agent.
      <br/>
      More specifically, for a certain local module, the frontal module 11 is a lower-order agent, the module making up the AIT subsystem 14 is a higher-order agent, and other local modules of the PIT 14 may be considered as same-order agents.
    </p>
    <p num="61">
      Further, local modules 21 are structured so as to group together items which are similar in topology, or items which resemble one another in the form of their given local images.
      <br/>
      Agents which are members of the same group as the standard agent are referred to as "neighboring" agents.
    </p>
    <p num="62">Further, in image information processing according to the present invention, "bottom-up processing" is conducted in which information progresses from lower-order agents toward higher order agents, in the direction of the arrows shown above; as well as the reverse flow, namely, "top-down processing" is conducted in which information is controlled from higher order agents toward lower order agents.</p>
    <p num="63">
      In FIG. 2, R0 sub-module 22 holds information of an average vector  PSI  of learned images of a given locus.
      <br/>
      In the case of a human face, as conducted in the simulation of the presented invention to be described later on, for an average vector  PSI  of the R0 sub-module of a nose referent local module, N-dimensional local data M are collected from M images centered on the nose, and each data is summated with a normalized N-dimensional vector  GAMMA I (1 &lt;= i &lt;= M), thereby determining an average vector  PSI  from the following equation (3).  (Equation image '2' not included in text)
    </p>
    <p num="64">
      The activity of the R0 sub-module 22 is a "hypothesis control agent" which expresses the strength of the hypothesis.
      <br/>
      With respect to normalized new image information  GAMMA NEW which is input, the initial value thereof becomes a vector value  GAMMA NEW *  PSI  which is considered as an initial hypothesis.
      <br/>
      The value of the R0 sub-module 22 is controlled, by control signals from neighboring or higher order agents, so as to subsequently issue an output, progressing toward globally consistent image recognition.
    </p>
    <p num="65">
      The R1 sub-module 23 possesses knowledge U of the eigenspace of an object image learned of a given object.
      <br/>
      The eigenspace is determined by the processing described below.
      <br/>
      Taking the least square best fit distribution of a vector  PHI , obtained by subtracting the average vector  PSI  from the N-dimensional vector  GAMMA i of the local image data ( PHI i = GAMMA i - PSI ), an orthonormal vector uk, as shown by equations (4-1), is sequentially summated from k=1 to k=M' (M' &lt;= M),  (Equation image '3' not included in text)
    </p>
    <p num="66">wherein the k sub-scripted vector uk is selected to maximize equation (4-1) under the condition of equation (4-2).</p>
    <p num="67">
      Vector uk and scalar quantity  LAMBDA k respectively become the eigenvector and eigenvalue of the covariant matrix C expressed by equation (5) below.
      <br/>
      The space derivable from the vector uk is referred to as the eigenspace of this location.  (Equation image '4' not included in text)
    </p>
    <p num="68">The vector  GAMMA NEW - GAMMA  expressing the characteristics of the input image  GAMMA NEW which are extracted by the R0 sub-module 22 are projected onto the eigenspace U which expresses separate characteristics of the learned local image in the R1 sub-module 23, thereby calculating the projected distance to the eigenspace of the input vector (hereinafter referred to as a "distance from feature space" or DFFS).</p>
    <p num="69">Local image information input to the R1 sub-module 23, through switch SW2 which is activated upon evaluation of characteristics in the R0 sub-module, can be highly accurate information input from the memory of the frontal module 11.</p>
    <p num="70">Further, in the R1 sub-module 23, based on a signal issued by a higher-order agent, top-down control is conducted for controlling the lower-order agent by reverse mapping, by which the eigenspace vector U is projected onto the input local image vector.</p>
    <p num="71">In the R2 sub-module 24, the projected distance (DFFS) to the eigenspace of the input vector is employed as principal information for evaluating consistency between the input local image and the projected eigenspace.</p>
    <p num="72">Recognition of the input local vector  GAMMA NEW is conducted based on the projected distance DFFS toward the eigenspace U of the input local vector  GAMMA NEW, as determined from the scalar product of the feature vector  GAMMA NEW - PSI  and the eigenvector uk.</p>
    <p num="73">Further, based on top-down control from the AIT subsystem, which is an upper-order agent, the R2 sub-module receives control concerning the evaluation of consistency of the input local image based on bottom-up processing, as discussed above.</p>
    <p num="74">
      The AIT subsystem 14 is made up of a module 18 for conducting analytical processing of the global image.
      <br/>
      The module 18 can be constructed in the same manner as the previously described local module 21.
      <br/>
      More specifically, it is constructed of sub-module s R0, R1 and R2, and the knowledge and functions possessed by each sub-module, aside from the image object becoming a global image in place of a local image, are entirely the same as described above.
    </p>
    <p num="75">
      FIG. 2 illustrates, using arrows, control operations of the sub-modules 22, 23 and 24, wherein such control operations shall be described below.
      <br/>
      As main control operations, there are four types of control operations between local module s, whereas, there are two types of internal control operations between sub-modules of the same local module.
    </p>
    <p num="76">First, control operations between local modules, or between the local modules and higher order or lower order agents, shall be described.</p>
    <p num="77">1 Local Activation Control, A-Type (shown by A arrows)</p>
    <p num="78">
      In the PIT 15, when a sub-module 22 of one local module 21, from among plural local modules 21 in charge of input images having different topologies of a given characteristic location, is activated, outputs are issued to the R0 sub-modules 22 of other local modules which are in charge of the same characteristic local image, thereby activating them.
      <br/>
      As a result, the R0 sub-modules of all local modules 21 in charge of the concerned characteristic location are activated, and image processing of the input image progresses.
      <br/>
      For example, supposing among plural local modules in charge of image processing of a nose, the R0 sub-module 22 of a local module 21 to which certain topology information is input is activated, then the R0 sub-modules of all local modules in charge of image processing of the nose are activated, thereby causing image processing of the nose to progress.
    </p>
    <p num="79">In particular, from among the local modules making up neighboring agents, as a result of this control, conjectures concerning characteristics of local images of resembling images, or whether there are similar topologies, can be formulated in a shortened time period.</p>
    <p num="80">2 Local Deactivation Control, B-Type (shown by B arrows)</p>
    <p num="81">
      In the case that plural local modules 21 in charge of local images having differing characteristics receive image information from regions having similar topology, when the R0 sub-module 22 of a specific local module 21 is activated, deactivation signals are issued to the R0 sub-modules of local modules which are in charge of other characteristic local images.
      <br/>
      As a result, competition among plural hypotheses directed to image information of regions having similar topology is prevented.
    </p>
    <p num="82">3 Top-Down Control, C-Type (shown by C arrows)</p>
    <p num="83">
      Top-down control is a control directed at lower order agents by which the activated agents themselves receive inputs.
      <br/>
      In a local module 21, local image information for which consistency has been determined is reevaluated based on a template for evaluating consistency of the local image which is provided in the AIT subsystem 14.
      <br/>
      If an inconsistent condition is judged in the AIT subsystem, operation of the concerned local module 21 is halted by a deactivation signal.
    </p>
    <p num="84">When inconsistency is judged in the R0 sub-module of the local module 21, a deactivation signal is sent to the lower-order agent frontal module 11, whereby input of image information to the concerned local module 21 is controlled.</p>
    <p num="85">Further, based on an evaluated result of consistency by global image processing in the AIT subsystem 14, an activation signal is output to local modules 21 which are currently in a deactivated state and which are in charge of characteristic local images of the concerned global image, thus prompting operation of such local modules.</p>
    <p num="86">4 Bottom-Up Control, D-Type (shown by D arrows)</p>
    <p num="87">If an evaluated value, taking as main information the projected distance DFFS in the R2 sub-module 24 of each local module 21, is greater than a threshold value, switch SW1 is closed, and an output is issued to the AIT subsystem as local image information indicative of consistency with the eigenspace information in the R1 sub-module 23.</p>
    <p num="88">When it is judged that the evaluated value of the R2 sub-module 24 exceeds a threshold, based on bottom-up image processing in the local module 21, if the input image is sufficiently described from the information held in each local module 21, consistency can also be confirmed based on top-down processing (C-Type) from the AIT subsystem 14, and from image information of the activated local modules 21.</p>
    <p num="89">The following shall now describe the two types of control operations undertaken between the R0, R1 and R2 sub-modules which make up the local modules.</p>
    <p num="90">1 Local Module Internal Top-Down Control, E-Type (shown by E arrows)</p>
    <p num="91">
      When an evaluation value in the R2 sub-module 24 making up the local module 21 is judged to exceed a threshold value, the R2 sub-module 24 deactivates the R0 sub-module 22 of the same local module 21.
      <br/>
      If the evaluation value of the R2 sub-module becomes greater than the threshold, based on the aforementioned bottom-up control (D-type), information in the R1 sub-module 23 is output to the AIT subsystem through switch SW1, and should this indicate consistency, recognition of the local image is completed.
    </p>
    <p num="92">
      In the case that activation of the R0 sub-module continues, based on the aforementioned local deactivation control (B-Type), R0 sub-modules 22 of different local modules 21 having input thereto image information of resembling topology remain deactivated.
      <br/>
      This causes the sequence seeking options, if recognition of the local image is still in an unconfirmed condition, to become narrowed.
      <br/>
      Based on deactivation of the R0 sub-modules of local modules for which recognition has already been completed, it becomes possible for other local modules 21 making up neighboring agents to be activated.
    </p>
    <p num="93">2 Local Module Internal Sub-Module Input Control, F-Type (shown by F arrows)</p>
    <p num="94">
      In local module 21, so long as the value of the R0 sub-module 22 does not exceed the threshold value, switch SW2 is not closed, and image information is not input to the R1 sub-module 23.
      <br/>
      When it is judged that the level of extracted characteristics from the R0 sub-module 22 is greater than the threshold value, switch SW2 closes and image information which is projected from the frontal module 11 onto the eigenspace of the R1 sub-module 23 is input
    </p>
    <p num="95">As a result of such control, unnecessary calculations are avoided and computer load is lessened, together with improving the quality of image information which is projected by the R1 sub-module 23, while also minimizing noise attributed to higher-order agents.</p>
    <p num="96">Similarly, until the value of the R2 sub-module also exceeds a threshold, and its consistency is evaluated, control is effected by switch SW1 such that image information from the R1 sub-module 23 is not transmitted to the AIT subsystem 14.</p>
    <p num="97">FIG. 3 illustrates overall image processing flow in the PIT subsystem 15.</p>
    <p num="98">
      The explanations below are presented, by way of example, referring to an actual simulation according to teachings of the present invention which was performed for recognizing a human face.
      <br/>
      However, the human face is but one example of an object image which is made up of several components by sub-designation, and which is suitable for such a structurally designated description.
      <br/>
      The application of the present invention is by no means limited to recognition of faces, and the techniques herein are applicable to any type of image recognition in which recognition of an input global image is performed based on reconstruction of the global image from plural local images.
    </p>
    <p num="99">The simulation of the present system was effected using a Sun SS10 Workstation which was programmed in GNU C++ code.</p>
    <p num="100">
      The data provided in Step 101 is image data of a scanned pixel region from a window of attention optionally set from within a global image input from a camera.
      <br/>
      In Step 102, necessary preprocessing, such as normalization processing to compensate for variations in the input image data brought about by variations in illumination at camera input, or variations in the input image data brought about by linear response characteristics of a CCD camera, or scale normalization processing and the like, are performed, and thereafter the data is stored in a memory area of the frontal module 11.
    </p>
    <p num="101">
      Based on scan position information of the window of attention, the image information stored in the memory is respectively input to the R0 sub-modules of local modules corresponding to the input topology.
      <br/>
      Appropriate PCA processing may also be performed on the input image information by the frontal module 11.
    </p>
    <p num="102">
      The R0 sub-module 22 is defined by an average vector of all locations corresponding to pixel position, taken from plural image data gained from learning concerning all local images.
      <br/>
      In the present simulation, an average vector of a 200 dimensional vector of principal components, concerning the left and right eyes, nose and mouth and 12 types of distinct facial outlines, was used.
    </p>
    <p num="103">In Step 103, the R0 sub-modules 22 of each local module 21 conduct pattern matching using the average vector from the input local image data as a template, and output an analog signal corresponding to the distance therefrom.</p>
    <p num="104">The R0 sub-module can scan a vicinity (3 * 3 pixel region) of the input local image, and select the input local image exhibiting the greatest activity.</p>
    <p num="105">However, because pattern detection ability, when subjecting such a type of average vector to templating, is low and produces ambiguities, the R0 sub-module can also be considered as a hypothesis generator.</p>
    <p num="106">
      The knowledge possessed in the R0 sub-module as an average vector is expedient for feature extraction of the input image information, and extracted characteristics can be output to the R1 sub-module 23 and projected onto eigenspace.
      <br/>
      However, considering the R0 sub-module as a hypothesis generator as described above, in the case that image information output to the R1 sub-module is made up such that, corresponding to the degree of activity of the R0 sub-module 22, very high dimensional image data is input from the memory of the local module 11, a simple template can be used from relatively few principal component vectors, as knowledge of the R0 sub-module 22.
    </p>
    <p num="107">
      In Step 104, the result of the above-described consistency determination is specified as a verification assessment value.
      <br/>
      In the present embodiment, evaluation of consistency in the R0 sub-module 22 is undertaken by the R2 sub-module 24.
      <br/>
      More specifically, the R2 sub-module 24 is constructed as a module for performing evaluation of image information processing of the R0 sub-module 22 and the R1 sub-module 23.
    </p>
    <p num="108">
      In Step 104, controls are received based on activation inputs (Arrow C) from the above-stated AIT subsystem 14, deactivation (Arrow B) and activation (Arrow A) signal inputs from other local modules 21, and deactivation signal inputs (Arrow E) from the R2 sub-module 24 of the local module 21 itself.
      <br/>
      In the case that deactivation signals are not input and activation is maintained, evaluation is made using an R0 function, and based on the result of that evaluation, output to other agents are performed.
      <br/>
      The R0 function is expressed by the following equation (6)
    </p>
    <p num="109">
      R0�k��y��x�(t)
      <br/>
      =f(the value based on the distance between template K and local input
      <br/>
      +local activation control from a different neighboring local module, A-Type
      <br/>
      -local deactivation control, B-Type
      <br/>
      +lower-order agent top-down control, C-Type
      <br/>
      -local module internal top-down control, E-Type
      <br/>
      -normalization based on total R0 activity
      <br/>
      =f(template�k� * localinput�y��x�(t-1)
      <br/>
      +w_Neighbor_Act* SIGMA neighbor yy,xx R0�k��yy��xx�(t-1)
      <br/>
      -maxkk not equal to k R0�kk��y��x�(t-1)
      <br/>
      +w_Next_Feedback_Act*ACT_NextR1�k��y��x�(t-1)
      <br/>
      -R2�kk��y��x�(t-1)
      <br/>
      -max ( SIGMA kk,yy,xx R0�kk��yy��xx�(t-1)-INH_NextR2(t-1,0))  (6)
    </p>
    <p num="110">wherein, k is a suffix expressing local characteristics, y and x are suffixes expressing column position, t is a time step of the dot operator, coefficients starting with w represent weightings, such that w_Neighbor_Act=0.5 and w_Next_Feedback_Act=0.2 are set at such values in all R0 sub-modules 22, f is the monotone increasing function f .di-elect cons.�0, 1�, template�k� is the M dimensional vector for the template and localinput�y��x� is the M dimensional vector of the input image data input to the R0 sub-module.</p>
    <p num="111">
      In the R0 sub-module 22, as described by items of the aforementioned control operations, based on control signals of the local activation control (Arrow A) or local deactivation control (Arrow B) from the R0 sub-module of another agent, activation or deactivation control is received, and further, top-down control is received from the R2 sub-module 24 and the AIT subsystem 14.
      <br/>
      Top-down control (Arrow E) from the R2 sub-module 24 is a deactivation control; and thus, image processing concerning hypotheses already interpreted by upper-order agents with respect to the signal processing level of the R0 sub-module 22 are halted.
    </p>
    <p num="112">
      AIT subsystem top-down control (Arrow C) involves both activation and deactivation.
      <br/>
      Local module 21 conducts image processing, and local image information for which consistency has been evaluated is projected onto the global image in the AIT, wherein based on the reevaluated result, functions of the local module are subjected to top-down control.
      <br/>
      In Step 105 (according to the present embodiment, Step 5 is performed in the R2 sub-module 24), if a value f based on a distance between the template k and the input image data vector is above a given threshold, local image information is output from the memory of the frontal module 11 to the R1 sub-module 23 of the same local module.
    </p>
    <p num="113">Further, based on the evaluation in Step 104, A-Type and B-Type control signals are output to the R0 sub-modules of other local modules.</p>
    <p num="114">
      The R1 sub-modules 23 contain eigenspace knowledge based on learned individual image information of each local image.
      <br/>
      In the actual face simulation, from among a 200 dimensional vector of learned image information, vectors composed of 20 large value upper-order components were used as principal components, and individual eigenspaces of the local images were generated.
    </p>
    <p num="115">In step 106, when the R0 sub-module 22 of the local module 21 is activated, input image information of the concerned location is output from the memory of the frontal module 11 to the R1 sub-module 23.</p>
    <p num="116">
      The input local image information  GAMMA NEW is a vector expressing characteristics subtracted by the average vector, wherein a principal component vector, composed of 20 upper-order components of the concerned vector, is projected onto the individual eigenspaces possessed by the R1 sub-modules 23.
      <br/>
      The is expressed more concretely by equation (7)
      <br/>
      R1�k��y��x��j�(t)
      <br/>
      =PC�k��j� * (localinput�y��x�(t-1)-Mean�k�)
      <br/>
      (if R0�k��y��x�(t-1) &gt;= R0_threshold, wherein the hypothesis is created), or
      <br/>
      =R1�k��y��x��j�(t-1)
      <br/>
      (if R0�k��y��x�(t-1)R0_threshold, wherein activation is temporarily maintained)  (7)
    </p>
    <p num="117">wherein, suffixes k, y, x and t are the same as those for R0 in equation (6), j is a suffix of the eigenvector, PC�k��j� is the j-th eigenvector of location k, Mean�k� is the average vector of location k, and localinput�y��x� is the M-dimensional vector for the input local vector information input to R1.</p>
    <p num="118">The input local image vector in the R1 sub-module 23 and projected eigenspace distance DFFS contained as information in the R1 sub-module 23 are used for evaluating image recognition, wherein this evaluation is undertaken by the R2 sub-module 24 in Step 107.</p>
    <p num="119">The R2 sub-module which performs the evaluation in Step 107 receives top-down control (C-Type) from the AIT subsystem which activates a priority operated local module 21, based on a global image processing result in the AIT subsystem 14 which serves as an upper-order agent.</p>
    <p num="120">Based on such control signals, unnecessary processing in the R2 sub-module 24 is deactivated, wherein processing time for image recognition of the overall system is shortened.</p>
    <p num="121">Equation (8), below, expresses the evaluation in the R2 sub-module 24.</p>
    <p num="122">
      R2�k��y��x��j�(t)
      <br/>
      =1.0 (normalization factor)
      <br/>
      +.parallel.R1�k�(t-1).parallel.2 R1 activation and image information to the AIT subsystem, F-Type control)
      <br/>
      -.parallel.Mean�k�-localinput�y��x�(t-1).parallel.2 (distance between input image and average vector)
      <br/>
      -maxR2�kk��y��y�(t-1)(local module deactivation, E-Type control)
      <br/>
      +ACT_NextR1�k��y��x�(t-1) (AIT top-down, C-Type control)  (8)
    </p>
    <p num="123">In the evaluation in the R2 sub-module 24 performed in Step 107, if the evaluation level is above a threshold, the local image information of the R1 sub-module 23 is output to the AIT subsystem 14 serving as an upper-order agent (Arrow D).</p>
    <p num="124">Further, the R2 sub-module of the local module which receives C-Type top-down control from the AIT subsystem 14 and is activated thereby, outputs a reverse mapping, which is a projection of the eigenspace in the R1 sub-module 23 onto the input image vector, as a top-down hypothesis (Arrow R0).</p>
    <p num="125">In the R2 sub-module 24, when an evaluation of consistency taking the projected distance DFFS as principal information exceeds a threshold value, and the input image is recognized, a self-deactivation control signal (Arrow E) is output to the R0 sub-module.</p>
    <p num="126">The AIT subsystem 14 is constructed as a module for analytical processing of image information of the global image, into which global image information from the frontal module 11 and local image information processed by each local module 21 from the PIT subsystem 11 is input, thereby performing recognition of the global image which is consistent with each local image.</p>
    <p num="127">
      The AIT subsystem 14 modules are constructed similarly to the above-described local modules 21 and R0, R1 and R2 sub-modules.
      <br/>
      The functions thereof, and the functions of the relevant sub-modules of such local modules 21, are substantially the same.
      <br/>
      More specifically, the R0 sub-modules possess knowledge of the average vector of learned global images, as well as a template function for extracting characteristics of the input global image.
      <br/>
      The R1 sub-modules possess information of individual eigenspaces of learned global images, and a function for projecting the global image information onto eigenspace, based on evaluation from a template of the input global image in the R0 sub-module.
      <br/>
      The R2 sub-modules possess knowledge of a threshold value of the projected distance DFFS, and using the expression value of the R1 sub-module, perform evaluations to determine consistency based on the DFFS.
    </p>
    <p num="128">In the simulation for recognition of human faces according to the present invention, the average vector in the R0 sub-module of the AIT subsystem 14 was defined by an average vector of image data (105 * 105 pixel positions  * 3 principal component vectors) which was appropriately centered and selected from global images (128 * 128 pixel positions  * 200 principal component vectors) of faces of 35 people, from out of 105 individual's learned facial images.</p>
    <p num="129">
      The 105 * 105 pixel positions define a pixel region making up a window of attention 203 as shown in FIG. 4.
      <br/>
      The position of the face cannot be specified from the input image of an actual camera alone; and therefore, based on an instruction from the AIT subsystem 14, the camera is scanned, wherein based on template matching in the R0 sub-module 204, the position of the window of attention is decided.
    </p>
    <p num="130">
      The AIT subsystem 14 further possesses a template relating to local image information input from local modules 21 of the PIT subsystem 15.
      <br/>
      Initially, the template is the same average vector as that of the R0 sub-module 22 of the local module 21 of the PIT subsystem 15.
      <br/>
      But also, depending on the circumstances, based on an evaluation of the global image in the AIT subsystem 14, the template can be an eigenspace derived from a high multidimensional vector, as a template with respect to image information from the local module 21 of the PIT subsystem 15 which has been top-down activated.
    </p>
    <p num="131">
      The size of the window of attention defined by the AIT subsystem 14 is 105 * 105 pixels, whereas the local image which is input to the AIT subsystem 14 from the PIT subsystem 15 is defined by 35 * 35 pixel positions  * 16 locations.
      <br/>
      In the actual simulation, taking into account computer calculational ability, a vector defined by 12 * 12 pixel positions  * 16 locations  * 20 components was used.
    </p>
    <p num="132">Referring to the flowchart of the overall system of the invention shown by FIG. 4, an illustrative example, based on a face simulation, shall now be described.</p>
    <p num="133">
      An image input by the camera is stored in a memory area 202 of the frontal module 11, as a global image comprised of digital data which has been subject to the aforementioned normalization processing.
      <br/>
      In the present simulation, a 128 * 128 pixel region, 256 tonal density image was used.
    </p>
    <p num="134">
      Via a global image data base 300, the image is input to the AIT sub-system 204.
      <br/>
      The global facial image (3 principal component vector) is evaluated by template matching in the R0 sub-module 204 to judge whether it is a human face or not, and features of the input facial image are extracted using an average vector template.
      <br/>
      Following evaluation in the R0 sub-module 204, the facial image is projected onto the eigenspaces of faces of 105 individuals in the R1 sub-module 205.
    </p>
    <p num="135">
      In the R2 sub-modules 206 of the AIT sub-system 14, the DFFS values are determined and compared against a threshold value.
      <br/>
      By the evaluation in the R2 sub-module 206, in the case that one DFFS of eigenspace is specified by the threshold value, the input facial image may be recognized as an individual's face specified by that eigenspace.
      <br/>
      However, considering the need to simplify calculation and calculational speed, only a few principal component vectors may be sampled, and for performing the above image information processing, plural eigenspaces may be specified in the evaluation of the R2 sub-module 206.
    </p>
    <p num="136">In this manner, an activation signal is output as a top-down control to local modules of the PIT subsystem in charge of local images belonging to the face (global image) of the plural specified eigenspaces.</p>
    <p num="137">On the other hand, the input global image is scanned by a window of attention 204 set based on topology information of local modules of the PIT subsystem, and is input to corresponding local modules based on a data base 300 formed by local images of the mouth, nose, left and right ears and 12 facial outline segments.</p>
    <p num="138">For example, for local image data input to a nose local module, the learned average vector of the nose in the R0 sub-module 208 is used as a template, thereby enabling the data to be recognized as being a nose image, and simultaneously, the characteristics of the input nose image are extracted.</p>
    <p num="139">
      Once the input local image is evaluated as being a nose, component vectors expressing characteristics extracted by the R0 sub-module 208 are projected onto the eigenspace of the nose in the R1 sub-module 209.
      <br/>
      Alternatively, high-dimensional input image information from the memory of the frontal module 202 is input and projected onto the eigenspace of the nose in the R1 sub-module 209.
      <br/>
      The projected distance DFFS obtained as a result of this projection is evaluated based on a threshold value in the R2 sub-module 210, and when a judgment of consistency occurs, input image information of the R1 sub-module 209 is output through the AIT subsystem 300, wherein consistency with the global image is evaluated based on a template in the AIT subsystem.
    </p>
    <p num="140">
      In this case, in place of the input image information of the R1 sub-module, high dimensional input information from the memory of the frontal module 202 may as be input to the AIT subsystem.
      <br/>
      Should this be done, highly precise image recognition can be achieved by projecting image information of higher dimensional principal components onto the eigenspace.
    </p>
    <p num="141">
      In the R0 sub-module 208 of the PIT subsystem, when the fact of a nose image has been recognized, B-type control signals are output to other neighboring local modules, controlling operation of other local modules which process the same local image information.
      <br/>
      Also, A-type activation control signals are output to R0 sub-modules of different local modules in charge of processing other local images having resembling topologies, for thereby promoting processing therein.
    </p>
    <p num="142">When consistency is confirmed by evaluation in the R2 sub-module 210, E-type control signals are output to the R0 sub-module 208 of the same local module for deactivating operation thereof.</p>
    <p num="143">Based on an evaluation of consistency by global image processing in the AIT subsystem, in local modules of the PIT subsystem which are activated by top-down control, concerning local image information evaluated by the R0 sub-modules therein, it is also acceptable if only the projected distance DFFS between concerned local images of individuals recognized in the AIT subsystem and the eigenspace is evaluated.</p>
    <p num="144">In the case where plural individuals are hypothesized by top-down control and taken as local modules, during evaluation in the R2 sub-module, if the local image of one such individual is recognized, at that level, the global image is specified and recognized.</p>
    <p num="145">
      If the input image is that of an unlearned object, recognition for specifying the input image as a result is also possible.
      <br/>
      In the case of such yet unlearned input images, by establishing functions for automatically adding such image processing information to the knowledge of sub-modules of each subsystem, a system capable of self-learning can be obtained.
      <br/>
      Furthermore, by memorizing recognized images which are consistent with features of input image information for which image recognition has been performed once, and/or by memorizing eigenspaces of each subsystem for which consistency has been confirmed, modules possessing information of eigenspaces corresponding to input image characteristics can be activated in priority, resulting in a system having high learning efficiency.
    </p>
    <p num="146">
      FIG. 5 shows visually the progression of image processing in local modules in the simulation.
      <br/>
      More specifically, FIG. 5(a) shows a case in which top-down processing from the AIT subsystem is not performed, whereas FIG. 5(b) shows the case, recognizing the same input image, yet wherein top-down processing from the AIT subsystem is performed.
      <br/>
      In the figures, time steps (one step equaling 5 milliseconds) are represented in the columnar direction, whereas line 1 shows a progression of global image processing in the AIT subsystem.
    </p>
    <p num="147">
      Line 2 shows the activation state of the R2 sub-modules of 16 local modules of the PIT subsystem.
      <br/>
      The lines therebelow show, in time series, activation states of the R0 sub-modules of local modules for the right eye, left eye, nose and mouth, respectively.
    </p>
    <p num="148">
      Unlearned images were used for the input image shown in FIG. 5.
      <br/>
      In the case of FIG. 5(a) in which top-down control was not performed, first the facial outline was detected, and immediately thereafter, both eyes and the nose were detected.
      <br/>
      However, because an average vector was uses as a template for the R0 sub-module, numerous mistakes resulted in recognition of the left and right eyes, and further, up through nine steps, the mouth could still not be detected.
    </p>
    <p num="149">
      In the case shown by FIG. 5(b), in which top-down control from the AIT subsystem was performed, detection of the facial outline, both eyes and nose were similar to the case of FIG. 5(a).
      <br/>
      However, by step 5, the mouth was also detected.
      <br/>
      Comparing the activity state of the R0 sub-module in FIGS. 5(a) and 5(b), it can be judged that activation in the R0 sub-module was less when top-down control was performed.
      <br/>
      This demonstrates that, due to top-down control, the activities of inconsistent sub-modules were suppressed, and it can be understood that, from among less activity information, the most consistent data was selected in the R2 sub-module.
    </p>
    <p num="150">
      Further, as understood from the time sequence variations of global image processing in the AIT subsystem shown by line 1, at first it was weak and coarse, but gradually became more detailed.
      <br/>
      Accordingly, it is understood that top down control becomes more detailed and precise over time.
    </p>
    <p num="151">
      Chart 1 shows detection rates for locations of the eyes, nose and mouth in respect to 105 facial images used in the simulation.
      <br/>
      From the chart, it can be appreciated that, even when compared to the detection rate using the prior art eigenspaces method, the image recognition method according to the present invention offers a substantial improvement in detection ability.
    </p>
    <p num="152">
      -- CHART 1
      <br/>
      -- Detection Rate for Local Characteristics
      <br/>
      -- (no. detected/no. of cases)
      <br/>
      --                      A                  B
      <br/>
      --                      Simulation by      Eigenspace
      <br/>
      -- Rates                Present Invention  Method Only
      <br/>
      -- Detected             99.5%   (209/210)   94.3%   (198/210)
      <br/>
      -- (Eyes)
      <br/>
      -- Mis-detected         0.0%    (0/210)     0.0%    (0/210)
      <br/>
      -- (Eyes)
      <br/>
      -- Undetectable         0.5%    (1/210)     5.7%    (12/210)
      <br/>
      -- (Eyes)
      <br/>
      -- Detected             100.0%  (105/105)   92.4%   (97/105)
      <br/>
      -- (Nose)
      <br/>
      -- Mis-detected (Nose)  0.0%    (0/105)     0.0%    (0/105)
      <br/>
      -- Undetectable         0.0%    (0/105)     7.6%    (8/105)
      <br/>
      -- (Nose)
      <br/>
      -- Detected             99.0%   (104/105)   76.2%   (80/105)
      <br/>
      -- (Mouth)
      <br/>
      -- Mis-detected (Mouth) 0.0%    (0/105)     0.0%    (0/105)
      <br/>
      -- Undetectable         1.0%    (1/105)     23.8%   (25/105)
      <br/>
      -- (Mouth)
    </p>
    <p num="153">
      In the image recognition system according to the present invention, recognition processing of a global image and local images are performed simultaneously in parallel by plural modules whose functions are distributed.
      <br/>
      Further, processes which are inconsistent are quickly deactivated by deactivation signals between processing modules, and further, due to functions for which image processing in local modules necessary for recognition of the global image are promoted, it becomes possible to reduce computational load while still enabling image recognition in a shortened time period.
    </p>
    <p num="154">
      Further, image processing according to the present invention, as a result of the above structure, provides a system in which at the initiation of image processing, globally consistent recognized forms are realized through multi-agent calculations on the basis on image recognition hypotheses from highly compressed data.
      <br/>
      Accordingly, probability evaluations which accompany complex equations such as maximum-likelihood analysis, as in the prior art, are not required.
    </p>
    <p num="155">
      Further, in image recognition according to the present invention, multiple local modules perform sequence sequential processing of input images, possessing both bottom-up processing flow reaching recognition of a global image, and top-down processing flow in which, through local module processing, hypothetical recognition obtained from a global image is confirmed.
      <br/>
      In processing agents which are in agreement with these two types of processing flows, at the point at which consistency is evaluated, because it can be considered whether true recognition of the global image has been established, image recognition in a rapid time period can be achieved.
    </p>
  </description>
  <claims format="original" lang="en" id="claim_en">
    <claim num="1">
      <claim-text>What is claimed is:</claim-text>
      <claim-text>1.</claim-text>
      <claim-text>An image recognition system for recognizing an object using a global image processing means and a local image processing means performing parallel processing on a global image of an object of recognition input from a camera or other optical pick-up device and on characteristic local images, comprising:</claim-text>
      <claim-text>global image processing means for performing analytical processing on a global image, comprising means for extracting a global characteristic of an input global image and means for evaluating consistency of said extracted global characteristic; local image processing means for performing analytical processing on a plurality of local images, said local image processing means being made up from a plurality of local modules corresponding respectively to each local image, each of said local modules comprising means for extracting a characteristic of an input local image and means for evaluating consistency of the extracted characteristic with the image to be recognized, wherein said global image processing means receives an input image and exchanges signals with said local modules, and deactivates functions of said local modules which are inconsistent with said global characteristic, while activating functions of said local modules which are consistent with said global characteristic, thereby effecting a hierarchical control wherein consistency evaluations made in said global image processing means influence activation or deactivation of functions in said local modules, and wherein respective local modules of said local image processing means control functions of said global image processing means based on signals indicating consistency with said local images.</claim-text>
    </claim>
    <claim num="2">
      <claim-text>2. The image recognition system according to claim 1, wherein each of said local modules making up said local image processing means comprises plural local modules corresponding to different topologies of the same local image, and determined by local images which correspond in location with each other.</claim-text>
    </claim>
    <claim num="3">
      <claim-text>3. The image recognition system according to claim 1, wherein each of said local modules making up said local image processing means comprises plural local modules corresponding to different topologies of the same local image, and determined by local images which correspond in form with each other.</claim-text>
    </claim>
    <claim num="4">
      <claim-text>4. The image recognition system according to claim 1, wherein each of said local modules making up said local image processing means comprises: a first sub-module comprising means for extracting a characteristic of an input local image; a second sub-module having stored therein knowledge of an eigenspace of a given local icon of a pre-learned object of recognition;</claim-text>
      <claim-text>and a third sub-module comprising means for evaluating consistency based on a projected distance from a result of projecting the image information of said input local image having the characteristic extracted by said first sub-module onto an eigenspace stored in said second sub-module.</claim-text>
    </claim>
    <claim num="5">
      <claim-text>5. The image recognition system according to claim 4, wherein said first sub-module has stored therein knowledge of an average vector of the given local image of said pre-learned object of recognition.</claim-text>
    </claim>
    <claim num="6">
      <claim-text>6. An image recognition system for recognizing an object from a global image of the object of recognition input from a camera or other optical pick-up device, and from characteristic local images, comprising: global image processing means for performing analytical processing on a global image, comprising means for extracting a global characteristic of an input global image and means for evaluating consistency of said extracted global characteristic; local image processing means for performing analytical processing on a plurality of local images, said local image processing means being made up from a plurality of local modules arranged in correspondence with a topology of a local image, each of said local modules comprising (a) a first sub-module comprising means for extracting a characteristic of an input local image, (b) a second sub-module having stored therein knowledge of an eigenspace of a given local icon of a pre-learned object of recognition, and (c) a third sub-module comprising means for evaluating consistency based on a projected distance from a result of projecting the image information of said input local image having the characteristic extracted by said first sub-module onto an eigenspace stored in said second sub-module, wherein said global image processing means receives both an input image and inputs from said local modules, and deactivates functions of said local modules which are inconsistent with said global characteristic while activating functions of said local modules which are consistent with said global characteristic, and wherein among plural local modules which have input thereto image information from neighboring regions and which are in charge of different local images, one local module having an activated first sub-module therein outputs a signal for deactivating the first sub-module of another local module in charge of a different local image.</claim-text>
    </claim>
    <claim num="7">
      <claim-text>7. An image recognition system for recognizing an object from a global image of the object of recognition input from a camera or other optical pick-up device, and from characteristic local images, comprising: global image processing means for performing analytical processing on a global image, comprising means for extracting a global characteristic of an input global image and means for evaluating consistency of said extracted global characteristic; local image processing means for performing analytical processing on a plurality of local images, said local image processing means being made up from a plurality of local modules arranged in correspondence with a topology of a local image, each of said local modules comprising (a) a first sub-module comprising means for extracting a characteristic of an input local image, (b) a second sub-module having stored therein knowledge of an eigenspace of a given local icon of a pre-learned objection of recognition, and (c) a third sub-module comprising means for evaluating consistency based on a projected distance from a result of projecting the image information of said input local image having the characteristic extracted by said first sub-module onto an eigenspace stored in said second sub-module, wherein said global image processing means receives both an input image and inputs from said local modules, and deactivates functions of said local modules which are inconsistent with said global characteristic while activating functions of said local modules which are consistent with said global characteristic, and wherein image information is input to said second sub-module of one of said local modules only when said first sub-modules is activated above a predetermined threshold.</claim-text>
    </claim>
    <claim num="8">
      <claim-text>8. An image recognition system according to claim 6, wherein plural local modules in charge of the same local image are arranged in correspondence with a topology of image information formed by the local image.</claim-text>
    </claim>
    <claim num="9">
      <claim-text>9. An image recognition system according to claim 7, wherein plural local modules in charge of the same local image are arranged in correspondence with a topology of image information formed by the local image.</claim-text>
    </claim>
    <claim num="10">
      <claim-text>10. An image recognition system according to claim 6, wherein when consistency is confirmed based on an evaluation in said third sub-module of one of said local modules, the first sub-module of the same local module is deactivated.</claim-text>
    </claim>
    <claim num="11">
      <claim-text>11. An image recognition system according to claim 7, wherein when consistency is confirmed based on an evaluation in said third sub-module of one of said local modules, the first sub-module of the same local module is deactivated.</claim-text>
    </claim>
    <claim num="12">
      <claim-text>12. An image recognition system for recognizing an object using a global image processing means and a local image processing means performing parallel processing on a global image of an object of recognition input from a camera or other optical pick-up device and on characteristic local images, comprising: global image processing means for performing analytical processing on a global image, comprising means for extracting a global characteristic of an input global image and means for evaluating consistency of said extracted global characteristic; local image processing means for performing analytical processing on a plurality of local images, said local image processing means being made up from a plurality of local modules arranged in correspondence with a topology of a local image, each of said local modules comprising (a) a first sub-module comprising means for extracting a characteristic of an input local image, (b) a second sub-module having stored therein knowledge of an eigenspace of a given local icon of a pre-learned object of recognition, and (c) a third sub-module comprising means for evaluating consistency based on a projected distance from a result of projecting the image information of said input local image having the characteristic extracted by said first sub-module onto an eigenspace stored in said second sub-module, wherein said global image processing means receives an input image and exchanges signals with the sub-modules of said local modules, and deactivates functions of the sub-modules of said local modules which are inconsistent with said global characteristic, while activating functions of the sub-modules of said local modules which are consistent with said global characteristic, thereby effecting a hierarchical control wherein consistency evaluations made in said global image processing means influence activation or deactivation of functions in said sub-modules of said local modules, and wherein respective sub-modules of said local modules of the local image processing means mutually deactivate functions of the sub-modules of other local modules.</claim-text>
    </claim>
    <claim num="13">
      <claim-text>13. An image recognition system for recognizing an object using a global image processing means and a local image processing means performing parallel processing on a global image of an object of recognition input from a camera or other optical pick-up device and on characteristic local images, comprising: global image processing means for performing analytical processing on a global image, comprising means for extracting a global characteristic of an input global image and means for evaluating consistency of said extracted global characteristic; local image processing means for performing analytical processing on a plurality of local images, said local image processing means being made up from a plurality of local modules arranged in correspondence with a topology of a local image, and wherein plural local modules in charge of the same local image are arranged in correspondence with a topology of image information formed by the local image, each of said local modules comprising (a) a first sub-module comprising means for extracting a characteristic of an input local image, (b) a second sub-module having stored therein knowledge of an eigenspace of a given local icon of a pre-learned object of recognition, and (c) a third sub-module comprising means for evaluating consistency based on a projected distance from a result of projecting the image information of said input local image having the characteristic extracted by said first sub-module onto an eigenspace stored in said second sub-module, wherein said global image processing means receives an input image and exchanges signals with the sub-modules of said local modules, and deactivates functions of the sub-modules of said local modules which are inconsistent with said global characteristic, while activating functions of the sub-modules of said local modules which are consistent with said global characteristic, thereby effecting a hierarchical control wherein consistency evaluations made in said global image processing means influence activation or deactivation of functions in said sub-modules of said local modules, and wherein respective sub-modules of said local modules of the local image processing means mutually deactivate functions of the sub-modules of other local modules.</claim-text>
    </claim>
  </claims>
</questel-patent-document>