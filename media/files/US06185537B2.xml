<?xml version="1.0" encoding="UTF-8"?>
<questel-patent-document lang="en" date-produced="20180805" produced-by="Questel" schema-version="3.23" file="US06185537B2.xml">
  <bibliographic-data lang="en">
    <publication-reference publ-desc="Granted patent as second publication">
      <document-id>
        <country>US</country>
        <doc-number>06185537</doc-number>
        <kind>B2</kind>
        <date>20010206</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>US6185537</doc-number>
      </document-id>
    </publication-reference>
    <original-publication-kind>B2</original-publication-kind>
    <application-reference family-id="21865294" extended-family-id="14416829">
      <document-id>
        <country>US</country>
        <doc-number>08984221</doc-number>
        <kind>A</kind>
        <date>19971203</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>1997US-08984221</doc-number>
      </document-id>
      <document-id data-format="questel_Uid">
        <doc-number>14736800</doc-number>
      </document-id>
    </application-reference>
    <language-of-filing>en</language-of-filing>
    <language-of-publication>en</language-of-publication>
    <priority-claims>
      <priority-claim kind="national" sequence="1">
        <country>US</country>
        <doc-number>98422197</doc-number>
        <kind>A</kind>
        <date>19971203</date>
        <priority-active-indicator>N</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="1">
        <doc-number>1997US-08984221</doc-number>
      </priority-claim>
      <priority-claim kind="national" sequence="2">
        <country>US</country>
        <doc-number>3250796</doc-number>
        <kind>P</kind>
        <date>19961203</date>
        <priority-active-indicator>Y</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="2">
        <doc-number>1996US-60032507</doc-number>
      </priority-claim>
    </priority-claims>
    <dates-of-public-availability>
      <publication-of-grant-date>
        <date>20010206</date>
      </publication-of-grant-date>
    </dates-of-public-availability>
    <classifications-ipcr>
      <classification-ipcr sequence="1">
        <text>G10L  15/22        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>10</class>
        <subclass>L</subclass>
        <main-group>15</main-group>
        <subgroup>22</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="2">
        <text>G10L  15/00        20060101AFI20060310RMJP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>10</class>
        <subclass>L</subclass>
        <main-group>15</main-group>
        <subgroup>00</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>JP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20060310</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="3">
        <text>G06F   3/16        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>06</class>
        <subclass>F</subclass>
        <main-group>3</main-group>
        <subgroup>16</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="4">
        <text>G10L  15/26        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>10</class>
        <subclass>L</subclass>
        <main-group>15</main-group>
        <subgroup>26</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="5">
        <text>G10L  15/28        20060101ALI20060310RMJP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>10</class>
        <subclass>L</subclass>
        <main-group>15</main-group>
        <subgroup>28</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>JP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20060310</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="6">
        <text>G10L  19/00        20060101ALI20060310RMJP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>10</class>
        <subclass>L</subclass>
        <main-group>19</main-group>
        <subgroup>00</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>JP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20060310</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="7">
        <text>H04M   1/27        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>H</section>
        <class>04</class>
        <subclass>M</subclass>
        <main-group>1</main-group>
        <subgroup>27</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="8">
        <text>H04M   3/533       20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>H</section>
        <class>04</class>
        <subclass>M</subclass>
        <main-group>3</main-group>
        <subgroup>533</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
    </classifications-ipcr>
    <classification-national>
      <country>US</country>
      <main-classification>
        <text>704275000</text>
        <class>704</class>
        <subclass>275000</subclass>
      </main-classification>
      <further-classification sequence="1">
        <text>704272000</text>
        <class>704</class>
        <subclass>272000</subclass>
      </further-classification>
      <further-classification sequence="2">
        <text>704E15040</text>
        <class>704</class>
        <subclass>E15040</subclass>
      </further-classification>
    </classification-national>
    <classifications-ecla>
      <classification-ecla sequence="1">
        <text>G10L-015/22</text>
        <section>G</section>
        <class>10</class>
        <subclass>L</subclass>
        <main-group>15</main-group>
        <subgroup>22</subgroup>
      </classification-ecla>
      <classification-ecla sequence="2">
        <text>G06F-003/16</text>
        <section>G</section>
        <class>06</class>
        <subclass>F</subclass>
        <main-group>3</main-group>
        <subgroup>16</subgroup>
      </classification-ecla>
      <classification-ecla sequence="3">
        <text>G10L-015/26A</text>
        <section>G</section>
        <class>10</class>
        <subclass>L</subclass>
        <main-group>015</main-group>
        <subgroup>26A</subgroup>
      </classification-ecla>
      <classification-ecla sequence="4">
        <text>H04M-001/27A</text>
        <section>H</section>
        <class>04</class>
        <subclass>M</subclass>
        <main-group>001</main-group>
        <subgroup>27A</subgroup>
      </classification-ecla>
      <classification-ecla sequence="5">
        <text>H04M-003/533</text>
        <section>H</section>
        <class>04</class>
        <subclass>M</subclass>
        <main-group>3</main-group>
        <subgroup>533</subgroup>
      </classification-ecla>
    </classifications-ecla>
    <patent-classifications>
      <patent-classification sequence="1">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>G10L-015/22</classification-symbol>
        <section>G</section>
        <class>10</class>
        <subclass>L</subclass>
        <main-group>15</main-group>
        <subgroup>22</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20141105</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="2">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>G06F-003/167</classification-symbol>
        <section>G</section>
        <class>06</class>
        <subclass>F</subclass>
        <main-group>3</main-group>
        <subgroup>167</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20141103</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="3">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>G10L-015/26</classification-symbol>
        <section>G</section>
        <class>10</class>
        <subclass>L</subclass>
        <main-group>15</main-group>
        <subgroup>26</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20150109</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="4">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>G10L-2015/223</classification-symbol>
        <section>G</section>
        <class>10</class>
        <subclass>L</subclass>
        <main-group>2015</main-group>
        <subgroup>223</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20180220</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="5">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>H04M-001/271</classification-symbol>
        <section>H</section>
        <class>04</class>
        <subclass>M</subclass>
        <main-group>1</main-group>
        <subgroup>271</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20141105</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="6">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>H04M-003/533</classification-symbol>
        <section>H</section>
        <class>04</class>
        <subclass>M</subclass>
        <main-group>3</main-group>
        <subgroup>533</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20141105</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="7">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>H04M-2201/40</classification-symbol>
        <section>H</section>
        <class>04</class>
        <subclass>M</subclass>
        <main-group>2201</main-group>
        <subgroup>40</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20141105</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="8">
        <classification-scheme office="EP" scheme="ICO"/>
        <classification-symbol>T04M-001/27A</classification-symbol>
      </patent-classification>
      <patent-classification sequence="9">
        <classification-scheme office="EP" scheme="ICO"/>
        <classification-symbol>T04M-201/40</classification-symbol>
      </patent-classification>
    </patent-classifications>
    <number-of-claims>7</number-of-claims>
    <exemplary-claim>5</exemplary-claim>
    <figures>
      <number-of-drawing-sheets>5</number-of-drawing-sheets>
      <number-of-figures>6</number-of-figures>
      <image-key data-format="questel">US6185537</image-key>
    </figures>
    <invention-title format="original" lang="en" id="title_en">Hands-free audio memo system and method</invention-title>
    <references-cited>
      <citation srep-phase="examiner">
        <patcit num="1">
          <text>PIRZ FRANK C, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4348550</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4348550</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="2">
          <text>BRENIG THEODORE</text>
          <document-id>
            <country>US</country>
            <doc-number>4426733</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4426733</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="3">
          <text>VANDER MOLEN DONALD R</text>
          <document-id>
            <country>US</country>
            <doc-number>4520576</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4520576</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="4">
          <text>BORTH DAVID E, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4737976</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4737976</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="5">
          <text>HASHIMOTO KAZUO</text>
          <document-id>
            <country>US</country>
            <doc-number>4737979</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4737979</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="6">
          <text>KITA KAZUNORI, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5014317</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5014317</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="7">
          <text>KOPP DIETER, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5420912</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5420912</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="8">
          <text>ENGLEHARDT C DUANE</text>
          <document-id>
            <country>US</country>
            <doc-number>5477511</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5477511</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="9">
          <text>BERTINO GIAN L, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5481645</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5481645</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="10">
          <text>NORRIS ELWOOD G, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5491774</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5491774</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="11">
          <text>RUSSELL STEVEN P, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5526407</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5526407</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="12">
          <text>BISSONNETTE W MICHAEL, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5602963</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5602963</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="13">
          <text>MOZER TODD F</text>
          <document-id>
            <country>US</country>
            <doc-number>5657380</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5657380</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="14">
          <text>TAYLOR CHARLES E, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5684506</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5684506</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="15">
          <text>WALTERS TIMOTHY L, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5794205</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5794205</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="16">
          <text>DOUGLAS H RUSSELL</text>
          <document-id>
            <country>US</country>
            <doc-number>5812977</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5812977</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="17">
          <text>BENNETT JAMES D</text>
          <document-id>
            <country>US</country>
            <doc-number>5878395</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5878395</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="18">
          <text>ABELOW DANIEL H</text>
          <document-id>
            <country>US</country>
            <doc-number>5999908</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5999908</doc-number>
          </document-id>
        </patcit>
      </citation>
    </references-cited>
    <related-documents>
      <related-publication>
        <document-id>
          <country>US</country>
          <doc-number>60/032,507</doc-number>
          <date>19961203</date>
        </document-id>
        <document-id>
          <country>US</country>
          <doc-number>60/032507</doc-number>
          <date>19961203</date>
        </document-id>
      </related-publication>
    </related-documents>
    <parties>
      <applicants>
        <applicant data-format="original" app-type="applicant" sequence="1">
          <addressbook lang="en">
            <orgname>Texas Instruments Incorporated</orgname>
            <address>
              <address-1>Dallas, TX, US</address-1>
              <city>Dallas</city>
              <state>TX</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </applicant>
        <applicant data-format="questel" app-type="applicant" sequence="2">
          <addressbook lang="en">
            <orgname>TEXAS INSTRUMENTS</orgname>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </applicant>
      </applicants>
      <inventors>
        <inventor data-format="original" sequence="1">
          <addressbook lang="en">
            <name>Oh, Stephen S.</name>
            <address>
              <address-1>Richardson, TX, US</address-1>
              <city>Richardson</city>
              <state>TX</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
        <inventor data-format="original" sequence="2">
          <addressbook lang="en">
            <name>Popik, Stephen Ira</name>
            <address>
              <address-1>Plano, TX, US</address-1>
              <city>Plano</city>
              <state>TX</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
      </inventors>
      <agents>
        <agent sequence="1" rep-type="agent">
          <addressbook lang="en">
            <name>Marshall, Jr., Robert D.</name>
          </addressbook>
        </agent>
        <agent sequence="2" rep-type="agent">
          <addressbook lang="en">
            <name>Brady, III, W. James</name>
          </addressbook>
        </agent>
        <agent sequence="3" rep-type="agent">
          <addressbook lang="en">
            <name>Telecky, Jr., Frederick J.</name>
          </addressbook>
        </agent>
      </agents>
    </parties>
    <examiners>
      <primary-examiner>
        <name>Dorvil, Richemond</name>
      </primary-examiner>
    </examiners>
    <lgst-data>
      <lgst-status>EXPIRED</lgst-status>
    </lgst-data>
  </bibliographic-data>
  <abstract format="original" lang="en" id="abstr_en">
    <p id="P-EN-00001" num="00001">
      <br/>
      This invention (10, 50) uses an interface (12, 52) to receive a voice input from a user, and a speech recognition unit (18, 54) coupled to the interface (12, 52) to monitor the voice input and recognize a predetermined set of voice commands from the voice input.
      <br/>
      The speech recognition unit (18, 54) generates a command signal that corresponds to the recognized voice command, which is received by a controller unit (20, 58).
      <br/>
      The controller unit (20, 58) activates a speech acquisition unit (16, 56) coupled to the controller unit (20, 58) to collect and stop collecting the voice input in response to a control signal generated by the controller unit (20, 58). a memory (24, 56) is provided to store the collected voice input.
    </p>
  </abstract>
  <description format="original" lang="en" id="desc_en">
    <p num="1">
      This applications claims benefit to U.S. provisional application Ser.
      <br/>
      No. 60/032,507 filed Dec. 3, 1996.
    </p>
    <heading>TECHNICAL FIELD OF THE INVENTION</heading>
    <p num="2">
      This invention is related in general to the field of personal electronic systems.
      <br/>
      More particularly, the invention is related to a hands-free audio memo system and method for making the same.
    </p>
    <heading>BACKGROUND OF THE INVENTION</heading>
    <p num="3">
      It is common knowledge that we are currently living in the Information Age.
      <br/>
      Data come to us in visual, audio, and written forms through a myriad of channels: radio, telecommunications, television, internet, world wide web, and just plain seeing, hearing, and feeling things as events occur around us.
      <br/>
      There are many instances when it is desirable to retain some of the information in a more reliable manner than the ability or inability to recall data we are born with, for example, that telephone number announced on the radio, the location of that specialty store, or that ingenious idea about a novel gadget to solve a stubborn problem.
    </p>
    <p num="4">
      The old standby to record data is the pen and paper.
      <br/>
      However, there are times when it is inconvenient to write, such as when one is operating an automobile, or when pen and paper are not accessible.
    </p>
    <p num="5">
      Dictaphones, which use audio tape cassettes, and some newer digital recorders, have been used to fill this void.
      <br/>
      However, they all require the use of at least one hand to hold the device, and to operate the many buttons on the device to turn on the device, record, retrieve, erase, and turn off the device.
      <br/>
      Further, because it has been shown that the use of one hand to handle a wireless telephone while operating an automobile can lead to unsafe driving and possibly higher incidents of traffic accidents, it is less than desirable to also require the driver to devote the use of one hand to operate the recording device.
    </p>
    <heading>SUMMARY OF THE INVENTION</heading>
    <p num="6">Accordingly, there is a need for an audio memo system and method therefor which enable hands-free operations.</p>
    <p num="7">In accordance with the present invention, a hands-free audio memo system and method are provided which eliminate or substantially reduce the disadvantages associated with prior devices.</p>
    <p num="8">
      In one aspect of the invention, an hands-free audio memo system and method uses an interface to receive a voice input from a user, and a speech recognition unit coupled to the interface to monitor the voice input and recognize a predetermined set of voice commands from the voice input.
      <br/>
      The speech recognition unit generates a command signal that corresponds to the recognized voice command, which is received by a controller unit.
      <br/>
      The controller unit activates a speech acquisition unit coupled to the controller unit to collect and stop collecting the voice input in response to a control signal generated by the controller unit.
      <br/>
      A memory is provided to store the collected voice input.
    </p>
    <p num="9">
      In another aspect of the invention, an hands-free personal memo system includes an analog interface receiving a voice input from a user, a speech recognition unit coupled to the interface adapted for receiving the voice input therefrom, recognize a predetermined set of voice commands from the voice input, and generating a command signal in response thereto.
      <br/>
      A controller unit is coupled to the speech recognition unit which generates a control signal in response to receiving the command signal from the speech recognition unit.
      <br/>
      A digital telephone answering device is coupled to the controller unit and analog interface for collecting and storing the voice input.
    </p>
    <p num="10">In yet another aspect of the invention, a method for hands-free audio memo includes the steps of receiving a voice input from a user, recognizing a voice commands in the voice input indicative of the user's desire to record an audio memo, collecting subsequent voice input, and storing the subsequent voice input.</p>
    <p num="11">
      Hands-free audio memo system and method therefor of the present invention provide a way for users to record audio memos and perform other functions without the use of a hand for its operation.
      <br/>
      This is especially advantageous for persons who are operating an automobile or performing other tasks that require concentration and generally the use of both hands.
    </p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading>
    <p num="12">
      For a better understanding of the present invention, reference may be made to the accompanying drawings, in which:
      <br/>
      FIG. 1 is a simplified functional block diagram of an exemplary hands-free audio memo system constructed according to the teachings of the present invention;
      <br/>
      FIG. 2 is a simplified block diagram of an alternative embodiment of the hands-free audio memo system of the present invention;
      <br/>
      FIG. 3 is an exemplary flowchart of a simplified hands-free audio memo algorithm according to the teachings of the present invention;
      <br/>
      FIG. 4 is an exemplary flowchart of an hands-free audio memo algorithm according to the teachings of the present invention;
      <br/>
      FIG. 5 is an exemplary flowchart of voice playback and memo management functions of the hands-free audio memo algorithm according to the teachings of the present invention; and
      <br/>
      FIG. 6 is an exemplary flowchart showing exemplary voice inputs to the system according to the teachings of the present invention.
    </p>
    <heading>DETAILED DESCRIPTION OF THE INVENTION</heading>
    <p num="13">The preferred embodiment(s) of the present invention is (are) illustrated in FIGS. 1-6, like reference numerals being used to refer to like and corresponding parts of the various drawings.</p>
    <p num="14">
      Referring to FIG. 1, a functional block diagram of an exemplary hands-free audio memo system 10 constructed according to the teachings of the present invention is shown.
      <br/>
      System 10 includes an analog interface 12 which receives voice input of a user captured by a microphone 14, and converts the analog voice input into a digital voice input signal.
      <br/>
      Analog interface 12 is further coupled to a speech acquisition unit 16, which functions to collect the digital voice input signal.
      <br/>
      The collected digital voice input signal is then provided to a speech recognition unit 18, which receives the digital voice input signal and searches for a set of predetermined voice commands and responses stored in a speaker-independent speech model memory 19 and/or an optional speaker-dependent speech model memory 21.
      <br/>
      For example, the voice command may be "MEMO START" or "TAKE MEMO" to initiate memo recording, "MEMO TERMINATE" to stop memo recording, and other appropriate responses.
      <br/>
      Further, certain commands and responses may be only valid during certain times and ignored at other times.
      <br/>
      For example, when memo recording is taking place, speech recognition unit 16 may only listen for a smaller set of commands and/or responses from the user, such as "MEMO TERMINATE," and not "YES" or "NO."
    </p>
    <p num="15">
      Speech recognition unit 18 is further coupled to a controller unit or microcontroller unit (MCU) 20. When speech recognition unit 16 recognizes a valid command or response, it generates a signal to inform controller unit 20 to take appropriate actions.
      <br/>
      Controller unit 20 is further coupled to a speech compression unit 22, which is also coupled to speech acquisition unit 16.
      <br/>
      Speech compression unit 22 compresses the digital voice input signals collected by speech acquisition unit 16 using known compression algorithms and stores the compressed signals into a memory 24.
    </p>
    <p num="16">
      A speech decompression unit 26 and a speech synthesis unit 27 are further coupled between controller unit 20 and analog interface 12.
      <br/>
      Controller unit 20 instructs speech compression unit 26 to decompress stored speech in memory 28 and provide to speech synthesis unit 27 to produce a speech prompt or response at appropriate times, which is then broadcast to the user by a speaker 30 coupled to analog interface 12.
    </p>
    <p num="17">
      Optionally, a communications link 31 may be provided to download voice input signals stored in memory 24 to a personal computer (not shown).
      <br/>
      In addition, a dialer 32 and link 34 may be further provided to a personal communications system (not shown) to perform functions related to telecommunications, such as dialing a particular number or "CALL HOME."
    </p>
    <p num="18">
      FIG. 2 is a simplified block diagram of an embodiment of hands-free audio memo system 50 according to the teachings of the present invention.
      <br/>
      System 50 includes an analog interface 52 coupled to a speech recognition unit 54 and a digital telephone answering device (DTAD) 56. DTAD 56 typically includes speech acquisition and compression functions, and a memory. a microcontroller unit 58 is further coupled to speech recognition unit 54 and DTAD 56.
    </p>
    <p num="19">
      System 50 may be implemented with commercially available components or devices.
      <br/>
      For example, interface 52 may be implemented with TCM320AC36 or TCM320AC37 Voice-Band Audio Processors (VBAP) (tm)  manufactured by Texas Instruments Incorporated of Dallas, Tex.; speech recognition unit 54 may be implemented with TMS320C5X Digital Signal Processor (DSP) also manufactured by Texas Instruments Incorporated; DTAD 56 may be implemented with the MSP58C8X product line of Texas Instruments Incorporated; and microcontroller unit 58 may be implemented with TMS370 family products of Texas Instruments Incorporated.
    </p>
    <p num="20">
      A single chip implementation is also contemplated by the present invention.
      <br/>
      For example, components in Texas Instrument's cDSP (tm)  product line may be incorporated and formed on a single silicon substrate to construct an integrated circuit.
      <br/>
      For example, a C54X core for performing the speech recognition and DTAD functions, an Advanced RISC (reduced instruction set computing) Machines (ARM (tm) ) 7TDMI core for performing the controller unit functions, and a Voice-Band Audio Processor core for performing analog interface functions may be combined into a single integrated circuit.
      <br/>
      It may be seen that the above are merely examples and other suitable substitutes may be used.
    </p>
    <p num="21">
      Referring to FIG. 3 as well as the block diagrams in FIGS. 1 and 2, an exemplary process flow 70 for hands-free audio memo systems 10 and 50 is provided.
      <br/>
      Speech acquisition 16 or DTAD 56 and recognition 16 or 54 is first activated in step 72.
      <br/>
      The activation may be done at the time the automobile (not shown) is started, by the push of a button, or by leaving the key in the accessory position, for example.
      <br/>
      In steps 72 and 74, speech recognition unit 18 or 54 searches for a valid command appropriate for the occasion, such as "MEMO START" to start the memo recording process.
      <br/>
      Once a valid command is recognized, as determined in step 76, controller unit 20 or 58 is notified, such as by a signal generated by speech recognition unit 18 or 54, as shown in step 78.
      <br/>
      Controller unit 20 or 58 then activates the memo function, as shown in step 80.
      <br/>
      Once the system is ready, an optional audio prompt or speech (e.g., "MEMO SYSTEM READY") may be generated in step 82 to signal to the user that he/she may begin to speak.
      <br/>
      A timer or counter (not shown) set for a predetermined time period may be started when speech acquisition 16 begins to capture voice input in step 84.
      <br/>
      The collected voice input is converted to digital signals, compressed and stored in memory 24, as shown in step 86.
      <br/>
      When the timer expires, speech acquisition is stopped, as shown in step 88.
      <br/>
      Controller unit 20 or 58 is then notified that memo recording terminated, as shown in step 90, and execution returns to step 74 to be ready for the next memo.
    </p>
    <p num="22">
      A second version of the method for hands-free audio memo 100 is shown in FIG. 4.
      <br/>
      Speech acquisition 16 or DTAD 56 and speech recognition 18 or 54 are activated either by starting the automobile, leaving the key in the accessory position, or the push of a button (not shown), for example, as shown in step 102.
      <br/>
      Speech recognition 18 or 54 monitors the speech uttered by the user(s) in the vicinity and searches for recognizable valid voice commands and/or responses, such as a command to start the memo process, as shown in step 104.
    </p>
    <p num="23">
      When it is determined that the captured voice input is a valid command, such as "MEMO START," controller unit 20 or 58 is notified, as shown in steps 106 and 108.
      <br/>
      Controller unit 20 or 58 in turn activates the memo function, as shown in step 110.
      <br/>
      In step 112, an audio prompt or speech (e.g., "MEMO SYSTEM READY") may be generated to signal to the user that he/she may begin to speak.
      <br/>
      The user's speech is then captured and compared with recognizable commands appropriate for the circumstances, such as "MEMO TERMINATE" to end the process, as shown in steps 114 and 116.
      <br/>
      Speech recognition 18 or 54 may be running in a low resource mode at this time to look for only those commands that are valid during this time, such as only the command to terminate or pause the memo taking process.
      <br/>
      If the captured utterance is not a recognizable and valid command, then it is collected, compressed, and stored, as shown in step 118.
      <br/>
      If in step 116, it is determined that the captured speech is a recognizable and valid command to end the memo process, for example, then controller unit 20 or 58 is notified, as shown in block 120.
      <br/>
      Controller unit 20 or 58 then pauses speech acquisition, as shown in step 122, and instructs speech decompression 26 and speech synthesis 27 to issue an audible prompt for confirmation, such as "READY TO TERMINATE MEMO?" The subsequent voice input is then captured and monitored for a valid response to the prompt, such as "YES" or "NO," as shown in steps 126 and 128.
      <br/>
      If the received voice input is not a recognizable valid response to the confirmation, then an appropriate audio response may be generated to reconfirm, as shown in step 132.
      <br/>
      If the voice input is recognized as a response indicative that the user is not ready to terminate the memo process, then execution returns to step 112, to continue to record memo.
      <br/>
      If on the other hand the voice input is recognized as an affirmative response in step 130, then the memo function is stopped in step 134, and controller unit 20 or 58 is notified in step 136.
      <br/>
      Execution then returns to step 104 to prepare for the next memo.
    </p>
    <p num="24">
      FIG. 5 is a flowchart of memo playback and memo management functions of system 10 and 50.
      <br/>
      At step 76 shown in FIG. 3 or step 106 shown in FIG. 4, if the voice input is not a valid start command, it is also checked for whether it is a valid playback command, as shown in step 140.
      <br/>
      If it is, controller unit 58 is notified in step 142 and the user is prompted for additional input, which is captured, as shown in step 144.
      <br/>
      The captured speech input is then examined to determined whether it is a valid response to the prompt given in step 144, if not, some appropriate action is taken in step 148, such as issue an appropriate audio statement.
      <br/>
      If it is a valid response, then the memo playback function 150 is launched, where the user may play back one or more previously recorded memos, skip one or more memos, etc.
      <br/>
      At the end of the memo playback function, the algorithm may return to step 74 in FIG. 3 or step 114 shown in FIG. 4.
    </p>
    <p num="25">
      If in step 140 it is determined that the speech input is not a valid playback command, then a determination is made as to whether it is a valid memo management command in step 152.
      <br/>
      If not, then the process may return to step 74 in FIG. 3 or step 114 shown in FIG. 4 to continue to capture the speech input.
      <br/>
      Otherwise, controller unit 58 is notified in step 154 and the user is prompted for additional input, which is captured, as shown in step 156.
      <br/>
      The captured speech input is then examined to determined whether it is a valid response to the prompt given in step 158, if not, some appropriate action is taken in step 148, such as issue an audio statement.
      <br/>
      If it is a valid response, then the memo management function 160 is launched, where the user may perform operations such as delete, save, and protect on previously recorded memos.
      <br/>
      At the end of the memo management function, the algorithm may return to step 74 in FIG. 3 or step 114 shown in FIG. 4.
    </p>
    <p num="26">
      Referring to FIG. 6, a more detailed process flow is shown.
      <br/>
      As voice input is captured in step 170, it is determined whether it matched any recognizable and valid command and response in step 172.
      <br/>
      For example, one or more recognized key phrases may be used to initiate system 50 in a memo recording mode 180, memo playback mode 182, memo management mode 184, dialer mode 186, and voice mail mode 188, where each mode is shown with exemplary valid phrases recognized when system 50 is in the respective modes.
      <br/>
      The key phrases to launch each mode may include "MEMO START" to launch the memo recording functions; "MEMO PLAYBACK" to launch the memo playback functions; "MEMO MANAGEMENT" to launch the memo management functions; "CALL X" to launch the dialer functions; and "GET MAIL" to launch the voice mail functions.
      <br/>
      Thus, speech recognition unit 54 need only to focus on a subset of possible valid utterances as to speed up search and processing time and to conserve resources.
    </p>
    <p num="27">Although the present invention and its advantages have been described in detail, it should be understood that various changes, substitutions and alterations can be made therein without departing from the spirit and scope of the invention as defined by the appended claims.</p>
  </description>
  <claims format="original" lang="en" id="claim_en">
    <claim num="1">
      <claim-text>What is claimed is:</claim-text>
    </claim>
    <claim num="5">
      <claim-text>5.</claim-text>
      <claim-text>A method for hands-free audio memo, comprising the steps of: receiving a voice input from a user; recognizing a memo start voice command in the voice input indicative of the user's desire to record an audio memo; collecting subsequent voice input; storing the subsequent voice input; storing a speech model of digital voice signal inputs corresponding to each of a plurality of voice commands; recognizing a voice command in the voice input indicative of a specific function by determining if said voice input matches any speech model corresponding to one of said specific functions;</claim-text>
      <claim-text>and following recognition of a voice command indicative of a specific function recognizing subsequent voice commands by attempting to match subsequent voice input to a subset of speech models of digital voice signals corresponding to valid commands following said specific function.</claim-text>
      <claim-text>1. A method for hands-free audio memo, comprising the steps of:</claim-text>
      <claim-text>receiving a voice input from a user; recognizing a memo start voice command in the voice input indicative of the user's desire to record an audio memo; collecting subsequent voice input; storing the subsequent voice input; recognizing a memo management voice command in the voice input indicative of the user's desire to manage stored voice input;</claim-text>
      <claim-text>and performing management functions in response to further voice input.</claim-text>
      <claim-text>2. The method, as set forth in claim 1, wherein: said memo management voice command includes a delete memo voice command;</claim-text>
      <claim-text>and said method further comprises deleting a current memo in response to a delete memo voice command.</claim-text>
      <claim-text>3. The method, as set forth in claim 1, wherein: said memo management voice command includes a save memo voice command;</claim-text>
      <claim-text>and said method further comprises saving a current memo in response to a save memo voice command.</claim-text>
      <claim-text>4. The method, as set forth in claim 1, wherein: said memo management voice command includes a protect memo voice command;</claim-text>
      <claim-text>and said method further comprises protecting a current memo in response to a protect memo voice command.</claim-text>
    </claim>
    <claim num="6">
      <claim-text>6. The method, as set forth in claim 5, wherein: said step of recognizing a voice command in the voice input indicative of a specific function includes attempting to match voice input to a first subset of said speech models.</claim-text>
    </claim>
    <claim num="7">
      <claim-text>7. The method as set forth in claim 6, wherein: said first subset of said speech models includes a speech model for each of the voice commands "CALL X", "MEMO PLAYBACK", "MEMO START", "GET MAIL" AND "MEMO MANAGEMENT"; said subset of speech models corresponding to "CALL X" includes the voice commands "HANG UP", "REDIAL", "YES" and "NO"; said subset of speech models corresponding to "MEMO PLAYBACK" includes the voice commands "PLAY MEMO", "SKIP MEMO", "YES" and "NO"; said subset of speech models corresponding to "MEMO START" includes the voice commands "MEMO PAUSE", "MEMO TERMINATE", "YES" and "NO"; said subset of speech models corresponding to "GET MAIL" includes the voice commands "EXIT MAIL", "NEXT MAIL", "YES" and "NO";</claim-text>
      <claim-text>and said subset of speech models corresponding to "MEMO MANAGEMENT" includes the voice commands "DELETE MEMO", "SAVE MEMO", "YES" and "NO".</claim-text>
    </claim>
  </claims>
</questel-patent-document>