<?xml version="1.0" encoding="UTF-8"?>
<questel-patent-document lang="en" date-produced="20180805" produced-by="Questel" schema-version="3.23" file="US06183259B2.xml">
  <bibliographic-data lang="en">
    <publication-reference publ-desc="Granted patent as second publication">
      <document-id>
        <country>US</country>
        <doc-number>06183259</doc-number>
        <kind>B2</kind>
        <date>20010206</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>US6183259</doc-number>
      </document-id>
    </publication-reference>
    <original-publication-kind>B2</original-publication-kind>
    <application-reference family-id="46253097" extended-family-id="58659398">
      <document-id>
        <country>US</country>
        <doc-number>09184433</doc-number>
        <kind>A</kind>
        <date>19981102</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>1998US-09184433</doc-number>
      </document-id>
      <document-id data-format="questel_Uid">
        <doc-number>68290162</doc-number>
      </document-id>
    </application-reference>
    <language-of-filing>en</language-of-filing>
    <language-of-publication>en</language-of-publication>
    <priority-claims>
      <priority-claim kind="national" sequence="1">
        <country>US</country>
        <doc-number>18443398</doc-number>
        <kind>A</kind>
        <date>19981102</date>
        <priority-active-indicator>N</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="1">
        <doc-number>1998US-09184433</doc-number>
      </priority-claim>
      <priority-claim kind="national" sequence="2">
        <country>US</country>
        <doc-number>68443596</doc-number>
        <kind>A</kind>
        <date>19960719</date>
        <priority-linkage-type>1</priority-linkage-type>
        <priority-active-indicator>Y</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="2">
        <doc-number>1996US-08684435</doc-number>
      </priority-claim>
      <priority-claim kind="national" sequence="3">
        <country>US</country>
        <doc-number>37561695</doc-number>
        <kind>A</kind>
        <date>19950120</date>
        <priority-linkage-type>C</priority-linkage-type>
        <priority-active-indicator>Y</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="3">
        <doc-number>1995US-08375616</doc-number>
      </priority-claim>
      <priority-claim kind="international" sequence="4">
        <country>WO</country>
        <doc-number>US9600919</doc-number>
        <kind>A</kind>
        <date>19960122</date>
        <priority-linkage-type>W</priority-linkage-type>
        <priority-active-indicator>N</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="4">
        <doc-number>1996WO-US00919</doc-number>
      </priority-claim>
    </priority-claims>
    <dates-of-public-availability>
      <publication-of-grant-date>
        <date>20010206</date>
      </publication-of-grant-date>
    </dates-of-public-availability>
    <term-of-grant>
      <disclaimer/>
    </term-of-grant>
    <classifications-ipcr>
      <classification-ipcr sequence="1">
        <text>A63B   9/00        20060101A I20051110RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>A</section>
        <class>63</class>
        <subclass>B</subclass>
        <main-group>9</main-group>
        <subgroup>00</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051110</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="2">
        <text>G09B   9/00        20060101A I20051110RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>09</class>
        <subclass>B</subclass>
        <main-group>9</main-group>
        <subgroup>00</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051110</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="3">
        <text>G09B  19/00        20060101A I20051110RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>09</class>
        <subclass>B</subclass>
        <main-group>19</main-group>
        <subgroup>00</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051110</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="4">
        <text>A63F   7/00        20060101A N20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>A</section>
        <class>63</class>
        <subclass>F</subclass>
        <main-group>7</main-group>
        <subgroup>00</subgroup>
        <classification-value>N</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="5">
        <text>A63F   7/06        20060101A N20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>A</section>
        <class>63</class>
        <subclass>F</subclass>
        <main-group>7</main-group>
        <subgroup>06</subgroup>
        <classification-value>N</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
    </classifications-ipcr>
    <classification-national>
      <country>US</country>
      <main-classification>
        <text>434247000</text>
        <class>434</class>
        <subclass>247000</subclass>
      </main-classification>
      <further-classification sequence="1">
        <text>345156000</text>
        <class>345</class>
        <subclass>156000</subclass>
      </further-classification>
      <further-classification sequence="2">
        <text>345629000</text>
        <class>345</class>
        <subclass>629000</subclass>
      </further-classification>
      <further-classification sequence="3">
        <text>434307000R</text>
        <class>434</class>
        <subclass>307000R</subclass>
      </further-classification>
      <further-classification sequence="4">
        <text>434365000</text>
        <class>434</class>
        <subclass>365000</subclass>
      </further-classification>
      <further-classification sequence="5">
        <text>463003000</text>
        <class>463</class>
        <subclass>003000</subclass>
      </further-classification>
      <further-classification sequence="6">
        <text>463004000</text>
        <class>463</class>
        <subclass>004000</subclass>
      </further-classification>
      <further-classification sequence="7">
        <text>482902000</text>
        <class>482</class>
        <subclass>902000</subclass>
      </further-classification>
    </classification-national>
    <patent-classifications>
      <patent-classification sequence="1">
        <classification-scheme office="EP" scheme="CPC">
          <date>20140902</date>
        </classification-scheme>
        <classification-symbol>A63F-013/42</classification-symbol>
        <section>A</section>
        <class>63</class>
        <subclass>F</subclass>
        <main-group>13</main-group>
        <subgroup>42</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20160507</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="2">
        <classification-scheme office="EP" scheme="CPC">
          <date>20140902</date>
        </classification-scheme>
        <classification-symbol>A63F-013/285</classification-symbol>
        <section>A</section>
        <class>63</class>
        <subclass>F</subclass>
        <main-group>13</main-group>
        <subgroup>285</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20160507</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="3">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>A63F-013/52</classification-symbol>
        <section>A</section>
        <class>63</class>
        <subclass>F</subclass>
        <main-group>13</main-group>
        <subgroup>52</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20160507</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="4">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>A63F-2300/6607</classification-symbol>
        <section>A</section>
        <class>63</class>
        <subclass>F</subclass>
        <main-group>2300</main-group>
        <subgroup>6607</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130821</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="5">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>A63F-2300/8041</classification-symbol>
        <section>A</section>
        <class>63</class>
        <subclass>F</subclass>
        <main-group>2300</main-group>
        <subgroup>8041</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130821</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="6">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>A63F-2300/8052</classification-symbol>
        <section>A</section>
        <class>63</class>
        <subclass>F</subclass>
        <main-group>2300</main-group>
        <subgroup>8052</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130821</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="7">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>G09B-019/22</classification-symbol>
        <section>G</section>
        <class>09</class>
        <subclass>B</subclass>
        <main-group>19</main-group>
        <subgroup>22</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130821</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="8">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>Y10S-482/902</classification-symbol>
        <section>Y</section>
        <class>10</class>
        <subclass>S</subclass>
        <main-group>482</main-group>
        <subgroup>902</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130518</date>
        </action-date>
      </patent-classification>
    </patent-classifications>
    <number-of-claims>85</number-of-claims>
    <exemplary-claim>1</exemplary-claim>
    <figures>
      <number-of-drawing-sheets>36</number-of-drawing-sheets>
      <number-of-figures>37</number-of-figures>
      <image-key data-format="questel">US6183259</image-key>
    </figures>
    <invention-title format="original" lang="en" id="title_en">Simulated training method using processing system images, idiosyncratically controlled in a simulated environment</invention-title>
    <references-cited>
      <citation srep-phase="examiner">
        <patcit num="1">
          <text>BIZZI EMILIO, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5554033</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5554033</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="2">
          <text>CARMEIN DAVID E E</text>
          <document-id>
            <country>US</country>
            <doc-number>5562572</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5562572</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="3">
          <text>JARVIK ROBERT</text>
          <document-id>
            <country>US</country>
            <doc-number>5577981</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5577981</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="4">
          <text>KATAYAMA MUNEOMI</text>
          <document-id>
            <country>US</country>
            <doc-number>5857855</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5857855</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="5">
          <text>IANNAZO DENNIS J, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5882204</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5882204</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="6">
          <text>JOSEPH MAZZA</text>
          <document-id>
            <country>US</country>
            <doc-number>2153384</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US2153384</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="7">
          <text>MCCOLLOUGH GEORGE T, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>3408750</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US3408750</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="8">
          <text>TARNOPOLSKY I, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>3729837</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US3729837</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="9">
          <text>NATER ROBERT A, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>3898438</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US3898438</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="10">
          <text>VALENTINE CHARLES G</text>
          <document-id>
            <country>US</country>
            <doc-number>4003140</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4003140</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="11">
          <text>MITSUHASHI KANZI, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4021651</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4021651</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="12">
          <text>JERNSTROM HANS EBBE INGEVAR, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4085540</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4085540</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="13">
          <text>KAISER ARTHUR, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4179704</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4179704</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="14">
          <text>CROMARTY JOHN I</text>
          <document-id>
            <country>US</country>
            <doc-number>4304406</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4304406</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="15">
          <text>STUBBEN DAVID R, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4324401</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4324401</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="16">
          <text>CONNELLY EDWARD M</text>
          <document-id>
            <country>US</country>
            <doc-number>4337049</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4337049</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="17">
          <text>STERN GARLAND</text>
          <document-id>
            <country>US</country>
            <doc-number>4600919</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4600919</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="18">
          <text>YASUDA YOSHINORI, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4615526</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4615526</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="19">
          <text>RIVKIN EFIM</text>
          <document-id>
            <country>US</country>
            <doc-number>4688792</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4688792</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="20">
          <text>DENNIS LARRY W</text>
          <document-id>
            <country>US</country>
            <doc-number>4734038</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4734038</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="21">
          <text>PETERSON DEAN M, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4752764</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4752764</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="22">
          <text>ROI DU MAROC II SA MAJESTE H</text>
          <document-id>
            <country>US</country>
            <doc-number>4805631</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4805631</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="23">
          <text>SEIDEL GARY E, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4828500</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4828500</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="24">
          <text>MANN RALPH V</text>
          <document-id>
            <country>US</country>
            <doc-number>4891748</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4891748</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="25">
          <text>WINN DONALD G, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4941660</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4941660</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="26">
          <text>VOHNOUT VINCENT J</text>
          <document-id>
            <country>US</country>
            <doc-number>4984986</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4984986</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="27">
          <text>WILDE MARK S</text>
          <document-id>
            <country>US</country>
            <doc-number>5100138</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5100138</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="28">
          <text>BREGMAN WALTER, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5118112</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5118112</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="29">
          <text>MANN RALPH V</text>
          <document-id>
            <country>US</country>
            <doc-number>5184295</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5184295</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="30">
          <text>FEOKHARI VALERY</text>
          <document-id>
            <country>US</country>
            <doc-number>5224710</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5224710</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="31">
          <text>O'LEARY GEORGE P, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5249967</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5249967</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="32">
          <text>DE GYARFAS VICTOR S, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5286202</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5286202</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="33">
          <text>CAGLE DAVID G</text>
          <document-id>
            <country>US</country>
            <doc-number>5312109</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5312109</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="34">
          <text>BAUM DAVID R</text>
          <document-id>
            <country>US</country>
            <doc-number>5320538</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5320538</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="35">
          <text>BURNS LES</text>
          <document-id>
            <country>US</country>
            <doc-number>5344323</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5344323</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="36">
          <text>LARSON NOBLE G, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5363297</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5363297</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="37">
          <text>CROMARTY JOHN I</text>
          <document-id>
            <country>US</country>
            <doc-number>5419562</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5419562</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="38">
          <text>STEWART LARRY D, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5443260</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5443260</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="39">
          <text>HOLLAND SIMON, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5454722</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5454722</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="40">
          <text>BAKER RICK</text>
          <document-id>
            <country>US</country>
            <doc-number>5486001</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5486001</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="41">
          <text>REYNOLDS JULIA, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5513991</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5513991</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="42">
          <text>RIESS THOMAS</text>
          <document-id>
            <country>US</country>
            <doc-number>5597309</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5597309</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="43">
          <text>KUNII TOSIYASU, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5625577</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5625577</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="44">
          <text>SAKIYAMA TOMOKO, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5659764</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5659764</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="45">
          <text>KIM CHARLES HONGCHUL</text>
          <document-id>
            <country>US</country>
            <doc-number>5694340</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5694340</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="46">
          <text>GEVINS ALAN, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5724987</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5724987</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="47">
          <text>SEVER JR FRANK</text>
          <document-id>
            <country>US</country>
            <doc-number>5736986</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5736986</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="48">
          <text>NAFIS CHRISTOPHER ALLEN, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5740802</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5740802</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="49">
          <text>CASSILY JAMES F, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5743744</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5743744</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="50">
          <text>LUBELL ALAN, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5797805</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5797805</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="51">
          <text>GILLIO ROBERT G</text>
          <document-id>
            <country>US</country>
            <doc-number>5800178</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5800178</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="52">
          <text>BAILEY BRADFORD E</text>
          <document-id>
            <country>US</country>
            <doc-number>5800179</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5800179</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="53">
          <text>KOZAK EDWARD J, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5803745</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5803745</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="54">
          <text>BRUDNY JOSEPH, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5810747</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5810747</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="55">
          <text>IKUTA JUNICHI</text>
          <document-id>
            <country>US</country>
            <doc-number>5813864</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5813864</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="56">
          <text>EASTERBROOK NORMAN JOHN</text>
          <document-id>
            <country>US</country>
            <doc-number>5823786</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5823786</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="57">
          <text>STRONG SIMON DAVID</text>
          <document-id>
            <country>WO</country>
            <doc-number>9004848</doc-number>
            <kind>A1</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>WO9004848</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="58">
          <text>BAKER RICHARD JOHN</text>
          <document-id>
            <country>WO</country>
            <doc-number>9221412</doc-number>
            <kind>A1</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>WO9221412</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="59">
          <text>MACRI VINCENT J, et al</text>
          <document-id>
            <country>WO</country>
            <doc-number>9622134</doc-number>
            <kind>A2</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>WO9622134</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="60">
          <text>MASSACHUSETTS INST TECHNOLOGY, et al</text>
          <document-id>
            <country>WO</country>
            <doc-number>9601138</doc-number>
            <kind>A1</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>WO9601138</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="1">
          <text>Okui et al., "Locus Display of Moving Sports Players," SMPTE J., 96(7):667-673 (Jul. 1987).</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="2">
          <text>International Search Report, PCT/US96/00919 dated Aug. 6, 1996.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="3">
          <text>Alciatore et al, "Matrix Solution of Digitized Planar Human Body Dynamics For Biomechanics Laboratory Instruction," 1992, Computers In Engineering, vol. 2, pp. 271-276.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="4">
          <text>Fauvet et al, "Human Movement Analysis With Image Processing In Real Time," 1990, SPIE vol. 1358, pp. 620-630.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="5">
          <text>Morasso et al, "Kinesis: A Model-Driven Approach To Human Motion Analysis," 1990, SPIE vol. 1395, pp. 775-780.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="6">
          <text>Baca, A. "Application of Image Processing to Human Motion Analysis" SPIE, vol. 1135, pp. 153-156, 1989.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="7">
          <text>Rab, G.T. "Application of Three-Dimensional Videography To Human Motion Studies: constraints, assumptions and mathematics," SPIE, vol. 832, 1987, pp. 31-34.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="8">
          <text>Burnie et al, "Microcomputing In Motion Analysis," Journal of Microcomputer Applications, (1987) 10, 113-117.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="9">
          <text>Rong et al, "Human Body Motion Image Analysis System" IEEE, 11th International Conference, 1953.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="10">
          <text>Kepple et al, "The Presentation and Evaluation of a Video Based Six-Degree-of-Freedom Approach For Analyzing Human Motion," IEEE 10th Annual Conference, 1988.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="11">
          <text>Andriacchi, T.P. "Clinical Applications of the SelSpot System" AMD (Symposia Series) ASME, 1987, pp. 339-342.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="12">
          <text>DeLuzio et al, "A Procedure To Validate Three-Dimensional Motion Assessment Systems," J. Biomechanics, vol. 26 No. 6, pp. 753-759, 1992.</text>
        </nplcit>
      </citation>
    </references-cited>
    <related-documents>
      <continuation>
        <relation>
          <parent-doc>
            <document-id>
              <country>US</country>
              <doc-number>68443596</doc-number>
              <kind>A</kind>
              <date>19960719</date>
            </document-id>
          </parent-doc>
        </relation>
        <relation>
          <parent-doc>
            <document-id>
              <country>US</country>
              <doc-number>5890906</doc-number>
              <kind>A</kind>
            </document-id>
          </parent-doc>
        </relation>
      </continuation>
      <related-publication>
        <document-id>
          <country>US</country>
          <doc-number>37561695</doc-number>
          <kind>A</kind>
          <date>19950120</date>
        </document-id>
      </related-publication>
      <continuation-in-part>
        <relation>
          <parent-doc>
            <document-id>
              <country>WO</country>
              <doc-number>PCT/US96/00919</doc-number>
              <date>19960122</date>
            </document-id>
          </parent-doc>
        </relation>
      </continuation-in-part>
    </related-documents>
    <parties>
      <applicants>
        <applicant data-format="original" app-type="applicant" sequence="1">
          <addressbook lang="en">
            <name>Macri, Vincent J.</name>
            <address>
              <address-1>Durham, NH, US</address-1>
              <city>Durham</city>
              <state>NH</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </applicant>
      </applicants>
      <inventors>
        <inventor data-format="original" sequence="1">
          <addressbook lang="en">
            <name>Macri, Vincent J.</name>
            <address>
              <address-1>Durham, NH, 03824, US</address-1>
              <city>Durham</city>
              <state>NH</state>
              <postcode>03824</postcode>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
        <inventor data-format="original" sequence="2">
          <addressbook lang="en">
            <name>Zilber, Paul</name>
            <address>
              <address-1>Plainview, NY, US</address-1>
              <city>Plainview</city>
              <state>NY</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
      </inventors>
    </parties>
    <examiners>
      <primary-examiner>
        <name>Cheng, Joe H.</name>
      </primary-examiner>
    </examiners>
    <pct-or-regional-filing-data>
      <document-id>
        <country>WO</country>
        <doc-number>US9600919</doc-number>
        <date>19960122</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>1996WO-US00919</doc-number>
      </document-id>
    </pct-or-regional-filing-data>
    <pct-or-regional-publishing-data>
      <document-id>
        <country>WO</country>
        <doc-number>9622134</doc-number>
        <kind>A2</kind>
        <date>19960725</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>WO9622134</doc-number>
      </document-id>
    </pct-or-regional-publishing-data>
    <lgst-data>
      <lgst-status>EXPIRED</lgst-status>
    </lgst-data>
  </bibliographic-data>
  <abstract format="original" lang="en" id="abstr_en">
    <p id="P-EN-00001" num="00001">
      <br/>
      The present invention pertains to a method and apparatus for simulated training in physical movement using processing system images.
      <br/>
      Said images may be demonstrative images representing examples of any physical movement, user images that are idiosyncratically controlled by individual users, and/or any combination or separate display of both such images.
      <br/>
      Simulated training is directed to the personal, interactive control of displayed user controllable images such that the user acquires a level of know-how related to the physical movement of any particular image.
      <br/>
      Embodiments of the invention could include the physical movement of animals, bacteria, viruses, objects, living, mechanical and electromechanical or other systems and images from live video taping, or animated or computer generated images or live or robots real time images from any other source.
    </p>
  </abstract>
  <description format="original" lang="en" id="desc_en">
    <heading>RELATED APPLICATION</heading>
    <p num="1">
      This application is a continuation of U.S. patent application Ser.
      <br/>
      No. 08/684,435, filed on Jul. 19, 1996, now U.S. Pat. No. 5,890,906 which is a continuation-in-part of U.S. patent application Ser.
      <br/>
      No. 08/375,616, filed on Jan. 20, 1995, now abandoned, and a continuation-in-part of PCT International application Ser.
      <br/>
      No. PCT/US96/00919, filed on Jan. 22, 1996, both of which are incorporated herein in their entirety by reference.
    </p>
    <heading>BACKGROUND OF THE INVENTION</heading>
    <p num="2">
      Technology that has been designed for sports and other activities involving motor skills is generally dedicated to improving strength, swing, motion, balance, speed, and agility.
      <br/>
      Accordingly, for golf and other sports, swinging motion trajectory is emphasized.
      <br/>
      For the most part the focus has been on developing aids for the individual athlete to improve a particular physical skill.
      <br/>
      Relatedly, technology for hazardous or potentially jeopardizing work, such as airplane piloting, is generally directed to practice in a simulated environment.
      <br/>
      In both areas mentioned above (motor skills and dangerous or jeopardizing work) the emphasis is on the individual taking action, i.e., making movements that imitate real circumstances.
      <br/>
      The prior art related to sports and physical actions is generally aided by technology directed to biomechanics and locomotion, i.e., to executing motor skills.
    </p>
    <p num="3">
      There is little technology that is used for instruction or simulation in playing sports such as ice hockey.
      <br/>
      Commonplace instructional aides include stopwatches, plastic orange cones (used for skating drills), parachutes (pulled to increase leg strength), 2" * 4" boards (used for jumping in agility, speed, and balance drills), and surgical tubing harnesses (to increase strength and force the skater to skate while bent forward at the waist).
      <br/>
      None of the devices noted above provide the student with instructive and cognitive information concerning principles, tactics, maneuvers, skills and strategies used in physical activities such as playing ice hockey.
      <br/>
      In addition, these devices do not enable the player to receive quantitative feed-back concerning his/her progress in learning and mastering the cognitive aspects of the sport.
    </p>
    <heading>SUMMARY OF THE INVENTION</heading>
    <p num="4">
      The present invention provides a method and an apparatus for interactive, tutorial, self and assisted instruction and simulated preparation, training and competitive play and entertainment directed toward improving comprehension, exercising cognitive functions and providing amusement.
      <br/>
      The method and apparatus provide for user interaction with images on the screen of a computer monitor, or other viewing device, containing multimedia, interactive, reconfigurable, audio-visual, tacticle systems.
    </p>
    <p num="5">
      The present invention provides a user with the opportunity to engage in simulated training or game playing comprised of instructive or ideal images and/or adoptable, controllable images.
      <br/>
      Such images provide the means by which the user can come to understand what to do using his/her cognitive and visualization skills.
      <br/>
      The use of such skills typically precedes the use of motor or motion skills.
    </p>
    <p num="6">
      In one embodiment, as the program executes, an instructive or ideal image displays the preferred style, technique, posture, procedure, skills, drills, positions, maneuvers, tactics, strategies or plays of any activity that calls for using cognitive and motor functions.
      <br/>
      The user's cognitive functions are developed and/or improved by viewing the procedural methods used by an instructive or ideal image, shown on a portion of the screen.
      <br/>
      As the program further executes, the user's level of comprehension improves by: (1) receiving information and cues from visual, audio, textual and tactile transmissions, (2) visualizing the actions of the instructive or ideal image, (3) making cognitive choices and decisions as to what manipulations should be made, and then (4) manipulating a user controllable, adopted (by the user) image on another portion of the screen so as to cause the adopted image to simulate or approximate the preferred style, technique, posture, procedures, skills, drills, positions, maneuvers, tactics, strategies or play of any activity represented by the instructive or ideal image.
    </p>
    <p num="7">Alternatively, using the method and apparatus of the present invention, the user learns or is entertained by manipulating the controllable, adopted image(s) on a full or partial screen in competition against one or more instructive or ideal images or against other controllable images.</p>
    <p num="8">
      The present invention can use, for either the instructive or ideal image, or for the adopted, controllable image, or for both, images from live video taping, or animated or computer generated images or live, or robots real time images.
      <br/>
      It will be apparent to those skilled in the art that the instructive image(s) can teach at any level of expertise from first time beginners to professional experts.
      <br/>
      Feedback can be provided to all uses.
    </p>
    <p num="9">
      One embodiment pertains to a method of instruction in ice and roller hockey.
      <br/>
      More particularly, the embodiment pertains to a tutorial and interactive method for self and directed instruction and simulated training and play in a sport (or other activity) such as ice or roller hockey including, but not limited to, the principles of skating, stickhandling, shooting, checking, offensive and defensive play, positional and situational play, tactical and strategic plays and maneuvers.
    </p>
    <p num="10">
      The present invention includes observing an instructive (or ideal) way of doing something, receiving information and visual, audio, textual and tactile cues, visualizing what needs to be done and manipulating the adopted image to make sequential, positional changes pursuant to reviewing objectives and options and making choices in a dynamic decision process.
      <br/>
      This is a cognitive and to some extent a motor process that prepares the individual to engage in an activity coupling cognitive with motor skills.
      <br/>
      The present invention provides a simulated environment within which to prepare.
      <br/>
      The present invention combines perceptual experience and cognition with physical sensation and uses cues to facilitate user trial, error and improvement.
      <br/>
      The present invention facilitates the acquisition of accelerated albeit simulated experience.
    </p>
    <p num="11">
      By virtue of the facility available to users of the present invention, simulated training can be accomplished as well as simulated drill and maneuvers in taking up proper positions, reacting to situational play and executing tactics and strategies.
      <br/>
      Further, without a requirement for actual imitative movement of an instructive figure or its movement and without the requirement to use actual tools, implements or equipment, users of the present invention can engage in simulated repetition and/or competition geared to either learning or entertainment and can do so within the economy of space used for a computer monitor and keyboard or other economical, including virtual reality, input devices, (as shown in FIGS. 25 and 26).
    </p>
    <p num="12">
      The present invention provides facilities for users to engage in simulated training, repetition and entertainment in activities that couple cognitive and motor skills.
      <br/>
      Such facilities, using skating as an example, include a range of functions such as simply understanding proper positioning for "pushing off" (for example, starting from a stationary standing position while on ice) to sophisticated, preferred, proactive play and preferred reactive play in response to numerous and varied competitive situations.
      <br/>
      The present invention, by virtue of its capacity to simultaneously present pre-programmed images following a predetermined system of activities and/or images that are controllable adopted by the user also provides for engaging entertainment and amusement.
    </p>
    <p num="13">
      Using any form of communication of information, including but not limited to floppy disks, CD-ROM disks, a local network, the Internet or wireless communication, users can compete against single opponents, against prior stored images of any individuals (or animations), including themselves or robots and against a full or partial opposing team, group or force.
      <br/>
      The elements of use of the present invention, including but not limited to viewing preferred or opposing activity, visualizing what the user should do in response to the activity viewed, including identifying options and making choices and decisions and interacting with the present invention through adoptable, controllable images open up a host of imaginative applications and uses for teachers, coaches, trainers, game players and the general public.
    </p>
    <p num="14">An alternative embodiment of the present invention is to have dynamic interaction among all images on the screen so that users may adopt and control all images in a learning, competitive or entertainment oriented environment.</p>
    <p num="15">
      The present invention is particularly well-suited to providing to the user accelerated experience through simulation.
      <br/>
      The processes of observation, orientation, and judgment are facilitated by the present method and apparatus.
    </p>
    <p num="16">
      The present invention is not limited to visual sensors but can incorporate others including audio etc. as needed.
      <br/>
      The use of auditory, visual and other cues decreases distractions and improves comprehension.
      <br/>
      Using the present invention as a relatively economical training simulator, implementation of visualization in a simulated environment is facilitated.
      <br/>
      Utilizing pre-programmed, automated and/or user controllable images, the users' awareness of spatial relationships among and between images, objects and locations is enhanced.
      <br/>
      Dynamic interaction between the images adopted by users, acting in concert as teammates against opposing images as well as dynamic communication and coordination by and between users, whether athletes or not, and other users is also enhanced.
    </p>
    <p num="17">
      There are multiple opportunities to change the position of an individual or one's entire team, i.e. a group of adopted, controllable images in response to opposing positioning or formations.
      <br/>
      This can be accomplished using touch sensitive screens or other input devices and will lead to sharpened pre-training in individual and/or team technique.
      <br/>
      The present invention is applicable to activities other than sports and pastimes.
    </p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading>
    <p num="18">
      In the drawings, like reference characters generally refer to the same parts throughout the different views. Also, the drawings are not necessarily to scale, emphasis instead generally being placed upon illustrating the principles of the invention.
      <br/>
      FIG. 1 is a block diagram of a computer software system constructed according to the present invention.
      <br/>
      FIGS. 2A-2E contain flowcharts of the logical flow of the system of the present invention for preparation, processing of and interaction with video images.
      <br/>
      FIG. 3 is a pictorial diagram of a user interacting with one embodiment of the invention.
      <br/>
      FIG. 4 is a pictorial diagram of a user interacting with an alternative embodiment of the invention.
      <br/>
      FIGS. 5A and 5B contain pictorial diagrams of two users interacting with another alternative embodiment of the invention.
      <br/>
      FIG. 6 is a pictorial diagram of multiple users interacting with another alternative embodiment of the invention.
      <br/>
      FIG. 7 is a pictorial diagram of multiple users interacting with another alternative embodiment of the invention.
      <br/>
      FIG. 8 is a schematic pictorial diagram of another alternative embodiment of the invention used in conjunction with the worldwide Internet.
      <br/>
      FIGS. 9-32 are the pictorial diagrams of a user interacting with different embodiments of the invention.
    </p>
    <heading>DETAILED DESCRIPTION OF THE INVENTION</heading>
    <p num="19">
      One embodiment of the present invention provides a computer-based system for tutorial and interactive instruction in the sport of ice (or roller) hockey including, but not limited to, the principal activities of skating, stickhandling, shooting, checking and offensive and defensive play, positional and situational play, tactical and strategic plays and maneuvers.
      <br/>
      The system of the present invention is particularly well suited for use as a tutorial, self-study or assisted study aide and as a cognitive primer for performing the principal activities noted above.
      <br/>
      The system is also particularly well suited as an aid to instructors, trainers, coaches and officials or trainees at all levels of expertise.
    </p>
    <p num="20">
      Physical activities such as playing ice hockey (or roller hockey) can be hazardous.
      <br/>
      As with other hazardous or demanding endeavors, the practice of simulated action or the approximation of preferred action before actual execution is advisable.
      <br/>
      The use of computer assisted technology to enhance the amount of time to engage in simulated practice can be distinctly advantageous to the user's (the "player/student's") knowledge base.
      <br/>
      Unlike video and arcade games etc., which do not enable the game player to initiate or react to the actual dynamics of the real game, the present invention requires the player/student to review options, make choices and decisions and to cause the movement of an adopted, controllable image (in the form of, for example, a hockey player) as though he/she were in an actual game.
    </p>
    <p num="21">
      In the present invention the player/student is involved with hands-on, head-in activity, making actual choices and decisions and causing actual change of on-screen images in a virtually real environment under virtually real conditions.
      <br/>
      This type of control and initiative and the relevancy and accuracy of the simulation and the feedback, lead to a richer (robust) learning experience.
      <br/>
      Accordingly, the present invention also relates to a variety of sub-components of the principle activities noted above, such as, agility, balance, impact/contact, timing, and "read and react" skills.
    </p>
    <p num="22">
      The computer system of the present invention enables the player/student to see and hear (on a computer screen or other device) how a particular skating, stickhandling, checking or other skill or drill can be performed (as shown in FIGS. 16 and 17).
      <br/>
      The player/student, by observing an instructive or ideal image of the skill being performed, is presented with a clear example (as shown in FIGS. 11, 31 and 32).
      <br/>
      The player/student can then interact with the system by visualizing the example shown and actually cause an on-screen adopted image to perform the skill or drill by approximation.
      <br/>
      Feedback on the user's individual performance in comparison to the instructive or ideal pattern is visible in real time.
      <br/>
      Since feedback occurs in real time in the invention and since proficiency scores are posted on screen at the end of each drill, the invention may be used by anyone.
      <br/>
      Feedback can also be delayed and/or printed out in hard copy or distributed by other means.
    </p>
    <p num="23">
      For each preferred style, technique, posture, maneuver, position, tactic, skill, drill or play of any activity, the computer system of the present invention can provide an animated or digitized image of a player (or players) who performs the instructive or ideal performance (example).
      <br/>
      The instructive or ideal performance can be accompanied by textual and/or auditory tutorials.
      <br/>
      For some purposes the lesson may end at that point.
      <br/>
      For other purposes an interactive instruction can follow and for still other purposes a competitive or amusement scenario will ensue.
    </p>
    <p num="24">
      The interactive session can call upon the player/student to cause his/her adopted animated or digitized image(s) to execute maneuvers under the direction of the player/student.
      <br/>
      The present invention may use the ideal skater as is or may transform an animated or digitized ideal skater into an anatomical outline and provide that the outline have superimposed upon it or set beside it an "adopted" silhouetted figure which will make sequential positional changes at the direction of the player/student (as shown in FIGS. 9 and 29).
      <br/>
      This calls upon the player/student to understand the positional changes including the physical movements that are required by the maneuver and to strive to have his/her adopted image make those movements and sequential positional changes (as shown in FIGS. 20 and 30).
    </p>
    <p num="25">
      Since this interactive game can be played by pitting the adopted controllable image against the instructive or ideal image example, the difference between the execution by the player/student controlled image and ideal execution can be measured.
      <br/>
      Auditory and/or textual prompts can be given in real time as well as a score registered at the end of the exercise.
      <br/>
      For any given (predetermined) skating step, stride or maneuver, the player/student will have positive or negative feedback in both audio and visual form based on his/her ability to cause the controllable adopted image to approximate the drill being displayed.
      <br/>
      Advancements to the next higher level or repetition for correction are integral parts of the invention.
    </p>
    <p num="26">
      The applications of such instruction are numerous.
      <br/>
      Player/students will learn what they will be called upon to do on the ice.
      <br/>
      The opportunity for self-drill and repetition is excellent.
      <br/>
      Mental preparedness can be enhanced.
      <br/>
      Uniformity and economy in coaching and instruction can be enhanced.
      <br/>
      A lesson can be designed in advance, as can game plans.
      <br/>
      Computer usage (if not literacy) can be a learned by-product.
      <br/>
      Certain principles of geometry and physics can be learned in conjunction with understanding how efficient and effective execution of skills leads to enhanced results.
      <br/>
      For example, how tighter turns shorten the distance between present location and desired positions on the ice or how good balance, timing and form lead to increased skating speed and strength.
    </p>
    <p num="27">
      Benefits of the present invention include the opportunity to visualize an instructive or ideal image of specific skills and maneuvers used in ice hockey.
      <br/>
      The player/student can see how the skill or play can be executed as a dynamic whole (either in slow motion or actual speed) or can pick apart the skill or play by viewing images frame by frame (as shown in FIG. 19).
      <br/>
      The latter method enables the student to see details of the ideal maneuver which might not be apparent in a moving image.
    </p>
    <p num="28">
      The player/student is able to learn each skill at his/her own pace.
      <br/>
      Since images can be moved frame by frame and repeated as many times as desired, the player/student will be able to perfect his/her understanding and performance of each skill.
      <br/>
      Repeating each maneuver will improve recollection of what has been learned.
    </p>
    <p num="29">
      Feedback can be received in real time by the player/student as he/she is able to see on the computer screen how closely the adopted controllable image approximates the ideal imager (as shown in FIG. 12).
      <br/>
      In addition, the invention can provide means to numerically evaluate the player/student's ability to match the sequential positional changes and maneuvers of the ideal image.
      <br/>
      The quantitative evaluation will make it possible for the student to monitor his/her progress in performing the skill, play or tactic each time it is practiced.
    </p>
    <p num="30">
      Benefits to instructors include consistency and objectivity in the information given to each player/student.
      <br/>
      Each player/student will receive the same information and will be able to repeat the lesson or "game" until sufficient comprehension is acquired.
      <br/>
      Since player/students will be able to use the present invention independently, staff time can be quantitatively and qualitatively enhanced.
      <br/>
      Instructors and students will be able to select areas for more or less concentration and repetition, including what should be done in actual practice.
      <br/>
      Use of the present invention will also be of benefit if access to ice hockey rinks is limited or, conversely, more can be made of available ice time.
      <br/>
      Accident or injury reduction can result from increased awareness of dangerous situations or conditions.
    </p>
    <p num="31">
      An additional embodiment of the invention enables officials (referees, linesmen, and other officials) in the sport of ice hockey or in other sports or activities to practice positioning and making decisions regarding infractions of the rules while watching and controlling digitized images on a computer screen (as shown in FIG. 10).
      <br/>
      The digitized images include preselected plays or maneuvers executed by a single player against one or more opponents or by a player in combination with one or more of his/her teammates playing against an opposing team including one or more players or actions taken by team coaches, managers and others.
    </p>
    <p num="32">
      The officials view the execution of the play, while moving an "adopted" official, in digital form, to the best vantage point from which to see the plays, and decide whether the actions of the player or players or coaches are within the rules of the game or constitute an infraction of the rules for which a penalty is levied.
      <br/>
      The invention enables officials to record their decisions in the computer system.
    </p>
    <p num="33">
      The digitized images can be obtained from video tapes of live athletes playing ice hockey or from animated or computer generated or robotic sources.
      <br/>
      Appropriate sections of the video tape or other images can be selected and converted to or maintained in digitized format.
      <br/>
      The selected images can be viewed on a computer screen.
      <br/>
      Officials are able to speed up or slow down the images as they appear on the screen.
      <br/>
      Alternatively, the speed of on-screen action can be controlled by persons responsible for assessing the officials, performance.
      <br/>
      Officials can be required to move their adopted on-screen official image so as to be in the best position to see the play and make a call when warranted.
      <br/>
      Reverse angle views of the digitized images can be available on demand to provide real time feedback on whether a play included a foul or was cleanly executed.
    </p>
    <p num="34">
      After viewing each specific image sequence, the official can enter into the computer his/her decisions regarding the legality of the plays previously observed.
      <br/>
      The results are compiled by the computer and fed back to the officials via the computer screen or by hardcopy printouts.
    </p>
    <p num="35">
      The benefits of this embodiment include the opportunity for officials to practice decision making at their own pace in a relaxed environment.
      <br/>
      As with the previous embodiments of this invention, the images on the computer screen can be viewed frame by frame, or at any speed from slow motion to real time to super real time, and repeated as many times as desired.
      <br/>
      This will enable officials to study details which can be missed in the fast, high pressure setting of a hockey game or to test their skills in making calls at real time rates of speed or even faster (super) than actual playing speed.
      <br/>
      As the officials progress in their decision-making abilities, the images which are viewed at increasingly faster speeds, either real-time or faster, provide practice in quick orientation, observation and judgment and thereby sharpen officiating skills.
    </p>
    <p num="36">
      After each image sequence has been viewed, the official can use means such as a computer keyboard, mouse, joystick or other input device to input his/her decision regarding the legality of the observed plays into the computer system.
      <br/>
      The official is presented with a list of possible infractions and asked to select those infractions observed during the image sequence.
      <br/>
      The official gains points for correct judgments and lose points for incorrect judgments.
      <br/>
      At the end of the exercise, the computer can display a numeric score as well as a list of incorrect decisions.
      <br/>
      The score and list can also be printed in hardcopy format if desired.
    </p>
    <p num="37">
      If the user does not achieve a certain level of proficiency, he/she can be required to loop back and repeat testing on the image sequences previously viewed before advancing to a higher level.
      <br/>
      As with other embodiments of the invention, the computer can keep a record of the proficiency scores for each official.
      <br/>
      Current scores may be compared to those attained previously.
      <br/>
      This will enable each user to monitor his/her progress in perfecting his/her decision-making skills.
    </p>
    <p num="38">
      The purpose of this embodiment is to accelerate the experience and improve the positional and judgment skills of officials in ice hockey by increasing the number of decisions made while observing actual hockey plays and maneuvers.
      <br/>
      Officials obtain practice in orientation, positioning, observation and judgment.
      <br/>
      Officials can rehearse preferred positioning so as to decrease interfering with play.
      <br/>
      Consistency and objectivity will be increased.
      <br/>
      The rules of the game will be reinforced in the minds of the officials.
      <br/>
      In addition, use of this embodiment of the invention may also help students and players of ice hockey to learn the rules and regulations of the game.
    </p>
    <p num="39">
      FIG. 1 contains a block diagram of one embodiment of the computer system 200 of the invention for simulating physical activities such as skating and hockey maneuvers and for entertainment use according to the present invention.
      <br/>
      The system comprises a central processing unit (CPU) 202 which can be, but is not limited to, a microcomputer such as a personal computer operating with an Intel 80X86 microprocessor, a Motorola 680X0 processor, or a Power PC processor.
      <br/>
      The CPU 202 is coupled to a display 204, such as a CRT screen, for displaying images and prompting the user for the input of information.
      <br/>
      The CPU 202 is also coupled to an input device or devices 206 such as a keyboard, joystick, etc., for the input of information from the user and a printer 208 for printing information such as player/student accomplishment information in hard-copy media.
      <br/>
      Input and output with the user can also be accomplished with virtual reality devices 207 such as a virtual reality glove and helmet worn by the user.
      <br/>
      A memory 210 is provided, coupled to the CPU 202, for the storage of program instructions and data, recorded images and other information.
    </p>
    <p num="40">
      Several versions of computer programs are encompassed by the present invention.
      <br/>
      Each version varies in complexity as each is directed toward a player/student at a different level of expertise.
      <br/>
      Versions developed for home, rink, and arcade use are instructional as well as entertaining.
      <br/>
      More complex versions for use in structured training facilities are developed.
      <br/>
      For each version, the computer system stores a series of lessons and corresponding "ideal" images in a database.
    </p>
    <p num="41">
      FIGS. 2A-2E contain flowcharts which illustrate the logical flow of the computer system(s) used for the preparation and operation of the system of the present invention.
      <br/>
      FIG. 2A shows the procedure for scanning images into the computer system 200 and preparing them for display on the computer screen 204.
    </p>
    <p num="42">
      Steps 212 and 214, images for a particular skill or maneuver are scanned into the computer system 200.
      <br/>
      Individual drawings are created and scanned to complete the sequence of the instructive "master" or "ideal" hockey player performing the skill or maneuver as well as to encompass all possible movements that the player/student-controlled ("adopted") player may perform.
    </p>
    <p num="43">
      After all the instructive images in a given sequence have been scanned, the images can be prepared for display on the computer screen.
      <br/>
      This can be accomplished with a number of commercially available computer packages, including Photoshop, Photofinish, NeoPaint, Paint, Graphics Designer, and Correlpaint.
      <br/>
      In blocks 216, 218, 220, and 222, the images can be scaled or rotated, as necessary.
      <br/>
      If any images contain unwanted elements, they can be cropped in blocks 224 and 226.
      <br/>
      In blocks 228 and 230, the images can be refined to remove unwanted lines or blotches or to add lines that have faded during scanning.
      <br/>
      In blocks 232 and 234, the images can be enhanced by adding elements such as background or motion lines.
    </p>
    <p num="44">
      Each image is prepared in turn in the loop under control of blocks 236 and 238.
      <br/>
      After each image in the sequence has been prepared, it can be aligned with others on the computer screen in block 240.
      <br/>
      The preparation stage ensures that the images can be displayed as a smooth animation sequence when the program is run by users.
      <br/>
      All or any portion of a controllable figure can be displayed from any perspective.
    </p>
    <p num="45">
      In an alternative embodiment of the invention, images recorded by video tape can be used instead of the hardcopy drawings described above.
      <br/>
      In this case, video recordings are made of a particular skill or maneuver.
      <br/>
      Each frame of the video tape is digitized so that it can be stored in computer files using a standard such as JPEG (Joint Photographic Expert Group), MPEG (Motion Picture Experts Group), FLI, FLC, etc.
      <br/>
      The sequence of images is refined in connection with FIG. 2A. The result is a series of images in computer-readable form similar to those obtained from the hardcopy drawings described above.
      <br/>
      Images need not be exclusively drawn images or video images; mixed images may be used.
      <br/>
      Images can be obtained by videotaping an actual player performing a maneuver or by videotaping performance of the maneuver by preprogrammed robots such as the robots described in co-pending U.S. patent application Ser.
      <br/>
      No. 08/375,617, filed Jan. 20, 1995, entitled "Mechanized Robots for use in Instruction, Training and Practice in the Sport of Ice and Roller Hockey," by Vincent J. Macri, which is incorporated herein by reference.
      <br/>
      Polarized reflective markers can be attached to the player or the robot to enhance the video image.
      <br/>
      Alternatively, active motion sensors can be used.
    </p>
    <p num="46">
      FIG. 2B contains a flowchart showing the process of compressing the computerized files containing each sequence of drawings to conserve storage space.
      <br/>
      The process involves comparing each drawing to the next one in the sequence and recording only those pixel elements which have changed from one drawing to the next.
      <br/>
      Other compression techniques also exist and may be used.
    </p>
    <p num="47">
      In blocks 250 and 252, a sequence of images is selected, and the number of images in the sequence is determined.
      <br/>
      Beginning in block 254, with the initial image that would appear on the screen, each drawing is compared to the previous drawing in the sequence.
      <br/>
      In the case of the first drawing in the sequence, the comparison is made to an empty screen.
      <br/>
      Each pixel element which has changed from one drawing to the next is identified in block 256 and the change data is compressed in block 258.
      <br/>
      The compressed data is then stored in a sequence file in block 260.
      <br/>
      This procedure is repeated for each drawing in the sequence in blocks 262 and 264.
    </p>
    <p num="48">
      Referring to blocks 266 and 268, the relationship between each of the drawings is input in the form of a decision tree and recorded in a computer file.
      <br/>
      In the case of the sequence of the drawings representing the instructive master or ideal image, the decision tree is defined as a linear relationship progressing directly from the first frame to the last.
      <br/>
      In the case of the player/student controlled image, each frame is connected to a series of other frames representing the different ways in which the student can move the animated figure.
      <br/>
      The relationship between each interconnected image must be defined and stored.
      <br/>
      To make the definition of the decision tree easier, each part of the body which moves (such as an arm or leg) can be defined as a separate series of images.
      <br/>
      Composition of several partial images into a whole can provide the completed image.
    </p>
    <p num="49">
      FIGS. 2C through 2E illustrate the logical flow of the system of the present invention that is run by the player/student or user.
      <br/>
      In block 270 of FIG. 2C, the system displays an opening screen on the computer display which contains a title, logo, and welcome message.
      <br/>
      In block 272, the computer requests a user identification code (ID) and, in block 274, the player/student enters an ID into the computer via a keyboard, mouse or other user input device.
      <br/>
      In block 276, the system determines if the user has entered the system before.
      <br/>
      If the player/student has previously entered the system, information regarding his/her personal characteristics (e.g. name, identification code, weight, height, age, right or left handedness, level of expertise, team name, and coach/instructor's Name (as shown in FIGS. 27 and 28)) as well as his/her performance record can be retrieved from the database in block 278.
      <br/>
      If the player/student is new to the system, information regarding his/her personal characteristics can be recorded by the system in blocks 280 and 282.
      <br/>
      If the player/student has entered an invalid ID, the system can return to the input stage via block 274 after informing the user in block 284.
    </p>
    <p num="50">Following the identification process, the player/student is presented with, in block 286, a menu of general categories which describe the skills used in the sport of hockey, e.g.</p>
    <p num="51">
      (1) skating
      <br/>
      (2) checking
      <br/>
      (3) stickhandling
      <br/>
      (4) shooting
      <br/>
      (5) offensive play
      <br/>
      (6) defensive play
      <br/>
      (7) positional play
      <br/>
      (8) situational play
      <br/>
      (9) tactical play
      <br/>
      (10) maneuvers.
    </p>
    <p num="52">
      In block 288, the player/student selects the number of the category he/she wishes to study and/or practice.
      <br/>
      If the player/student chooses to press &lt;ESC&gt;, he/she will exit the system in block 290.
    </p>
    <p num="53">
      For each general category, the computer system stores a list or sub-menu of specific plays, maneuvers or skills which will develop the player/student's skills in the area selected.
      <br/>
      For example, under the category of skating, he/she may choose from the following options:
      <br/>
      (1) dash--forward
      <br/>
      (2) dash--backward
      <br/>
      (3) start
      <br/>
      (4) stop
      <br/>
      (5) forward left crossover
      <br/>
      (6) forward right crossover
      <br/>
      (7) backward left crossover
      <br/>
      (8) backward right crossover
    </p>
    <p num="54">
      The sub-menu for the general category selected by the player/student will be displayed in block 292, and the player/student selects the maneuver or skill he/she wishes to study and/or practice in block 294.
      <br/>
      If the user chooses to press &lt;ESC&gt;, he/she will return via block 296 to the category menu in block 286.
    </p>
    <p num="55">
      If the player/student opts to study a specific maneuver or skill, the system retrieves the appropriate image sequences from the image database in block 298.
      <br/>
      An options menu will be displayed in block 300.
      <br/>
      For each maneuver or skill selected, the player/student can either view the images of the ideal execution of the maneuver and/or interactively practice the maneuver or skill by manipulating via the user input device an adopted image of a hockey player which has been superimposed over or juxtaposed to the ideal image on the displays (as shown in FIGS. 23 and 24).
      <br/>
      When practicing a skill, the player/student attempts to manipulate the adopted player image to approximate as closely as possible the execution of the skill by the ideal image.
      <br/>
      The player/student makes this selection from the options menu in block 302.
      <br/>
      If the player/student chooses in block 302 to press &lt;ESC&gt;, he/she will return to the skill menu block 292 via block 304.
      <br/>
      If the player/student chooses, in block 306, to view the master or ideal execution of the skill or maneuver, control of the system will pass to FIG. 2D. If the player/student chooses in block 306 to practice the selected skill or maneuver, control passes from block 306 to FIG. 2E.
    </p>
    <p num="56">
      Referring to FIG. 2D, if the player/student has chosen to view the master or ideal image, in blocks 308 and 310 the player/student begins with the first image in the sequence.
      <br/>
      If the player/student chooses in block 312 to press, ESC., the system will return via block 314 to the options menu in block 300 (see FIG. 2C).
    </p>
    <p num="57">
      If the player/student chooses in block 316 to view the previous frame in the sequence, the system checks in block 318 to insure that a previous image exits, i.e., that the player/student is not currently viewing the first image in the sequence.
      <br/>
      If a previous frame exists, the system selects and displays the appropriate image in blocks 320 and 322.
    </p>
    <p num="58">
      If the player/student chooses in block 316 to view the next frame in the sequence, the system checks in block 324 to insure that a subsequent image exists, i.e., that the player/student is not currently viewing the last image in the sequence.
      <br/>
      If a subsequent frame exists, the system selects and displays the appropriate image in blocks 326 and 328.
    </p>
    <p num="59">
      Referring back to FIG. 2C, as mentioned above, if the player/student chooses in block 306 to practice the selected skill or maneuver, control passes to FIG. 2E. Referring to FIG. 2E, in block 330, the decision tree which describes the relationships between the drawings in the player/student controlled image sequence is retrieved.
      <br/>
      In block 332, the first image which will be displayed on the computer screen is determined and retrieved.
      <br/>
      In blocks 334 and 336, the first image in the master sequence, the first image in the player/student controlled sequence, the fixed image (or background), and the proficiency display graphics are combined and displayed.
    </p>
    <p num="60">
      In block 338, the player/student may exit to the options menu via block, 340 or select a computer key (or other input device, e.g. mouse, joystick) which will move the controlled image.
      <br/>
      If the player/student selects an invalid key, the system will give the player/student the option of selecting another key in block 342.
    </p>
    <p num="61">
      If the player/student selects a key to manipulate the controlled image, the next frame in the sequence will be selected in block 344.
      <br/>
      The controlled image will be updated in block 346 and proficiency measurements will be calculated in block 348.
      <br/>
      The system will then return to block 334 where the images will be combined and displayed.
    </p>
    <p num="62">
      While practicing the skill or maneuver, the player/student receives visual feedback by studying on the computer screen (monitor) how closely the adopted image approximates the ideal image.
      <br/>
      In addition, proficiency measurements are calculated and displayed on the computer screen together with the master and controlled images.
      <br/>
      The proficiency measurements can be continuously updated as the player/student proceeds through the practice session.
    </p>
    <p num="63">
      If the player/student chooses in block 340 to return to the options menu, the system can calculate a numeric score or scores in block 350 which will represent how close the player/student came to approximating the ideal image.
      <br/>
      If applicable, the system can compare current results to those retrieved from the player/student's records.
      <br/>
      This will enable him/her to monitor his/her progress in perfecting the particular skill being studied.
      <br/>
      This information will be displayed on the computer screen in block 352.
    </p>
    <p num="64">
      If the player/student chooses in block 354 to count the practice session as a "warm-up", the current score will not be recorded in the player/student's record.
      <br/>
      Otherwise, the player/student's current performance will be recorded in block 356 by the computer for future reference.
      <br/>
      Control of the system then returns to FIG. 2C.
    </p>
    <p num="65">
      In a further embodiment of the invention, the player/student has the option of controlling the adopted image of one or more hockey players against an opposing team in the context of a game (this embodiment of the invention can be used for practice sessions or drills).
      <br/>
      The opposing team may consist of one to five ideal players (plus one goaltender) that are controlled by the computer system.
    </p>
    <p num="66">
      The player/student can select the number of players (one to six including goaltender) on the opposing team from a menu screens (as shown in FIGS. 15 and 18).
      <br/>
      In addition, the player/student has the option of choosing whether the opposing team is in an offensive or defensive posture (as shown in FIG. 13).
    </p>
    <p num="67">
      The player/student can then choose from a list of attacking or defending formations for the opposing team.
      <br/>
      The list of formations can vary with the number of players on the opposing team, it being obvious that a full team of six (5 skaters and one goalie) will offer the maximum number of formations (as shown in FIG. 14).
    </p>
    <p num="68">
      The game can be played with the player/student causing the movement of the adopted image(s) to "play" against the ideal team which is pre-controlled by the computer software.
      <br/>
      The rules of the game will be those generally used in competitive hockey, i.e. by most American, Canadian, and international teams.
      <br/>
      Games can last from 10 to 45 minutes (more or less) depending on the delivery system being used, e.g. tutorial/interactive, coaching, arcade, home, etc.
    </p>
    <p num="69">
      Additional embodiments of the invention enable the player/student to play against an opposing team when his/her adopted image is part of an adopted team.
      <br/>
      Each team may consist of ideal (computer controlled) images or adopted images controlled by other player/students on additional terminals connected to the system or networked together.
      <br/>
      Each team may consist of one to five players (plus goalies), with fewer players for roller hockey; and more or less for other sports or activities.
    </p>
    <p num="70">FIGS. 3 through 8 are schematic pictorial illustrations of use of the various embodiments and configurations of the present invention.</p>
    <p num="71">
      FIG. 3 shows a single user using the invention.
      <br/>
      The user supplies inputs via a hand-held joystick to manipulate an adopted figure on the right side of the screen to approximate movement of an image of an instructive, preprogrammed figure on the left side of the screen.
      <br/>
      The user views the preprogrammed image, recognizes what is being demonstrated, visualizes the performance of the maneuver, makes cognitive choices and decisions (including considering options) then manipulates the controllable, adopted image on the right side of the screen to achieve the instructed desired result.
    </p>
    <p num="72">
      FIG. 4 shows another single user interfacing with the computer system of the invention.
      <br/>
      In this embodiment, the user, in a competitive setting, manipulates the controllable image (shown connected to the joystick controller by a dashed line) to effect a result or accomplish an objective playing in opposition to two preprogrammed images.
      <br/>
      It will be understood that the number of preprogrammed and controllable images can be different than those depicted in FIG. 4.
    </p>
    <p num="73">
      FIGS. 5A and 5B show two users using the invention simultaneously on two different computers which are networked together.
      <br/>
      In this configuration, two instructive images on the left side of the screen are approximated by two adopted, controllable images on the right side of the screen.
      <br/>
      The first user (FIG. 5A) manipulates one of the adopted figures to emulate a repositioning movement of one of the instructive figures, and the second user (FIG. 5B) manipulates the other adopted figure to emulate the repositioning of the other instructive figure.
    </p>
    <p num="74">
      FIG. 6 shows multiple users simultaneously interfacing with the method and apparatus of the invention on multiple networked computers.
      <br/>
      In this configuration, multiple users manipulate controllable images in a team setting against a preprogrammed team, with each user controlling one of the five controllable figures.
      <br/>
      Alternatively, the system can be used such that the opposing team is also comprised of controllable images.
    </p>
    <p num="75">
      FIG. 7 is a pictorial view showing multiple users interfacing with the invention.
      <br/>
      Again, the individual computers are networked such that all of the users can interface with the invention simultaneously.
      <br/>
      In this embodiment, the system is installed in a skating rink setting such that it is conducive to practice and training.
      <br/>
      The skating rink can be of the type described in co-pending U.S. patent application Ser.
      <br/>
      No. 08/375,606, filed Jan. 20, 1995, entitled "Interactive Ice and Roller Hockey Training Coaching and Playing Rinks," by Vincent J. Macri, which is incorporated herein by reference.
    </p>
    <p num="76">
      FIG. 8 is a schematic pictorial depiction of the invention being used by multiple users simultaneously with the systems being connected over the worldwide Internet.
      <br/>
      This configuration allows users remotely located from each other to simultaneously interface with the invention and with one another.
    </p>
    <p num="77">While this invention has been particularly shown and described with references to preferred embodiments thereof, it will be understood by those skilled in the art that various changes in form and details may be made therein without departing from the spirit and scope of the invention as defined by the appended claims.</p>
    <p num="78">
      The foregoing describes the use of the system of the present invention for cognitive instruction, simulation, pre-training and practice and entertainment in the sport of ice hockey.
      <br/>
      However, as will be recognized by those of ordinary skill in the art, the system of the present invention may be utilized for other sports or athletic training, including, but not limited to, street hockey, roller hockey, field hockey, figure skating, soccer, football, baseball, etc., or instruction in other non-sport activities.
      <br/>
      For example, for sports the system of the present invention may be utilized in an instructional program for soccer by storing images of ideal soccer plays and allowing the player/student to control an image of a soccer player in the manner described previously for ice hockey.
      <br/>
      For non-sport activities, such as bricklaying, diamond cutting etc., calling for the coupling of cognitive and motor skills the present invention is well-suited.
    </p>
    <p num="79">
      It will also be apparent to those skilled in the art that the scope of the present invention need not be limited to the technology described above.
      <br/>
      For example, the method used by the player/student to manipulate the "controlled" image may be changed to include a joystick, 3D controller for PC games such as SpaceOrb 360, virtual reality gloves (as shown in FIG. 21), helmet and states, foot pedals or other technology and the "adopted" and "instructive" images may appear on a virtual reality headset (as shown in FIG. 22) or other virtual reality device(s).
      <br/>
      The method of obtaining, storing, and displaying the "controlled" and "instructive" images may also be changed to include video, multi-media, or animation methods.
      <br/>
      All or any portion of a controllable figure can be displayed from any perspective.
    </p>
  </description>
  <claims format="original" lang="en" id="claim_en">
    <claim num="1">
      <claim-text>What is claimed is:</claim-text>
      <claim-text>1.</claim-text>
      <claim-text>A method of simulated training for at least one user interacting with at least one image displayed on a display means of a processing system, said method comprising the steps of:</claim-text>
      <claim-text>generating and storing in a memory means of the processing system, at least one processor displayed sequence of demonstrative images, wherein said at least one processor displayed sequence of demonstrative images comprises a series of images of demonstrative performance of physical activity movements; further generating and storing, in said memory means, at least one processor displayed user controllable image, wherein said at least one processor displayed user controllable image comprises a user controllable image idiosyncratically controlled by said user to perform a series of simulated physical activity movements idiosyncratic interactive with said sequence of demonstrative images; displaying an image of said sequence of demonstrative images on the display means of said processing system; further displaying said user controllable image on said display means of said processing system, providing said at least one user with input means of said processing system to transmit inputs to said processing system for idiosyncratically controlling said user controllable image, such that instead of said at least one user performing said interactive movements, said user controllable image is controlled to perform said simulated physical activity movements idiosyncratically interactive with said displayed image of said sequence of demonstrative images; storing, and maintaining in said memory means, a record of said movements representing said user controllable image as controlled by said input means such that said record is modified to represent said idiosyncratic movements as controlled by said input means;</claim-text>
      <claim-text>and displaying said movements of said user controllable image on said display means in sequence with said displayed image of said sequence of demonstrative images.</claim-text>
    </claim>
    <claim num="2">
      <claim-text>2. The method of claim 1 further comprising the step of providing means, to said at least one user, to select from examples of said demonstrative physical activity movements.</claim-text>
    </claim>
    <claim num="3">
      <claim-text>3. The method of claim 1 further comprising the step of providing means to receive inputs from said at least one user to select the speed at which said series of images of demonstrative performance of said movements is displayed.</claim-text>
    </claim>
    <claim num="4">
      <claim-text>4. The method of claim 1 further comprising the step of providing means to coordinate the speed of use of said input controls from said at least one user with the speed at which said series of images of user controllable movements is displayed.</claim-text>
    </claim>
    <claim num="5">
      <claim-text>5. The method of claim 1 further comprising the step of providing means to receive inputs from said at least one user to control the speed at which the user controllable image is caused to move.</claim-text>
    </claim>
    <claim num="6">
      <claim-text>6. The method of claim 1, wherein said user controllable image is displayed in a region of said display means that is adjacent to another region of said display means at which said series of images of demonstrative movements is displayed.</claim-text>
    </claim>
    <claim num="7">
      <claim-text>7. The method of claim 1, wherein said user controllable image is superimposed upon the series of images of said demonstrative movements.</claim-text>
    </claim>
    <claim num="8">
      <claim-text>8. The method of claim 1 further comprising the step of providing means to receive inputs from said at least one user to control the direction of movement of said user controllable image.</claim-text>
    </claim>
    <claim num="9">
      <claim-text>9. The method of claim 1 further comprising the step of receiving input from said at least one user to control the location of said user controllable image.</claim-text>
    </claim>
    <claim num="10">
      <claim-text>10. The method of claim 1 further comprising the step of receiving input from said at least one user to control the locomotion of said user controllable image.</claim-text>
    </claim>
    <claim num="11">
      <claim-text>11. The method of claim 1 further comprising the step of providing means to receive inputs from said at least one user to control the posture of said user controllable image.</claim-text>
    </claim>
    <claim num="12">
      <claim-text>12. The method of claim 1 further comprising the step of providing means to receive inputs from said at least one said user to control the distance between said at least one user controllable image and at least one other user controllable image.</claim-text>
    </claim>
    <claim num="13">
      <claim-text>13. The method of claim 1 further comprising the step of providing means to receive inputs from said at least one user to control at least one item used in said simulated physical movement activity by said at least one user controllable image.</claim-text>
    </claim>
    <claim num="14">
      <claim-text>14. The method of claim 1, wherein said series of simulated physical activity movements comprises a maneuver of said movements.</claim-text>
    </claim>
    <claim num="15">
      <claim-text>15. The method of claim 1, wherein said input means comprises a virtual reality input device.</claim-text>
    </claim>
    <claim num="16">
      <claim-text>16. The method of claim 1, wherein said display means comprises a virtual reality display device.</claim-text>
    </claim>
    <claim num="17">
      <claim-text>17. The method of claim 1, wherein said series of images of demonstrative performance of said movements comprises a first maneuver;</claim-text>
      <claim-text>and said input means provides input, by at least one said user, to control at least one said user controllable image to move in performance of a second maneuver to interact with the first maneuver.</claim-text>
    </claim>
    <claim num="18">
      <claim-text>18. The method of claim 17, wherein said series of images of demonstrative performance of said movements comprises a first maneuver;</claim-text>
      <claim-text>and said input means provides input, by at least one said user, to control at least one said user controllable image to move in performance of a second maneuver in opposition to the first maneuver.</claim-text>
    </claim>
    <claim num="19">
      <claim-text>19. The method of claim 17, wherein said series of images of demonstrative performance of said movements comprises a first maneuver;</claim-text>
      <claim-text>and said input means provides input, by at least one said user, to control at least one said user controllable image to move in performance of a second maneuver in cooperation with the first maneuver.</claim-text>
    </claim>
    <claim num="20">
      <claim-text>20. The method of claim 1, wherein said user controls movement of said at least one user controllable image to substantially respond to the movements of said displayed series of images of demonstrative performance of said movements.</claim-text>
    </claim>
    <claim num="21">
      <claim-text>21. The method of claim 1 further comprising the step of displaying visible communications to said at least one user.</claim-text>
    </claim>
    <claim num="22">
      <claim-text>22. The method of claim 1 further comprising the step of emitting audible communications to said at least one user.</claim-text>
    </claim>
    <claim num="23">
      <claim-text>23. The method of claim 1 further comprising the step of providing alphanumeric communications to said at least one user.</claim-text>
    </claim>
    <claim num="24">
      <claim-text>24. The method of claim 1, wherein said at least one user further controls at least one additional user controllable image.</claim-text>
    </claim>
    <claim num="25">
      <claim-text>25. The method of claim 1 further comprising at least one additional remotely located user to control at least one additional user controllable image.</claim-text>
    </claim>
    <claim num="26">
      <claim-text>26. The method of claim 1, wherein said series of simulated physical activity movements comprises simulated training for at least one of real physical movements and unreal physical movements.</claim-text>
    </claim>
    <claim num="27">
      <claim-text>27. The method of claim 1, wherein said at least one user selects to substantially suspend from being displayed said displayed image of said sequence of demonstrative images.</claim-text>
    </claim>
    <claim num="28">
      <claim-text>28. The method of claim 1, wherein said at least one user selects to substantially suspend from being displayed said displayed image of said user controllable image.</claim-text>
    </claim>
    <claim num="29">
      <claim-text>29. The method of claim 1, wherein said at least one user selects to substantially suspend said inputs to said processing system.</claim-text>
    </claim>
    <claim num="30">
      <claim-text>30. The method of claim 1, wherein the displayed image of said user controllable image is modified according to user specifications.</claim-text>
    </claim>
    <claim num="31">
      <claim-text>31. The method of claim 1, wherein the displayed image of said user controllable image is modified according to movements of said user controllable image.</claim-text>
    </claim>
    <claim num="32">
      <claim-text>32. The method of claim 1 further comprising the step of providing means to said at least one user to select biomechanical parameters of said series of images of demonstrative movements.</claim-text>
    </claim>
    <claim num="33">
      <claim-text>33. The method of claim 32, wherein said at least one user is a physically challenged user controlling movement of at least one part of the virtual body of said user controllable image without moving a corresponding part of said physically challenged user's body.</claim-text>
    </claim>
    <claim num="34">
      <claim-text>34. The method of claim 1, wherein said at least one user controls at least one part of the virtual body of said at least one user controllable image without moving a corresponding part of the user's body.</claim-text>
    </claim>
    <claim num="35">
      <claim-text>35. The method of claim 1, wherein said series of images of demonstrative movements are displayed in the foreground of said user controllable image.</claim-text>
    </claim>
    <claim num="36">
      <claim-text>36. The method of claim 1, wherein said series of images of demonstrative movements are displayed in the background of said user controllable image.</claim-text>
    </claim>
    <claim num="37">
      <claim-text>37. The method of claim 1, wherein the process of said at least one user idiosyncratically interacting with said at least one image is communicated to at least one other remotely located user.</claim-text>
    </claim>
    <claim num="38">
      <claim-text>38. The method of claim 1, wherein said at least one displayed sequence of demonstrative images comprises a series of images of objects representing said demonstrative movements.</claim-text>
    </claim>
    <claim num="39">
      <claim-text>39. The method of claim 1, wherein said displayed sequence of demonstrative images is responded to by user input controls to said processing system such that said at least one user's idiosyncratic controls are displayed as simulated responses made by said at least one user in the first person.</claim-text>
    </claim>
    <claim num="40">
      <claim-text>40. The method of claim 1, wherein said demonstrative image is comprised of a physical device.</claim-text>
    </claim>
    <claim num="41">
      <claim-text>41. The method of claim 1, wherein said user controllable image is comprised of a physical device.</claim-text>
    </claim>
    <claim num="42">
      <claim-text>42. The method of claim 1, wherein said displayed sequence of demonstrative images is displayed on a holographic display.</claim-text>
    </claim>
    <claim num="43">
      <claim-text>43. The method of claim 1, wherein said displayed sequence of user controllable images is displayed on a holographic display.</claim-text>
    </claim>
    <claim num="44">
      <claim-text>44. The method of claim 1, wherein said demonstrative image is a member of a group comprised of at least one live image.</claim-text>
    </claim>
    <claim num="45">
      <claim-text>45. The method of claim 44, wherein two such said groups compete against one another.</claim-text>
    </claim>
    <claim num="46">
      <claim-text>46. The method of claim 1, wherein said user controllable image is a member of a group comprised of at least one live image.</claim-text>
    </claim>
    <claim num="47">
      <claim-text>47. The method of claim 46, wherein two such said groups compete against one another.</claim-text>
    </claim>
    <claim num="48">
      <claim-text>48. The method of claim 1, wherein prior user inputs to said processing system to control said user controllable image are stored in said memory means of said processing system.</claim-text>
    </claim>
    <claim num="49">
      <claim-text>49. The method of claim 48, wherein said stored inputs are selected by said user such that said user controllable image performs said movements as controlled by said stored memory inputs without further inputs from said user.</claim-text>
    </claim>
    <claim num="50">
      <claim-text>50. The method of claim 1, wherein at least one said user controllable image performs at least one said simulated movement that is independent of said at least one user's control.</claim-text>
    </claim>
    <claim num="51">
      <claim-text>51. The method of claim 1, wherein said demonstrative image uses a representation of a real item used in an activity representing said movements.</claim-text>
    </claim>
    <claim num="52">
      <claim-text>52. The method of claim 1, wherein said demonstrative image uses a representation of an unreal item used in an activity representing said movements.</claim-text>
    </claim>
    <claim num="53">
      <claim-text>53. The method of claim 1, wherein said user controllable image is displayed using a real item in an activity representing said movements.</claim-text>
    </claim>
    <claim num="54">
      <claim-text>54. The method of claim 1, wherein said user controllable image is displayed using an unreal item in an activity representing said movements.</claim-text>
    </claim>
    <claim num="55">
      <claim-text>55. The method of claim 1 further comprising the step of evaluating by processing system means of said processing system, said simulated movements of said user controllable image with said sequence of demonstrative images.</claim-text>
    </claim>
    <claim num="56">
      <claim-text>56. The method of claim 55 further comprising the step of generating a processing system performance analysis of the user controllable image based on the result of said evaluation by said processor means.</claim-text>
    </claim>
    <claim num="57">
      <claim-text>57. The method of claim 1, wherein the processing system provides tactile feedback to said at least one user.</claim-text>
    </claim>
    <claim num="58">
      <claim-text>58. The method of claim 1, wherein said user controllable image is controlled by said user to perform a demonstrative performance of said physical activity movements.</claim-text>
    </claim>
    <claim num="59">
      <claim-text>59. The method of claim 1, further comprising allowing said user to control said user controllable image by voice inputs.</claim-text>
    </claim>
    <claim num="60">
      <claim-text>60. The method of claim 1, wherein said at least one of the demonstrative images and said at least one of the user controllable images are displayed in the same aspect.</claim-text>
    </claim>
    <claim num="61">
      <claim-text>61. The method of claim 1, wherein the characteristic parameters of said at least one of said demonstrative images are selected by said user.</claim-text>
    </claim>
    <claim num="62">
      <claim-text>62. The method of claim 1, wherein the characteristic parameters of said at least one of said user controllable images are selected by said user.</claim-text>
    </claim>
    <claim num="63">
      <claim-text>63. An article of manufacture comprising a processing system user medium having processor readable information embodied therein which evaluates a processing system displayed series of image sequences with a processing system displayed series of user idiosyncratically controlled image sequences comprising; means for generating a first sequence of processing system displayed images, said sequence comprising a series of discrete movements of at least one image as displayed by said processing system readable information; means for receiving said processing system controlled sequences, said sequences representing the processing system user's response to said processing system displayed first sequence;</claim-text>
      <claim-text>and means for evaluating said first and second sequences, and wherein said evaluation produces a result, said result providing an indication of the processing system user's demonstrated skill at responding to said first sequence.</claim-text>
    </claim>
    <claim num="64">
      <claim-text>64. A processing system for simulated training for at least one user interacting with said processing system comprising: a first memory means for storing at least one processing system displayed sequence of demonstrative images in a series, wherein said series of at least one of said processing system displayed sequence of demonstrative images comprises a series of pre-recorded images illustrating performance of physical activity movements; a second memory means for storing at least one processing system displayed user controllable image in a series, wherein said series of at least one computer displayed user controllable image comprises images derived from biomechanical parameters and controlled by at least one user to perform a series of idiosyncratic simulated physical activity movements responsive to the movements of said series of idiosyncratic images of demonstrative performance of the activity; display means for displaying said series of images of demonstrative performance of the activity with said series of user controllable images; input means, used by said at least one user, for transmitting input to said processing system to cause a series of user controllable images to perform a series of idiosyncratically controlled, simulated physical activity movements interactive with said movement of said series of images of demonstrative performance of the activity, without said user making corresponding movements, wherein said input comprises data controlling said movement of said user controllable image;</claim-text>
      <claim-text>and processing system means for storing said input from said at least one user in a third memory means;</claim-text>
      <claim-text>said processing system means for controlling said display means to display said series of images of demonstrative performance of the activity and said series of user controllable images, for retrieving said stored input from said third memory means, for providing said at least one user with means to evaluate said movement of said series of user controllable images with said movement of said series of images of demonstrative performance of the activity, and for displaying the results of said data related to controlled movement of said user controllable image.</claim-text>
    </claim>
    <claim num="65">
      <claim-text>65. The processing system of claim 64, wherein said input means comprises a virtual reality input device.</claim-text>
    </claim>
    <claim num="66">
      <claim-text>66. The processing system of claim 64, wherein said display means comprises a virtual reality display device.</claim-text>
    </claim>
    <claim num="67">
      <claim-text>67. The processing system of claim 64, wherein said series of at least one user controllable image performs at least one simulated physical activity movement that is independent of the said user's control.</claim-text>
    </claim>
    <claim num="68">
      <claim-text>68. A method of simulated training for at least one user to interact with at least one image displayed on a display means of a processing system, said method comprising the steps of: recording and storing in a memory means of a processing system, a first series of discrete images, said discrete images collectively comprising a sequence of physical activity movements; further recording and storing in a memory means a second series of discrete images collectively comprising a sequence of idiosyncratic simulated physical activity movements responsive to said first series of discrete images; displaying, on a display means of said processing system, at least one of said discrete images in said first series, said displayed image comprising a first displayed image; further displaying on said display means, at least one of said discrete images in said second series, said displayed image comprising a second displayed image; sequentially displaying the next discrete images in said first series; receiving input from at least one user to control said second displayed image to substantially interact with said first displayed image instead of said at least one user performing said interactive movements to said first series of discrete images displayed during said sequentially displaying step;</claim-text>
      <claim-text>and providing means for said at least one user to evaluate said second displayed images with said first displayed images, thereby said user being instructed to learn said movements interactive with said sequentially displayed discrete images in said first series.</claim-text>
    </claim>
    <claim num="69">
      <claim-text>69. The method of claim 68 further comprising the step of providing processing means of said processing system to evaluate said control of said second displayed images interacting with said first series of discrete images in order to provide the result of the evaluation to said at least one user.</claim-text>
    </claim>
    <claim num="70">
      <claim-text>70. The method of claim 68, wherein said processing system displayed series of image sequences of first discrete movements is comprised of images of live activity.</claim-text>
    </claim>
    <claim num="71">
      <claim-text>71. A processing system product usable with a programmable processor having processor readable information embodied therein which interactively obtains a sequence of processing system user input controls comprising: processing system readable information which comprises at least a base set of displayable images; processing system readable information which generates a sequence of various positions of one or more of said base set of displayable images; processing system readable information which comprise at least one displayed template of said at least displayable image which enables image input controls by said processing system user, said image input controls representing a user's idiosyncratic interaction with said processing system information generated sequence; wherein a user's demonstrated skill is determined based on an evaluation of said processing system user image input controls sequence to said processing system information generated sequence.</claim-text>
    </claim>
    <claim num="72">
      <claim-text>72. The processing system program information product of claim 71, wherein said base set of displayed images comprises live activity.</claim-text>
    </claim>
    <claim num="73">
      <claim-text>73. The processing system program information product of claim 71, wherein said product comprises capability for distributed operation.</claim-text>
    </claim>
    <claim num="74">
      <claim-text>74. A method of interactively determining a processing system user's demonstrated skill based on said user's interaction with one or more processing system displayed images, said method comprising; providing a training sequence of discrete movements of at least one displayed image, said training sequence additionally comprising at least one of audible, alphanumeric and visual communication; providing a processing system user with a displayed representation of said one or more displayed images; accepting user input controls representing said processing system user's idiosyncratic interaction with said training sequence;</claim-text>
      <claim-text>and evaluating said user input controls to said training sequence, scoring said evaluation, said scoring representing said processing system user's demonstrated skill.</claim-text>
    </claim>
    <claim num="75">
      <claim-text>75. The method of claim 74, wherein a multimedia method of interactively determining a processing system user's demonstrated skill is based on said user's interacting with input controls in said training sequence.</claim-text>
    </claim>
    <claim num="76">
      <claim-text>76. A method of improving a processing system user's simulated training skills based on interaction with one or more processing system displayed images, said method comprising: providing a training sequence of discrete movements of one or more displayed images, said training sequence representing at least one movement; providing a processing system user with a displayed representation of said one or more displayed images; accepting idiosyncratic user input controls representing said processing system user's response to said at least one movement; evaluating said user input controls to said training sequence, scoring said evaluation, said scoring representing said processing system user's demonstrated skill in interacting with said at least one movement.</claim-text>
    </claim>
    <claim num="77">
      <claim-text>77. The method of claim 76 wherein improving a processing system user's simulated training skills comprises said at least one movement which represents at least one of a coaching and training decision.</claim-text>
    </claim>
    <claim num="78">
      <claim-text>78. A method of simulated training for a least one user using personal, interactive, idiosyncratic, physical movement to control at least one image displayed by a processing system, said method comprising the steps of: generating and storing in a memory means of said processing system, at least one processor displayable sequence of demonstrative images, wherein said at least one processor displayable sequence of demonstrative images comprises a series of images of demonstrative performance of physical movements; further generating, storing and maintaining in said memory means, at least one processor displayable user controllable image, wherein said at least one processor displayable user controllable image comprises a user idiosyncratically controllable image controlled by said user so as to perform a sequence of personal, interactive, idiosyncratic, simulated physical movements interacting with said sequence of demonstrative images; displaying an image of said sequence of demonstrative images on a display means of said processing system; further displaying said user controllable image on said display means of said processing system, providing said at least one user with input means of said processing system to transmit inputs to said processing system for controlling said user controllable image, such that instead of displaying the movement of said at least one user performing said movement, said user controllable image is controlled to perform said movements interactive with said displayed image of said sequence of demonstrative images; storing and maintaining in said memory means, movements representing said user controllable image as controlled by said input means; displaying said simulated physical movements of said user controllable image on said display means with said displayed image of said sequence of demonstrative images;</claim-text>
      <claim-text>and providing said at least one user with means to compare said movement of said user controllable image to said displayed image of said sequence of demonstrative images, thereby said user being instructed to learn said simulated movements interactive with said displayed image of said sequence of demonstrative images.</claim-text>
    </claim>
    <claim num="79">
      <claim-text>79. The method of claim 78, wherein said sequence of images of demonstrative performance of an activity is comprised of images of at least one of live images or videotaping images or animated or computer generated images of at least one of things, objects or systems.</claim-text>
    </claim>
    <claim num="80">
      <claim-text>80. The method of claim 78, wherein said at least one user controllable image is comprised of an image of said at least one of live images or videotaping images or animated or computer generated images of at least one of things, objects or systems.</claim-text>
    </claim>
    <claim num="81">
      <claim-text>81. The method of claim 78, wherein said user controllable image is idiosyncratically controlled by the user to perform a demonstrative performance of said movements.</claim-text>
    </claim>
    <claim num="82">
      <claim-text>82. The method of claim 78, further comprising allowing said user to control said user controllable image by voice inputs.</claim-text>
    </claim>
    <claim num="83">
      <claim-text>83. The method of claim 78, wherein said at least one of the demonstrative images and said at least one of the user controllable images are displayed in the same aspect.</claim-text>
    </claim>
    <claim num="84">
      <claim-text>84. The method of claim 78, wherein the characteristic parameters of said at least one of said demonstrative images are selected by the user.</claim-text>
    </claim>
    <claim num="85">
      <claim-text>85. The method of claim 78, wherein the characteristic parameters of said at least one of said user controllable images are selected by the user.</claim-text>
    </claim>
  </claims>
</questel-patent-document>