<?xml version="1.0" encoding="UTF-8"?>
<questel-patent-document lang="en" date-produced="20180805" produced-by="Questel" schema-version="3.23" file="US06181442B2.xml">
  <bibliographic-data lang="en">
    <publication-reference publ-desc="Granted patent as second publication">
      <document-id>
        <country>US</country>
        <doc-number>06181442</doc-number>
        <kind>B2</kind>
        <date>20010130</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>US6181442</doc-number>
      </document-id>
    </publication-reference>
    <original-publication-kind>B2</original-publication-kind>
    <application-reference is-representative="YES" family-id="18408409" extended-family-id="28375194">
      <document-id>
        <country>US</country>
        <doc-number>08990169</doc-number>
        <kind>A</kind>
        <date>19971212</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>1997US-08990169</doc-number>
      </document-id>
      <document-id data-format="questel_Uid">
        <doc-number>29029745</doc-number>
      </document-id>
    </application-reference>
    <language-of-filing>en</language-of-filing>
    <language-of-publication>en</language-of-publication>
    <priority-claims>
      <priority-claim kind="national" sequence="1">
        <country>JP</country>
        <doc-number>35012596</doc-number>
        <kind>A</kind>
        <date>19961227</date>
        <priority-active-indicator>Y</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="1">
        <doc-number>1996JP-0350125</doc-number>
      </priority-claim>
    </priority-claims>
    <dates-of-public-availability>
      <publication-of-grant-date>
        <date>20010130</date>
      </publication-of-grant-date>
    </dates-of-public-availability>
    <classifications-ipcr>
      <classification-ipcr sequence="1">
        <text>H01L  27/14        20060101AFI20051220RMJP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>H</section>
        <class>01</class>
        <subclass>L</subclass>
        <main-group>27</main-group>
        <subgroup>14</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>JP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051220</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="2">
        <text>H04N   1/028       20060101ALI20051220RMJP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>1</main-group>
        <subgroup>028</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>JP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051220</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="3">
        <text>H04N   1/031       20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>1</main-group>
        <subgroup>031</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="4">
        <text>H04N   5/335       20110101ALI20101216RHJP</text>
        <ipc-version-indicator>
          <date>20110101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>5</main-group>
        <subgroup>335</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>JP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20101216</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="5">
        <text>H04N   5/357       20110101ALI20101216RHJP</text>
        <ipc-version-indicator>
          <date>20110101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>5</main-group>
        <subgroup>357</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>JP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20101216</date>
        </action-date>
      </classification-ipcr>
    </classifications-ipcr>
    <classification-national>
      <country>US</country>
      <main-classification>
        <text>358475000</text>
        <class>358</class>
        <subclass>475000</subclass>
      </main-classification>
    </classification-national>
    <classifications-ecla>
      <classification-ecla sequence="1">
        <text>H04N-001/031E</text>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>001</main-group>
        <subgroup>031E</subgroup>
      </classification-ecla>
    </classifications-ecla>
    <patent-classifications>
      <patent-classification sequence="1">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>H04N-001/0318</classification-symbol>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>1</main-group>
        <subgroup>0318</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20180123</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="2">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>H04N-2201/02497</classification-symbol>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>2201</main-group>
        <subgroup>02497</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20180123</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="3">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>H04N-2201/03112</classification-symbol>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>2201</main-group>
        <subgroup>03112</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20180123</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="4">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>H04N-2201/03116</classification-symbol>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>2201</main-group>
        <subgroup>03116</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20180118</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="5">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>H04N-2201/03133</classification-symbol>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>2201</main-group>
        <subgroup>03133</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20180118</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="6">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>H04N-2201/03141</classification-symbol>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>2201</main-group>
        <subgroup>03141</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20180118</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="7">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>H04N-2201/03145</classification-symbol>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>2201</main-group>
        <subgroup>03145</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20180118</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="8">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>H04N-2201/03162</classification-symbol>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>2201</main-group>
        <subgroup>03162</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20180123</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="9">
        <classification-scheme office="EP" scheme="ICO"/>
        <classification-symbol>T04N-201/024M</classification-symbol>
      </patent-classification>
      <patent-classification sequence="10">
        <classification-scheme office="EP" scheme="ICO"/>
        <classification-symbol>T04N-201/031E4D</classification-symbol>
      </patent-classification>
      <patent-classification sequence="11">
        <classification-scheme office="EP" scheme="ICO"/>
        <classification-symbol>T04N-201/031E3C</classification-symbol>
      </patent-classification>
    </patent-classifications>
    <number-of-claims>25</number-of-claims>
    <exemplary-claim>1</exemplary-claim>
    <figures>
      <number-of-drawing-sheets>5</number-of-drawing-sheets>
      <number-of-figures>7</number-of-figures>
      <image-key data-format="questel">US6181442</image-key>
    </figures>
    <invention-title format="original" lang="en" id="title_en">Close-contact type image sensor and image reading apparatus using the same</invention-title>
    <references-cited>
      <citation srep-phase="examiner">
        <patcit num="1">
          <text>BAN MIKICHI, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4787749</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4787749</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="2">
          <text>HOTTA HIROYUKI</text>
          <document-id>
            <country>US</country>
            <doc-number>5350914</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5350914</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="3">
          <text>TABATA MASAMI, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5357099</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5357099</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="4">
          <text>KASHIWAGI KAZUO</text>
          <document-id>
            <country>US</country>
            <doc-number>5412485</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5412485</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="5">
          <text>IMAMURA MASAYA, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5434681</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5434681</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="6">
          <text>KAMEYAMA SHINJI, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5442466</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5442466</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="7">
          <text>MIWA TAKESHI, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5473149</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5473149</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="8">
          <text>ISO TOSHIMITSU, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5489995</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5489995</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="9">
          <text>NOGUCHI KOICHI</text>
          <document-id>
            <country>US</country>
            <doc-number>5500738</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5500738</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="10">
          <text>KONISHI MASAYUKI, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>6056196</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US6056196</doc-number>
          </document-id>
        </patcit>
      </citation>
    </references-cited>
    <parties>
      <applicants>
        <applicant data-format="original" app-type="applicant" sequence="1">
          <addressbook lang="en">
            <orgname>Canon Kabushiki Kaisha</orgname>
            <address>
              <address-1>Tokyo, JP</address-1>
              <city>Tokyo</city>
              <country>JP</country>
            </address>
          </addressbook>
          <nationality>
            <country>JP</country>
          </nationality>
        </applicant>
        <applicant data-format="questel" app-type="applicant" sequence="2">
          <addressbook lang="en">
            <orgname>CANON</orgname>
          </addressbook>
          <nationality>
            <country>JP</country>
          </nationality>
        </applicant>
      </applicants>
      <inventors>
        <inventor data-format="original" sequence="1">
          <addressbook lang="en">
            <name>Ogura, Makoto</name>
            <address>
              <address-1>Sagamihara, JP</address-1>
              <city>Sagamihara</city>
              <country>JP</country>
            </address>
          </addressbook>
          <nationality>
            <country>JP</country>
          </nationality>
        </inventor>
        <inventor data-format="original" sequence="2">
          <addressbook lang="en">
            <name>Kawai, Tatsundo</name>
            <address>
              <address-1>Hadano, JP</address-1>
              <city>Hadano</city>
              <country>JP</country>
            </address>
          </addressbook>
          <nationality>
            <country>JP</country>
          </nationality>
        </inventor>
      </inventors>
      <agents>
        <agent sequence="1" rep-type="agent">
          <addressbook lang="en">
            <orgname>Robin, Blecker &amp; Daley</orgname>
          </addressbook>
        </agent>
      </agents>
    </parties>
    <examiners>
      <primary-examiner>
        <name>Grant, II, Jerome</name>
      </primary-examiner>
    </examiners>
    <lgst-data>
      <lgst-status>EXPIRED</lgst-status>
    </lgst-data>
  </bibliographic-data>
  <abstract format="original" lang="en" id="abstr_en">
    <p id="P-EN-00001" num="00001">
      <br/>
      For preventing occurrence of interference fringes derived from interference of light rays on a contact plane between a contact glass and a cover glass, and thus homogenizing an output wave that may otherwise undergo the adverse effect of interference fringes, a close-contact type image sensor includes a contact glass arranged to come into contact with a read surface of an original, a light source for irradiating light to the read surface, an image forming lens for converging light reflected from the read surface, an image sensor part for reading an image of the original formed on an image plane of the image forming lens, and a cover glass for fixing the light source and the image forming lens at respective predetermined positions in a housing, wherein an air layer is present between the contact glass and the cover glass.
    </p>
  </abstract>
  <description format="original" lang="en" id="desc_en">
    <heading>BACKGROUND OF THE INVENTION</heading>
    <p num="1">
      1.
      <br/>
      Field of the Invention
    </p>
    <p num="2">The present invention relates to an image sensor capable of being employed in a facsimile system, copier, scanner, or the like for reading light reflected from a read surface, and to an image reading apparatus using the image sensor.</p>
    <p num="3">2. Description of Related Art</p>
    <p num="4">In the past, a close-contact type image sensor using a sensor array to form a read image on a one-to-one reduced scale and thus read an image from a read original with the size of the image unchanged has been used as an original reading device to be employed in a facsimile system, copier, scanner, or the like.</p>
    <p num="5">
      A known method is, as described in Japanese Patent Publication No. Hei 8-34531, such that a rod lens array and a line light source are fixed directly to a housing by performing affixation or the like, and then a contact glass is fixed to the housing.
      <br/>
      However, the work is complex.
      <br/>
      Besides, since the rod lens array and the line light source are affixed mutually independently, a positional deviation is likely to occur.
      <br/>
      There is the fear of a failure to read an image properly.
    </p>
    <p num="6">
      FIG. 7 is a sectional view of a known example of a close-contact type image sensor in which the above drawbacks are taken into account.
      <br/>
      The close-contact type image sensor has a housing 1 serving as a supporting means in which a sensor substrate 10 on which a sensor part 4 composed of a plurality of sensor chips for defining the locations of a plurality of pixels and for carrying out photoelectric conversion, and a protective film for protecting the plurality of sensor chips are mounted, and an optical member 3 including light sources 13 attached to an end face thereof for irradiating light to a read original, a rod lens array 2 that is a lens for forming an image read from the read original as the pixels on the surface of the sensor part 4, and a contact glass 5 serving as an original reading plane, are mounted.
      <br/>
      Attached to the sensor substrate 10 are an image processing circuit 11 for processing an image signal read by the sensor part 4 composed of the sensor chips and a connection part 12 for externally outputting an output signal of the close-contact type image sensor.
      <br/>
      The rod lens array 2 and the optical member 3 are pressed to the housing 1 by the contact glass 5.
      <br/>
      The contact glass 5 is fixed to the housing 1 by performing affixation or the like.
    </p>
    <p num="7">
      However, in this known example, since the contact glass 5 determines the focal length of the rod lens array 2, there is no range of choice in determining the thickness of the contact glass 5.
      <br/>
      Therefore, a special glass having a specific thickness must be custom-made.
    </p>
    <p num="8">
      In recent years, the number of scenes in which color images are handled has increased with the prevalence of color printers and color displays.
      <br/>
      There is an increasing demand for reading of color originals rather than conventional monochrome originals.
    </p>
    <p num="9">
      When a close-contact type image sensor is used to read a color original, light emitted from a light source and falling on the original reading plane must be multi-color light so that the color original can be read.
      <br/>
      In general, trichromatic light of red, green, and blue is used as light incident on the original reading plane.
      <br/>
      Three light sources mutually separated to be associated with the three colors or a single light source formed with a recently-developed white LED for emitting white light having the three colors mixed therein is included in the close-contact type image sensor.
    </p>
    <p num="10">Moreover, studies have been made of a close-contact type image sensor for reading a color original, in which light reflected from the original reading plane and segmented into colors is passed through a rod lens array that is a lens for forming an image as pixels on the surfaces of sensor chips, guided to the plurality of sensor chips arranged to define the locations of pixels and designed to carry out photoelectric conversion, and then photoelectrically converted color by color.</p>
    <p num="11">An information processing apparatus using the close-contact type image sensor, for example, a scanner, may adopt either a flat-bed system or a sheet-through system.</p>
    <p num="12">The flat-bed system is such that a book or any other original is placed on a top glass with an information side thereof facing the top glass, and a close-contact image sensor opposed to the information side is moved to read image information from the original.</p>
    <p num="13">By contrast, the sheet-through system is such that mutually-independent original sheets are moved one by one while brought into close contact with a close-contact type image sensor.</p>
    <p num="14">In the known example, for coping with both the systems, the contact glass 5 must be replaced with another.</p>
    <p num="15">
      The prior art will be described in conjunction with FIGS. 5 and 6.
      <br/>
      FIG. 5 is a sectional view showing the structure of a close-contact type image sensor of the sheet-through system.
      <br/>
      FIG. 6 is a sectional view showing the structure of a close-contact type image sensor of the flat-bed system.
      <br/>
      The structure in accordance with the prior art will be described.
      <br/>
      That is to say, an optical member 3 serving as a light source unit including light sources 13 and a rod lens array 2 are pressed from above in the drawing by means of a cover glass 6 and located at desired positions in a housing 1.
      <br/>
      The cover glass and a housing 1 are secured by performing affixation or the like.
      <br/>
      As a result, the optical member 3 and the rod lens array 2 are fixed to the housing 1.
    </p>
    <p num="16">A sensor part 4 mounted on a sensor substrate 10 is formed by placing a sensor array on the sensor substrate 10, electrically connected to an image processing circuit 11, resistor, and capacitor, and also electrically connected to a connection part 12 enabling connections with external circuits.</p>
    <p num="17">
      In FIG. 5, the contact glass 5 which is arranged to be brought into contact with an original P is fixed to the cover glass 6.
      <br/>
      The reasons therefor will be described below.
    </p>
    <p num="18">
      (i) The thickness of a glass matching the focal length of the rod lens array 2 can be set to a selected value more easily by forming the glass with two glasses of the contact glass 5 and cover glass 6 than by forming the glass with a single glass.
      <br/>
      Consequently, at least the rod lens array 2 and the optical member 3 can be fixed at the same time.
    </p>
    <p num="19">(ii) For the one kind of housing 1, either the flat-bed system or the sheet-through system can be selected depending on the presence or absence of the contact glass 5.</p>
    <p num="20">
      In FIG. 6, the contact glass 5 is removed, and a top glass 9 is disposed in such a way as to ensure an optically equivalent structure.
      <br/>
      The optical member 3 serving as a light source unit uses a light guide to diffuse light rays emitted from the light sources (LEDs) 13 located at both ends in a longitudinal direction of the optical member 3, and to guide the diffused light to an original P. The original P is then illuminated with the light.
      <br/>
      Light reflected from the original P passes through the rod lens array 2, forms an image on the sensor part 4, and is then converted into an electrical signal.
      <br/>
      For reading a color image, LEDs for emitting light rays of three colors of red, green, and blue are included and lit.
    </p>
    <p num="21">
      However, as far as the foregoing close-contact type image sensor of the prior art is concerned, even when the surfaces of the contact glass 5 and the cover glass 6 shown in FIG. 5 which are in contact with each other are flattened to as great an extent as mechanically and economically possible, a gap of several um deep would exist partly on a contact plane 20 between the contact glass 5 and the cover glass 6.
      <br/>
      Therefore, interference fringes are produced by interference of light.
      <br/>
      Accordingly, an output wave undergoes the adverse effect of interference fringes and exhibits inhomogeneities associated with the interference fringes.
    </p>
    <p num="22">
      Moreover, the depth of the gap varies depending on the pressure given by a roller used to transport an original or depending on ambient temperature or humidity.
      <br/>
      The output wave varies accordingly.
      <br/>
      This poses a problem that since correction cannot be achieved properly, a read error occurs.
    </p>
    <p num="23">
      For reading especially a color image, when light rays with long wavelengths interfere with one another, interference fringes are likely to occur.
      <br/>
      The wavelengths of red, green, and blue light rays become different because of the interference fringes.
      <br/>
      Consequently, the problem that a color deviation occurs in read data becomes outstanding.
      <br/>
      The adverse effect of the interference fringes becomes more serious than that occurring when a monochrome image is read.
    </p>
    <heading>BRIEF SUMMARY OF THE INVENTION</heading>
    <p num="24">An object of the present invention is to solve the foregoing problems.</p>
    <p num="25">
      To attain the above object, in accordance with an aspect of the present invention, there is provided an image sensor which comprises a contact glass arranged to come into contact with a read surface of an original, a light source for irradiating light to the read surface, an image forming lens for converging light reflected from the read surface, an image sensor part for reading an image of the original formed on an image plane of the image forming lens, and a cover glass for fixing the light source and the image forming lens at respective predetermined positions in a housing, wherein an air layer is present between the contact glass and the cover glass.
      <br/>
      Accordingly, image reading can be achieved with high definition while being unaffected by interference fringes.
    </p>
    <p num="26">
      Further, the depth of the air layer is 50  MU m or more.
      <br/>
      Moreover, by removing the contact glass, the close-contact type image sensor coping with the sheet-through system can be remodeled to cope with the flat-bed system.
    </p>
    <p num="27">Other objects and features of the present invention will be apparent from the following detailed description of preferred embodiments thereof taken in conjunction with the accompanying drawings.</p>
    <heading>BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWING</heading>
    <p num="28">
      FIG. 1 is a sectional view showing a close-contact type image sensor in accordance with a first embodiment of the present invention.
      <br/>
      FIG. 2 is a sectional view showing a close-contact type image sensor in accordance with a second embodiment of the present invention.
      <br/>
      FIG. 3 is an exploded perspective view showing the structure of the close-contact type image sensor in accordance with the second embodiment.
      <br/>
      FIG. 4 is a circuit diagram showing a scanner having the close-contact type image sensor in accordance with the first embodiment.
      <br/>
      FIG. 5 is a sectional view showing a close-contact type image sensor of the sheet-through system in accordance with a prior art.
      <br/>
      FIG. 6 is a sectional view showing a close-contact type image sensor of the flat-bed system in accordance with the prior art.
      <br/>
      FIG. 7 is a sectional view showing a known example of a close-contact type image sensor of the sheet-through system.
    </p>
    <heading>DETAILED DESCRIPTION OF THE INVENTION</heading>
    <p num="29">Hereinafter, preferred embodiments of the present invention will be described in detail with reference to the drawings.</p>
    <p num="30">First Embodiment</p>
    <p num="31">
      FIG. 1 is a sectional view of a close-contact type image sensor in accordance with the first embodiment of the present invention.
      <br/>
      The close-contact type image sensor has a housing 1 serving as a supporting means in which a sensor substrate 10 on which a sensor part 4 composed of a plurality of sensor chips for defining the locations of pixels and carrying out photoelectric conversion and a protective film, which is not shown, for protecting the plurality of sensor chips are mounted, an optical member 3 including light sources 13 such as LEDs for irradiating light to a read original, a rod lens array 2 that is a lens for forming an image read from the read original as the pixels on the surface of the sensor part 4, a contact glass 5 serving as an original reading plane, and a cover glass 6 separated from the contact glass 5 by a gap, are mounted.
      <br/>
      Moreover, attached to the sensor substrate 10 are an image processing circuit 11 for processing an image signal read by the sensor part 4 composed of the sensor chips, and a connection part 12 for externally outputting an output signal of the close-contact type image sensor.
      <br/>
      Moreover, the cover glass 6 and the rod lens array 2 are in contact with each other.
    </p>
    <p num="32">
      The focal length of the rod lens array 2 employed in the first embodiment is 4.8 mm in the air.
      <br/>
      If one glass that is the contact glass 5 is used, the thickness of the contact glass must be 7.2 mm in consideration of the refractive index of the glass.
      <br/>
      A glass having such a thickness is unavailable in standard models and, therefore, becomes very special and expensive.
    </p>
    <p num="33">
      However, when the contact glass 5 is combined with the cover glass 6, the thicknesses of the contact glass 5 and the cover glass 6 may be set to 6 mm and 1.2 mm, respectively.
      <br/>
      Thus, a glass capable of covering the focal length of the rod lens array 2 can be realized at low cost.
    </p>
    <p num="34">
      However, when the glasses are merely assembled, the aforesaid problem of interference fringes takes place.
      <br/>
      The reasons therefor will be explained slightly in detail below.
      <br/>
      Assume that the depth of an air layer between the contact glass 5 and the cover glass 6 is d, the refractive index of air is n (approximately equal to "1"), and the wavelength of a light flux is  LAMBDA .
      <br/>
      For example, when light is incident vertically, an effect of interference varies depending on whether a quotient given by the expression below is an integer or half integer.
      <br/>
      (nd)/ LAMBDA   (1)
    </p>
    <p num="35">
      Accordingly, a transmittance varies.
      <br/>
      The depth of the air layer, d, varies depending on the flatness of a transparent member, accuracy in maintaining a gap, or any other factor of differentiating one product from another, and is, therefore, different from position to position.
      <br/>
      The difference leads to a difference in transmittance from position to position.
      <br/>
      Eventually, interference fringes appear.
    </p>
    <p num="36">
      In general, the wavelengths of light emitted from a light source,  LAMBDA , are not confined to one wavelength but constitute a spectrum.
      <br/>
      Assuming that a wavelength half-width of the spectrum is  DELTA  LAMBDA  and a center wavelength of the emitted light is  LAMBDA 0, if the following relationship is established:
      <br/>
      �+(nd)/( LAMBDA 0 - DELTA  LAMBDA )}-+(nd)/( LAMBDA 0 + DELTA  LAMBDA )}�&gt;1/2  (2),
    </p>
    <p num="37">
      interference in which light rays mutually intensify and interference in which light rays mutually attenuate occur among light rays with wavelengths in the spectrum.
      <br/>
      Consequently, interference fringes become indiscernible.
      <br/>
      The larger the value of the above left side is, the less discernible interference fringes are.
      <br/>
      Normally, the value should be equal to or larger than "5".
    </p>
    <p num="38">The following is deduced from the above:</p>
    <p num="39">
      (i) As the depth of the air layer, d, becomes larger, interference fringes become more indiscernible.
      <br/>
      (ii) As the center wavelength,  LAMBDA 0, becomes larger, interference fringes become more discernible.
      <br/>
      (iii) As the wavelength half-width,  DELTA  LAMBDA , becomes smaller, interference fringes become more discernible.
    </p>
    <p num="40">
      On the other hand, for reading a monochrome image, a light source glowing in green is used generally.
      <br/>
      However, for reading a color image, light sources glowing in red, green, and blue, respectively, are needed.
      <br/>
      In this case, interference fringes caused by red light whose center wavelength  LAMBDA 0 is long pose a problem.
    </p>
    <p num="41">
      In the first embodiment, LED chips are used as light sources glowing in red, green, and blue.
      <br/>
      The center wavelength  LAMBDA 0 of the LED glowing in green with high luminance ranges from about 510 to 530 nm and is thus rather short.
      <br/>
      An LED for emitting light whose center wavelength is rather short or about 600 to 630 nm is used as the LED glowing in red.
      <br/>
      However, as far as an LED chip capable of glowing with high luminance in the spectrum is concerned, a wavelength half-width of the spectrum,  DELTA  LAMBDA , is as small as about 20 nm.
      <br/>
      This facilitates occurrence of the problem of interference fringes.
    </p>
    <p num="42">
      In the first embodiment, if a value provided by the above expression:
      <br/>
      �+(nd)/( LAMBDA 0 - DELTA  LAMBDA )}-+(nd)/( LAMBDA 0 + DELTA  LAMBDA )}�  (3)
    </p>
    <p num="43">should be equal to or larger than "5", the depth of the air layer, d, must be equal to or larger than approximately 50  MU m.</p>
    <p num="44">In the first embodiment, the thickness of the contact glass 5 is 5 mm, the thickness of the cover glass 6 is 1 mm, and the depth of the air layer, d, is 0.8 mm.</p>
    <p num="45">
      In FIG. 1, while the rod lens array 2 and the optical member 3 are, like those in the prior art, fixed to the housing 1 by means of the cover glass 6, the contact glass 5 is fixedly secured to the housing 1, and a gap 30 is intentionally provided between the contact glass 5 and the cover glass 6, thus preventing occurrence of interference fringes.
      <br/>
      The gap 30 should preferably be defined according to the flatness of the parts so that the contact glass 5 and the cover glass 6 will not come into contact with each other and will be separated from each other by 50  MU m or more.
      <br/>
      Herein, as mentioned above, the gap 30 is 0.8  MU m deep.
      <br/>
      This numerical value is a value permitting the contact glass 5 and the cover glass 6 to be kept apart even when the contact glass 5 is pressed due to the weight of an original or the rotation of a roller.
      <br/>
      With the gap 30, a variation of the distance (focal length) from the rod lens array 2 to the sensor part 4 due to the pressing is quite limited.
      <br/>
      Therefore, the level of a signal representing a read image remains homogeneous.
    </p>
    <p num="46">Second Embodiment</p>
    <p num="47">
      FIG. 2 is a sectional view of a close-contact type image sensor in accordance with the second embodiment of the present invention.
      <br/>
      FIG. 3 is an exploded perspective view for explaining the structure of the image sensor.
    </p>
    <p num="48">
      The second embodiment is an improvement of the first embodiment described with reference to FIG. 1.
      <br/>
      In view of the fact that a gap of several  MU m deep might be present between the rod lens array 2 and the cover glass 6 in the first embodiment shown in FIG. 1, the structure of the second embodiment is intended to prevent interference fringes from occurring due to the gap between the rod lens array 2 and the cover glass 6.
    </p>
    <p num="49">
      In the second embodiment, in place of the cover glass 6, there is provided a cover member 7 having a concave part thereof formed as a gap 31 in an optical area thereof that is an area in contact with the rod lens array 2.
      <br/>
      Preferably, the cover member 7 and the rod lens array 2 should be separated from each other by 50  MU m or more.
      <br/>
      In the second embodiment, they are separated from each other approximately by 150  MU m.
    </p>
    <p num="50">
      Likewise, a gap 32 is created between the contact glass 5 and the cover member 7.
      <br/>
      In the second embodiment, as shown in FIG. 3, the cover member 7 is made of a transparent material such as polycarbonate or an acrylic.
      <br/>
      After the optical member 3 having substrates 8 that include light sources 13 such as LEDs attached to both ends thereof and also having a light guide member exhibiting high scattering efficiency between the substrates 8, and the rod lens array 2 are set to the housing 1, the cover member 7 is snapped down and fitted into the housing 1 from above in order to fix the optical member 3 and the rod lens array 2 to the housing 1.
      <br/>
      Thus, assembling is easy to do.
      <br/>
      This leads to improved productivity.
    </p>
    <p num="51">While the cover member 7 is made of a transparent material, the cover member 7 alternatively may be a sheet metal having a slit, for the purpose of attaining the same object.</p>
    <p num="52">
      In FIG. 3, the substrates 8 each have an LED mounted as the light source 13 thereon, each include a lead over which electric power is supplied, and are fixed to both ends of the optical member 3.
      <br/>
      After being assembled, the leads are linked to a substrate 10 of the sensor part 4 by performing soldering or the like so that the leads can be linked to a power supply by way of the substrate 10 of the sensor part 4 and the connection part 12.
    </p>
    <p num="53">
      Since a transparent material such as polycarbonate or an acrylic is used as the material of the optical member 3, expansion and contraction derived from an ambient change in heat or humidity are expected.
      <br/>
      For minimizing a positional deviation between the light sources 13 and the optical member 3 caused by expansion and contraction, the substrates 8 having the LEDs mounted thereon and the optical member 3 are mutually fixed.
      <br/>
      The fixing method may include caulking, affixation, and snap and fit.
    </p>
    <p num="54">
      The positions in a longitudinal direction of the housing 1 and the optical member 3 should preferably be determined in consideration of the foregoing expansion and contraction dependent on temperature and humidity.
      <br/>
      The longitudinal center of the optical member 3 is positioned to be fitted with a groove or projection (not shown) formed in the center of the housing 1.
      <br/>
      Moreover, the sensor part 4 is fitted into a groove in the housing 1 and then fixed by performing affixation, caulking, or snap and fit.
      <br/>
      After the cover member 7 is snapped down and fitted into the housing 1, the contact glass 5 is affixed to the housing 1.
    </p>
    <p num="55">
      The foregoing description has been made by taking the sheet-through system for instance.
      <br/>
      In the case of the flat-bed system, the contact glass 5 is not included but a top glass is disposed serving as the contact glass 5.
    </p>
    <p num="56">FIG. 4 is a circuit diagram showing a scanner in which the close-contact type image sensor in accordance with the first embodiment, for example, is incorporated.</p>
    <p num="57">
      Referring to FIG. 4, a signal processing circuit 41, which includes an A/D converter, is arranged to process a signal outputted from the connection part 12 of the close-contact type image sensor.
      <br/>
      An interface circuit 42 is arranged to perform bilateral communications with an external signal processing apparatus, such as a personal computer, i.e., to receive commands from the external signal processing apparatus and send image signals processed by the signal processing circuit 41 to the external signal processing apparatus.
      <br/>
      A controller 43, such as a CPU (central processing unit), is arranged to synchronously control the signal processing circuit 41, the interface circuit 42 and the light sources 13.
    </p>
    <p num="58">
      The structure of the close-contact type image sensor has been described mainly.
      <br/>
      The close-contact type image sensor is incorporated in an image reading part of an image reading apparatus, such as a scanner.
      <br/>
      For reading especially a color image, since occurrence of interference fringes can be prevented, an image signal permitting high image quality can be produced.
    </p>
  </description>
  <claims format="original" lang="en" id="claim_en">
    <claim num="1">
      <claim-text>What is claimed is:</claim-text>
      <claim-text>1.</claim-text>
      <claim-text>An image sensor comprising:</claim-text>
      <claim-text>a contact glass supported by a housing and arranged to come into contact with a read surface of an original; a light source for irradiating light to the read surface; an image forming lens for converging light reflected from the read surface; an image sensor part for reading an image of the original formed on an image plane of said image forming lens;</claim-text>
      <claim-text>and a cover glass for fixing said image forming lens at a predetermined position in said housing, so that an air layer is formed between the contact glass and said cover glass.</claim-text>
    </claim>
    <claim num="2">
      <claim-text>2. An image sensor according to claim 1, wherein a depth of said air layer is 50  MU m or more.</claim-text>
    </claim>
    <claim num="3">
      <claim-text>3. An image sensor according to claim 1, wherein said image sensor is used in a flat-bed scanner system or a sheet-through scanner system.</claim-text>
    </claim>
    <claim num="4">
      <claim-text>4. An image reading apparatus using said image sensor according to one of claims 1 to 3.</claim-text>
    </claim>
    <claim num="5">
      <claim-text>5. An image sensor according to claim 1, wherein a further air layer is formed between said image forming lens and said cover glass.</claim-text>
    </claim>
    <claim num="6">
      <claim-text>6. An image sensor according to claim 5, wherein a depth of said further air layer is 50  MU m or more.</claim-text>
    </claim>
    <claim num="7">
      <claim-text>7. An image sensor according to claim 1, wherein said cover glass further fixes said light source at a predetermined position in said housing.</claim-text>
    </claim>
    <claim num="8">
      <claim-text>8. An image reading system comprising: a) an image sensor including:</claim-text>
      <claim-text>- a contact glass supported by a housing and arranged to come into contact with a read surface of an original; - a light source for irradiating light to the read surface; - an image forming lens for converging light reflected from the read surface; - an image sensor part for reading an image of the original formed on an image plane of said image forming lens;</claim-text>
      <claim-text>and - a cover glass for fixing said image forming lens at a predetermined position in said housing, so that an air layer is formed between said contact glass and said cover glass; b) a signal processing circuit for processing an image signal read out from said image sensor part; c) an interface circuit for bilateral communications with an external signal processing apparatus;</claim-text>
      <claim-text>and d) a controller for controlling said light source, said signal processing circuit and said interface circuit.</claim-text>
    </claim>
    <claim num="9">
      <claim-text>9. An image reading system according to claim 8, wherein a further air layer is formed between said image forming lens and said cover glass.</claim-text>
    </claim>
    <claim num="10">
      <claim-text>10. An image reading system according to claim 9, wherein a depth of said further air layer is 50  MU m or more.</claim-text>
    </claim>
    <claim num="11">
      <claim-text>11. An image reading system according to claim 8, wherein said signal processing circuit includes an A/D converter.</claim-text>
    </claim>
    <claim num="12">
      <claim-text>12. An image reading system according to claim 8, wherein said signal processing includes a central processing unit.</claim-text>
    </claim>
    <claim num="13">
      <claim-text>13. An image reading system according to claim 8, wherein said cover glass further fixes said light source at a predetermined position in said housing.</claim-text>
    </claim>
    <claim num="14">
      <claim-text>14. An image reading system according to claim 8, wherein a depth of said air layer is 50  MU m or more.</claim-text>
    </claim>
    <claim num="15">
      <claim-text>15. An image sensor comprising: an image forming lens for converging light read from a read surface of an original; an image sensor part for reading an image of the original formed on an image plane of said image forming lens;</claim-text>
      <claim-text>and a cover glass for fixing said image forming lens at a predetermined position in a housing, so that an air layer is formed on a light-incident area of said image forming lens between said image forming lens and said cover glass.</claim-text>
    </claim>
    <claim num="16">
      <claim-text>16. An image sensor according to claim 15, further comprising a light source for irradiating light to the read surface of the original.</claim-text>
    </claim>
    <claim num="17">
      <claim-text>17. An image sensor according to claim 15, wherein a depth of said air layer is 50  MU m or more.</claim-text>
    </claim>
    <claim num="18">
      <claim-text>18. An image sensor according to claim 15, wherein said cover glass further fixes said light source at a predetermined position in said housing.</claim-text>
    </claim>
    <claim num="19">
      <claim-text>19. An image sensor according to claim 15, wherein said image sensor is used in a flat-bed scanner system or a sheet-through scanner system.</claim-text>
    </claim>
    <claim num="20">
      <claim-text>20. An image reading apparatus using said image sensor according to one of claims 15-19.</claim-text>
    </claim>
    <claim num="21">
      <claim-text>21. An image reading system comprising: a) an image sensor including: - a light source for irradiating light to a read surface of an original; - an image forming lens for converging light from the read surface; - an image sensor part for reading an image of the original formed on an image plane of said image forming lens;</claim-text>
      <claim-text>and - a cover glass for fixing said image forming lens at a predetermined position in a housing, so that an air layer is formed on a light-incident area of said image forming lens between said image forming lens and said cover glass; b) a signal processing circuit for processing an image signal read out from said image sensor part; c) an interface circuit for bilateral communications with an external signal processing apparatus;</claim-text>
      <claim-text>and d) a controller for controlling said light source, said signal processing circuit and said interface circuit.</claim-text>
    </claim>
    <claim num="22">
      <claim-text>22. An image reading system according to claim 21, wherein a depth of said air layer is 50  MU m or more.</claim-text>
    </claim>
    <claim num="23">
      <claim-text>23. An image reading system according to claim 21, wherein said cover glass further fixes a light source at a predetermined position in said housing.</claim-text>
    </claim>
    <claim num="24">
      <claim-text>24. An image reading system according to claim 21, wherein said signal processing circuit includes an A/D converter.</claim-text>
    </claim>
    <claim num="25">
      <claim-text>25. An image reading system according to claim 21, wherein said controller includes a central processing unit.</claim-text>
    </claim>
  </claims>
</questel-patent-document>