<?xml version="1.0" encoding="UTF-8"?>
<questel-patent-document lang="en" date-produced="20180805" produced-by="Questel" schema-version="3.23" file="US06184876B2.xml">
  <bibliographic-data lang="en">
    <publication-reference publ-desc="Granted patent as second publication">
      <document-id>
        <country>US</country>
        <doc-number>06184876</doc-number>
        <kind>B2</kind>
        <date>20010206</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>US6184876</doc-number>
      </document-id>
    </publication-reference>
    <original-publication-kind>B2</original-publication-kind>
    <application-reference is-representative="YES" family-id="24720064" extended-family-id="42112758">
      <document-id>
        <country>US</country>
        <doc-number>08677772</doc-number>
        <kind>A</kind>
        <date>19960710</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>1996US-08677772</doc-number>
      </document-id>
      <document-id data-format="questel_Uid">
        <doc-number>43170690</doc-number>
      </document-id>
    </application-reference>
    <language-of-filing>en</language-of-filing>
    <language-of-publication>en</language-of-publication>
    <priority-claims>
      <priority-claim kind="national" sequence="1">
        <country>US</country>
        <doc-number>67777296</doc-number>
        <kind>A</kind>
        <date>19960710</date>
        <priority-active-indicator>Y</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="1">
        <doc-number>1996US-08677772</doc-number>
      </priority-claim>
    </priority-claims>
    <dates-of-public-availability>
      <publication-of-grant-date>
        <date>20010206</date>
      </publication-of-grant-date>
    </dates-of-public-availability>
    <classifications-ipcr>
      <classification-ipcr sequence="1">
        <text>G06F   3/033       20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>06</class>
        <subclass>F</subclass>
        <main-group>3</main-group>
        <subgroup>033</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="2">
        <text>G06F   3/048       20060101A I20070721RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>06</class>
        <subclass>F</subclass>
        <main-group>3</main-group>
        <subgroup>048</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20070721</date>
        </action-date>
      </classification-ipcr>
    </classifications-ipcr>
    <classification-national>
      <country>US</country>
      <main-classification>
        <text>715235000</text>
        <class>715</class>
        <subclass>235000</subclass>
      </main-classification>
      <further-classification sequence="1">
        <text>715727000</text>
        <class>715</class>
        <subclass>727000</subclass>
      </further-classification>
      <further-classification sequence="2">
        <text>715978000</text>
        <class>715</class>
        <subclass>978000</subclass>
      </further-classification>
    </classification-national>
    <patent-classifications>
      <patent-classification sequence="1">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>G06F-003/167</classification-symbol>
        <section>G</section>
        <class>06</class>
        <subclass>F</subclass>
        <main-group>3</main-group>
        <subgroup>167</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130823</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="2">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>G06F-003/04815</classification-symbol>
        <section>G</section>
        <class>06</class>
        <subclass>F</subclass>
        <main-group>3</main-group>
        <subgroup>04815</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130823</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="3">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>Y10S-715/978</classification-symbol>
        <section>Y</section>
        <class>10</class>
        <subclass>S</subclass>
        <main-group>715</main-group>
        <subgroup>978</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130518</date>
        </action-date>
      </patent-classification>
    </patent-classifications>
    <number-of-claims>9</number-of-claims>
    <exemplary-claim>1</exemplary-claim>
    <figures>
      <number-of-drawing-sheets>8</number-of-drawing-sheets>
      <number-of-figures>16</number-of-figures>
      <image-key data-format="questel">US6184876</image-key>
    </figures>
    <invention-title format="original" lang="en" id="title_en">Method and apparatus for audibly communicating comparison information to a user</invention-title>
    <references-cited>
      <citation srep-phase="examiner">
        <patcit num="1">
          <text>KLEINMAN BARRY S</text>
          <document-id>
            <country>US</country>
            <doc-number>4974174</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4974174</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="2">
          <text>MCKIEL JR FRANK A</text>
          <document-id>
            <country>US</country>
            <doc-number>5223828</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5223828</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="3">
          <text>MCKIEL JR FRANK A</text>
          <document-id>
            <country>US</country>
            <doc-number>5287102</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5287102</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="4">
          <text>MCKIEL JR FRANK A</text>
          <document-id>
            <country>US</country>
            <doc-number>5374924</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5374924</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="5">
          <text>AINSCOW FRANK, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5428723</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5428723</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="6">
          <text>VENOLIA DANIEL S</text>
          <document-id>
            <country>US</country>
            <doc-number>5463722</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5463722</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="7">
          <text>DULUK JR JEROME F</text>
          <document-id>
            <country>US</country>
            <doc-number>5572634</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5572634</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="8">
          <text>NAGAI SACHIKO</text>
          <document-id>
            <country>US</country>
            <doc-number>5640499</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5640499</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="9">
          <text>MCKIEL JR FRANK ALBERT</text>
          <document-id>
            <country>US</country>
            <doc-number>6046722</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US6046722</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <nplcit num="1">
          <text>Blattner et al., "Multimedia Environments for Scientists," IEEE, pp. 348-353, 1991.</text>
        </nplcit>
      </citation>
    </references-cited>
    <parties>
      <applicants>
        <applicant data-format="original" app-type="applicant" sequence="1">
          <addressbook lang="en">
            <orgname>Intel Corporation</orgname>
            <address>
              <address-1>Santa Clara, CA, US</address-1>
              <city>Santa Clara</city>
              <state>CA</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </applicant>
        <applicant data-format="questel" app-type="applicant" sequence="2">
          <addressbook lang="en">
            <orgname>INTEL</orgname>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </applicant>
      </applicants>
      <inventors>
        <inventor data-format="original" sequence="1">
          <addressbook lang="en">
            <name>Miller, John David</name>
            <address>
              <address-1>Beaverton, OR, US</address-1>
              <city>Beaverton</city>
              <state>OR</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
      </inventors>
      <agents>
        <agent sequence="1" rep-type="agent">
          <addressbook lang="en">
            <name>Werner, Raymond J.</name>
          </addressbook>
        </agent>
      </agents>
    </parties>
    <examiners>
      <primary-examiner>
        <name>Hong, Stephen S.</name>
      </primary-examiner>
    </examiners>
    <lgst-data>
      <lgst-status>EXPIRED</lgst-status>
    </lgst-data>
  </bibliographic-data>
  <abstract format="original" lang="en" id="abstr_en">
    <p id="P-EN-00001" num="00001">
      <br/>
      Information objects are compared according to user a specified distance measure and the magnitude of this distance measure is reported to a user by producing audio tones representative of the magnitude of the distance measure between the information objects.
      <br/>
      In one embodiment, a user interface for computers, including audio feedback in which a substantially fixed pitch audio tone is associated with a first selected object, and a second audio tone, associated with a second selected object, varies in pitch as the second object is moved relative to the first object.
      <br/>
      User configurable audio tone parameters allow the use of any function relating variations in audio tone to a distance measure.
    </p>
  </abstract>
  <description format="original" lang="en" id="desc_en">
    <heading>BACKGROUND OF THE INVENTION</heading>
    <p num="1">
      1.
      <br/>
      Field of the Invention
    </p>
    <p num="2">
      The present invention relates to user interfaces for computer systems.
      <br/>
      More particularly, the invention relates to audio cues to assist in manipulation of objects in a computer generated environment.
    </p>
    <p num="3">2. Background</p>
    <p num="4">
      Human-computer interactions have changed over the years from batch processing of punched cards, to on-line text-based interactions known as command line interfaces, to the current generation of graphical user interfaces such as are found in use with most desktop personal computers.
      <br/>
      As the computational capabilities of modern computers have increased so have the uses to which these computers have been applied.
    </p>
    <p num="5">
      These increases in computational power have led to a number of applications which require the manipulation of computer-generated objects in multi-dimensional environments.
      <br/>
      Some techniques have been developed to facilitate the placement of objects in two-dimensional (2D) spaces such as the ability to snap to a grid.
    </p>
    <p num="6">
      Computer applications incorporating three-dimensional (3D) graphics present substantial user-interface challenges due to the increased complexity of the space.
      <br/>
      Working with objects in 3D can be difficult because of the lack of spatial information afforded by the 3D environment.
      <br/>
      In the "real world," shadows, texture velocity, specular highlights, reflections, emmisive and reflective light sources, and other information are available to interpret into size, shape, and position.
      <br/>
      Even photorealistic renderings that are too compute intensive to render in real-time hardly compare to the richness of information provided to our visual senses in the real world.
    </p>
    <p num="7">
      Physiological factors also play a role in human perception.
      <br/>
      In the real world, humans rely on binocular vision to help judge depth, although its utility is limited to about 20 meters.
      <br/>
      A widened field of vision is really the main benefit from having two eyes for tasks such as driving.
    </p>
    <p num="8">
      Also important to human perception of distance is the information received from real or apparent motion.
      <br/>
      Motion parallax is the difference in motion across the visual field, for example, as seen out the side window while riding in a car.
      <br/>
      That is, things near you appear to whiz by, while the horizon seems motionless.
      <br/>
      Kinetic Depth Effect (KDE), is a related phenomenon that gives us shape information about an object when it, or the human viewer undulates.
      <br/>
      Human viewers tend to move back and forth, rather than holding heads and eyes in one position.
      <br/>
      When both human viewer and the viewed objects remain motionless (as in a virtual world), 3D objects may be perceived as flat.
      <br/>
      The primary problem with creating motion parallax and KDE in the virtual world is limited computer performance.
      <br/>
      In other words, animation must be imperceptibly smooth to be effective and not annoying.
      <br/>
      However, frame rates on widely distributed, current generation computer systems are not high enough to produce the proper effect.
    </p>
    <p num="9">
      Current 3D editors require the user to visually check the size and position of objects.
      <br/>
      In order to clearly ascertain the size and position of an object in a complex composition in 3D space (3-space), a user may need to switch between several camera angles and/or numeric display of X/Y/Z axial position and rotation.
      <br/>
      This approach is computationally intensive, tedious, slow, and error-prone.
    </p>
    <p num="10">There is a need for a mechanism for aurally comparing quantitative information.</p>
    <p num="11">
      Further, there is a need for tools to make computer users more effective in 3-space and to overcome the inherent limitations of the artificial 3D environment.
      <br/>
      Such tools as methods and apparatus for quickly and accurately moving and/or sizing objects in a computer representation of a composition in 3-space would make users of 3D computer applications more effective and efficient.
    </p>
    <heading>SUMMARY OF THE INVENTION</heading>
    <p num="12">
      The present invention provides a mechanism for aurally comparing quantitative information.
      <br/>
      Briefly, information objects are compared according to a user specified distance measure and the magnitude of this distance measure is reported to a user by producing audio tones representative of the magnitude of the distance measure between the information objects.
    </p>
    <p num="13">Further, the present invention provides a user interface including audio feedback in which a substantially fixed pitch audio tone is associated with a first selected object, and a second audio tone, associated with a second selected object, varies in pitch as the second object is moved relative to the first object.</p>
    <p num="14">User configurable audio tone parameters allow the use of any function relating variations in audio tone to a distance measure.</p>
    <p num="15">In an alternative embodiment, the audio cues are synthetic speech rather than variable pitched tones.</p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading>
    <p num="16">
      FIG. 1 is a perspective view of two objects in a 3-space composition with a 2D grid and a reference cone.
      <br/>
      FIG. 2 is the same as FIG. 1 but with false shadows projected from the objects to the X-Z plane.
      <br/>
      FIG. 3 is a side view of the image of FIG. 2.
      <br/>
      FIG. 4 is a top view of the image of FIG. 2.
      <br/>
      FIG. 5 shows another example of a perspective view of two objects in a 3-space composition with a 2D grid and a reference cone.
      <br/>
      FIG. 6 shows the 3-space composition of FIG. 5 rotated 90 degrees in the X-Z plane.
      <br/>
      FIG. 7 is a flowchart illustrating the method of the present invention.
      <br/>
      FIG. 8 is a block diagram of the audio-ruler system of the present invention.
      <br/>
      FIG. 9 shows various mappings of the distance measure onto audio scales.
      <br/>
      FIG. 10 illustrates a reference object and an axis indicator in accordance with the present invention.
      <br/>
      FIG. 11 illustrates a selected object relative to the reference object in accordance with the present invention.
      <br/>
      FIG. 12 illustrates a more complex version of the scene depicted in FIG. 11, with selected and deselected objects.
      <br/>
      FIGS. 13(a)-(d) show examples of audio-ruler user interface icons.
    </p>
    <heading>DETAILED DESCRIPTION OF THE INVENTION</heading>
    <p num="17">
      Visual information is highly subject to vantage point and line of sight.
      <br/>
      Visually positioning two objects side-by-side in 3-space requires a user to "eyeball" the comparison from two or more angles, judging by distance and/or motion parallax.
      <br/>
      By presenting information audibly in a computer-generated graphical 3-space environment, the same information obtained from comparison of multiple camera angles, is available independent of the user's vantage point in 3-space relative to the objects being compared.
      <br/>
      Audio response time is much faster than the fastest graphical rendering typically available today.
      <br/>
      This can be critically important, especially in process control or augmented reality systems such as are found in neurosurgical visualization systems.
    </p>
    <p num="18">The present invention provides an "audio-ruler" method and apparatus which allows quantitative information to be communicated to a user by sound.</p>
    <p num="19">Terminology</p>
    <p num="20">User interface refers to the aspects of a computer system or program which can be seen, heard or otherwise perceived by a user of the computer system and the commands and mechanisms used to both control operation of, and input data to, the computer system.</p>
    <p num="21">
      Mouse refers to a common kind of input device connected to a computer system, also known as a pointing device.
      <br/>
      A mouse is moved over a flat horizontal surface and includes some means of converting its position in two dimensions into X-Y coordinates which the computer can read.
      <br/>
      The mouse has one or more buttons whose state can also be read by the computer to which it is attached.
      <br/>
      It is called a mouse because the wire connecting it to the computer or keyboard looks like a mouse's tail.
      <br/>
      Trackballs and joysticks are input devices with similar functionality.
      <br/>
      Wireless pointing devices that communicate with a computer by, for example, infrared signaling, are also available and provide functionality substantially similar to that of the mouse.
    </p>
    <p num="22">
      A window (or windowing) system refers to a facility which allows a computer display screen to be divided into, typically, rectangular areas each of which acts like a separate input/output device under the control of different application programs.
      <br/>
      This gives the user the ability to see the output of several processes at once and to choose which one will receive input by selecting its window, usually by pointing at it with an input device such as a mouse.
      <br/>
      A part of the window system called the window manager can arrange windows on a display screen by moving and resizing windows in response to user input or requests from applications programs running on the computer system.
      <br/>
      Window systems are typically implemented as software running on a computer, although hardware acceleration of some window functions are sometimes used.
      <br/>
      The hardware/software tradeoffs are design choices well-understood by those of ordinary skill in the field of window systems.
    </p>
    <p num="23">
      Graphical user interface (GUI) refers to the use of pictures rather than just words to represent the input and output of a program.
      <br/>
      A program with a GUI runs under some windowing system (e.g., Microsoft Windows.RTM.).
      <br/>
      The program displays certain icons, buttons, and dialogue boxes in its windows on the screen, and the user controls it mainly by moving a pointer on the screen (typically controlled by a mouse) and selecting certain objects by pressing buttons on the mouse while the pointer is pointing at them.
    </p>
    <p num="24">A command line interface requires the user to type textual commands and input at a keyboard, and produces a single stream of text as output.</p>
    <p num="25">Function key refers to a key on a computer keyboard, the activation of which instructs the computer to take a pre-determined action.</p>
    <p num="26">Multimedia refers to human-computer interaction involving text, graphics, audio, voice and video.</p>
    <p num="27">Audio output device refers to speakers, headphones or any other device that can generate sound waves which can be perceived by a computer user.</p>
    <p num="28">
      The term object as used herein refers to a computer representation of some manipulable information.
      <br/>
      A typical object is a visible computer-generated graphic.
      <br/>
      Such an object may represent data of any kind, text, files, video, audio, and so on, or may simply be a drawing element in a graphical design.
      <br/>
      An object may be invisible, i.e., undisplayed.
    </p>
    <p num="29">
      Texture velocity refers to the frequency in a texture pattern as it increases when farther away from an observer.
      <br/>
      For example, consider a rug, a road, or even a field of grass, when looking straight down an observer sees the pattern one way, but as the observer looks further and further out towards the horizon, the repeat of the pattern appears smaller and smaller because of the angle and distance.
    </p>
    <p num="30">
      A hypercube refers to a cube of more than three dimensions.
      <br/>
      A single (20 =1) point (or "node") can be considered as a zero dimensional cube, two (21) nodes joined by a line (or "edge") are a one dimensional cube, four (22) nodes arranged in a square are a two dimensional cube and eight (23) nodes are an ordinary three dimensional cube.
      <br/>
      Continuing this geometric progression, the first hypercube has 24 =16 nodes and is a four dimensional shape (a "four-cube") and an N dimensional cube has 2N nodes (an "N-cube").
      <br/>
      To make an N+1 dimensional cube, take two N dimensional cubes and join each node on one cube to the corresponding node on the other.
      <br/>
      A four-cube can be visualized as a three-cube with a smaller three-cube centered inside it with edges radiating diagonally out (in the fourth dimension) from each node on the inner cube to the corresponding node on the outer cube.
      <br/>
      Nodes in an N dimensional cube are each directly connected to N other nodes.
      <br/>
      Each node can be identified by a set of N Cartesian coordinates where each coordinate is either zero or one.
      <br/>
      Two nodes will be directly connected if they differ in only one coordinate.
    </p>
    <p num="31">
      Click refers to the user action of pressing a button on a mouse or other pointing device.
      <br/>
      This typically generates an event, also specifying the screen position of the cursor, which is then processed by the window manager or application program.
    </p>
    <p num="32">Select refers to the act of clicking when the position of the cursor overlaps the position of a displayed object.</p>
    <p num="33">Drag refers to the user action of selecting an object by clicking on it, and then moving the selected object by moving the mouse while continuing to hold down the mouse button.</p>
    <p num="34">Click, select and drag are fundamental operations in a GUI system.</p>
    <p num="35">Audio-ruler refers to the use of audio cues to provide information to a user regarding the magnitude of an arbitrary attribute of an object relative to a reference object.</p>
    <p num="36">
      MIDI refers to the Musical Instrument Digital Interface published by the MIDI Manufacturers Association, Los Angeles, Calif., USA. More information about MIDI technology can be obtained from the World Wide Web site of the MIDI Manufacturers Association, which can be found at
      <br/>
      http://home.earthlink.net/.about.mma/index.html
    </p>
    <p num="37">Hue/Lightness/Saturation (HLS) refers to a scheme for specifying color that is more natural than the RGB (red, green, blue, additive colors scheme) or the CMYK (cyan, magenta, yellow, black, subtractive colors scheme).</p>
    <p num="38">The 3-D Graphical Environment</p>
    <p num="39">
      Referring to FIGS. 1-4 and 5-6, the difficulty of positioning graphical objects in a 3D environment will be described.
      <br/>
      FIG. 1 is a front view of a first object 100 and a second object 102.
      <br/>
      The visual cues available to an observer indicate that second object 102 is nearer to the observer than first object 100.
      <br/>
      FIG. 2 provides a view of objects 100, 102 with a shadow 104, and a shadow 106 projected onto the X-Z plane.
      <br/>
      The visual cues provided by projected shadows 104, 106 indicate that object 100 and object 102 are actually at the same Z-axis location and that object 100 is only higher than object 104.
      <br/>
      The term "shadow" in this context is used as a term of art, since no single light source could generate such shadows.
      <br/>
      These "false" shadows are used for illustrative purposes to show that the true spatial relationship of objects 102, 104 can only be determined by viewing the composition from more than one camera angle.
      <br/>
      FIG. 3 is a side view illustrating how object 102 actually obscures object 104, making object 104 unviewable from this camera angle.
    </p>
    <p num="40">
      Collectively, FIGS. 1-6 illustrate the difficulty of determining the true spatial relationship of objects in a 3-space composition without viewing the composition from several camera angles.
      <br/>
      One problem with viewing multiple camera angles is the excessive time required by typical computers to render these scenes.
    </p>
    <p num="41">The General Method</p>
    <p num="42">
      Referring to FIG. 7 a preferred embodiment of the present invention is described.
      <br/>
      In a step 702, a user of a computer system having a visual display and audio output capability, selects a first one of the objects displayed on the visual display as a reference object.
      <br/>
      In a step 704, the computer system then generates a first audible tone having a substantially fixed pitch.
      <br/>
      This is referred to as toning.
      <br/>
      Typically this pitch is middle C (261.63 Hz) in an instrument timbre corresponding to the selected axis.
      <br/>
      Of course, any other pitch could be used, for example the A above middle C (440 Hz).
      <br/>
      In a step 706, a second object which is to be positioned, or aligned, with respect to the reference object is then selected.
      <br/>
      In a step 708, the distance measure between the reference object and the second object is determined.
      <br/>
      The computer system then, in a step 710, generates a second audible tone having a variable pitch.
      <br/>
      The pitch of the second audible tone is generated as a function of the distance measure between the reference object and the second object.
      <br/>
      The pitch of the second audible tone approaches the pitch of the first audible tone as the distance measure is reduced.
      <br/>
      The pitch of the first and second audible tones are substantially the same when the distance measure is reduced to zero, or is within some user selected value,  DELTA , of zero.
      <br/>
      In a step 712, the reference tone and the second tone are compared and if the tones are substantially similar, then in a step 714, the second object is placed, i.e., deselected and left in its current location.
      <br/>
      If the comparison of step 712 finds the tones to be not substantially similar, then in a step 716, the second object is further moved and the process repeats from step 708 until the user is satisfied with the location of the second object.
    </p>
    <p num="43">
      The reference tone should be chosen for a particular application based on factors such as how much room is required above and below the reference.
      <br/>
      Also whether environmental considerations favor a higher or lower reference tone.
      <br/>
      Higher tones tend to be perceived more clearly in a noisy environment, however many people have hearing deficiencies at the high end of the human audible spectrum.
    </p>
    <p num="44">The Distance Measure</p>
    <p num="45">
      The distance measure represents the magnitude of an arbitrary attribute of an object relative to a reference object.
      <br/>
      For example the distance measure may be the point to point distance in 3-space between a point P1 =(x1, y1, z1) in the reference object, and a point P2 =(x2, y2, z2) in the selected second object, as given by the Equation (1).
    </p>
    <p num="46">Distance=(x2 -x1 +L )2 +L +(y2 -y1 +L )2 +L +(z2 -z1 +L )2 +L   Equation (1)</p>
    <p num="47">
      In this case the points P1 and P2 may be chosen arbitrarily by the user.
      <br/>
      That is, P1 and P2 may be the center points of their respective objects, the lower left hand corner points, the points with the greatest magnitude y coordinates, or any other user defined algorithm for selection of these points.
    </p>
    <p num="48">Alternatively, the distance measure may represent the distance in a single dimension, or axis, between a point P1 =(x1, y1, z1) in the reference object, and a point P2 =(x2, y2, z2) in the selected second object, as given by the Equations (2)-(4).</p>
    <p num="49">
      Distance=x2 -x1 where X is the selected single dimension Equation (2)
      <br/>
      Distance=y2 -y1 where Y is the selected single dimension Equation (3)
    </p>
    <p num="50">Distance=z2 -z1 where Z is the selected single dimension Equation (4)</p>
    <p num="51">These examples of distance measures are scalable to N dimensions where N is a number greater than zero.</p>
    <p num="52">
      The distance measure can not only be made between two points, as described above, but between any two objects such as a line segment, a ray, a line, a circle, a plane, a polygon, a cube or any higher dimensional objects such as hypercubes or hyperspheres.
      <br/>
      The distance measure may be made to any arbitrary collection of points which make up the reference and selected objects.
    </p>
    <p num="53">
      In some circumstances a user may wish to include an offset in the distance measure computation.
      <br/>
      In other words, rather than trying to make the values of the reference and selected objects equal, the user desires to achieve a fixed offset in N-space between the reference and selected objects.
      <br/>
      As an example of this, Equation (2) is modified as follows:
      <br/>
      Distance=x2 -(x1 +offset) where X is the selected single dimension  Eq. (5)
    </p>
    <p num="54">The distance measure is not limited to the representation of the difference between physical locations, but can also represent other physical quantities including but not limited to such as the magnitude of a voltage difference, the magnitude of a temperature difference, the magnitude of rotor speed difference, the magnitude of manifold pressure difference and so on.</p>
    <p num="55">Specific Example--Apparatus</p>
    <p num="56">
      Referring to FIG. 8, an embodiment of the present invention is described.
      <br/>
      An audio-ruler 802 is typically coupled to a computer system such as a personal computer having a graphic display, a user input device, and audio output device.
      <br/>
      Audio-ruler 802, has a delta scaler 808, which receives a reference signal Vref as an input signal 804, and a comparison value Vsel as input signal 806.
      <br/>
      Vref represents the position in n-space of the reference object.
      <br/>
      Vsel represents the position in n-space of the object selected for comparison.
      <br/>
      Delta scaler 808 provides an output signal Vdelta 810 which is coupled to a pitch scaler 820.
      <br/>
      User programmable control inputs VfineScale 812, VfineRange 814, VcoarseScale 816 and VrefFreq 818 are also coupled to pitch scaler 820.
      <br/>
      Each of user programmable control inputs VfineScale 812, VfineRange 814, VcoarseScale 816 and VrefFreq 818 may be registers to which the computer writes information and from which pitch scaler 820 reads the information.
      <br/>
      Alternatively, these user programmable control inputs may be computer memory locations from which pitch scaler 820 reads information, or the user programmable control inputs may be potentiometers set by slides or knobs.
      <br/>
      The only requirements for the user programmable control inputs are that the user can vary their value, and that pitch scaler 820 can be coupled to these control inputs.
      <br/>
      Pitch scaler 820 provides an output signal VselFreq 822 which is coupled to a tone generator 828.
      <br/>
      Tone generator 828 provides an output signal 830 which is coupled to an audio output device 832.
      <br/>
      User programmable control input VrefFreq 818 is also coupled to a reference tone generator 824 which provides output signal 826.
      <br/>
      Output signal 826 is coupled to audio output device 832.
      <br/>
      Audio output unit 832, may be any device, such as speakers or headphones, capable of producing audible sounds.
      <br/>
      Audio output device 832 provides an audible output 834 which is heard by a user 836.
      <br/>
      The functionality of the various blocks of audio-ruler 802 can be implemented in either an analog or digital manner.
      <br/>
      The preferred embodiment of the present invention implements delta scaler 808, VfineScale 812, VfineRange 814, VcoarseScale 816, VrefFreq 818 and pitch scaler 820 in the digital domain.
    </p>
    <p num="57">Audio Scaling Function</p>
    <p num="58">
      Because the magnitude of the distance measure between the objects to which an audio-ruler can be applied can vary greatly, it is useful to have an audio scaling function which maps, or translates, the steps of the distance measure into steps of the selected pitch scale.
      <br/>
      Delta Scaler 808 performs this audio scaling function.
    </p>
    <p num="59">
      Delta Scaler 808 compares the two input values 804, 806, determines the distance measure, and then generates a resultant delta value, Vdelta 810 as an output.
      <br/>
      The transfer function of delta scaler 808 is chosen to match the scale(s) of Vref 804 and Vsel 806 so as to produce an output scale with the desired response curve and precision.
    </p>
    <p num="60">
      The output delta scale can be any function (e.g., linear, log, exponential), so long as the scale is monotonically increasing with increasing difference between Vref 804 and Vsel 806.
      <br/>
      For example, a Delta Scaler that compares linear Vref and Vsel and produces a linear Vdelta could operate on the input signal as follows:
      <br/>
      Vdelta=(Vsel--Vref)/scale_factor
    </p>
    <p num="61">where scale_factor is chosen to produce the desired sensitivity (precision) to differences in Vref and Vsel.</p>
    <p num="62">
      Pitch scaler 820 produces a monotonically increasing VselFreq above VrefFreq with increasing values of Vdelta&gt;0 and a monotonically decreasing VselFreq below VrefFreq with decreasing values of Vdelta&lt;0.
      <br/>
      VselFreq 822 is generated by a bimodal function where the first function, the "fine scale" (as enumerated by VfineScale) is applied when Vdelta is within +/-VfineRange, else the second function, the "coarse scale" (as enumerated by VcoarseScale) is applied.
      <br/>
      Scales VfineScale 812 and VcoarseScale 816 may include but are not limited to functions such as linear, log, and exponential, or "musical" scales including but not limited to chromatic, major, minor, diatonic, harmonic, melodic, enharmonic, pentatonic, twelve-tone, Lydian mode, Phrygian mode, Dorian mode, and mixolydian.
      <br/>
      In this way, the most appropriate scale can be selected for the application, taking into account the desired number of steps, as specified in VfineRange 814, user skill, locale (Eastern scale), and task.
    </p>
    <p num="63">
      Control input VfineRange 814 is used by pitch scaler 820 to determine the number of steps in the VfineScale before switching to VcoarseScale.
      <br/>
      In a typical application, the range of values covered by VfineScale defines a fine-grained "target" area, while the VcoarseScale provides simpler, ballistic control information.
      <br/>
      That is, in the coarse range, small pitch changes in a continuous tone convey sufficient feedback to user 836 to indicate whether they are moving farther away from, or closer to the goal state.
      <br/>
      User 836 detects being within fine range when the continuous-sweep tone becomes step-wise.
      <br/>
      With a properly selected VfineScale, user 836 can estimate the interval between VrefFreq and VselFreq with sufficient accuracy for the given application task.
    </p>
    <p num="64">Generating Audible Tones</p>
    <p num="65">
      Tone generators 824, 828 can be any one of a number of sound-producing mechanisms, including, but not limited to, digital or analog oscillators, MIDI-controlled synthesizers, and speech synthesizers.
      <br/>
      The audio output of tone generators 824, 828 can be any signal that when input to speakers or headphones produces an audible tone including, but not limited to, a variable-pitch audio tone of any quality or duration.
    </p>
    <p num="66">
      In a preferred embodiment of the present invention tone generator 824, is a MIDI synthesizer.
      <br/>
      Pitch information, i.e., VselFreq 822, received by tone generator 828 from pitch scaler 820 is a number between 1 and 127 inclusive, where middle C is 60.
      <br/>
      The Chromatic scale is expressed simply as VselFreq=VrefFreq+Vdelta.
      <br/>
      FIG. 9 illustrates the lookup values required by other scales, where at Vdelta=0, VselFreq=VrefFreq.
      <br/>
      For example, using the major chord scale yields the following:
    </p>
    <p num="67">
      --
      <br/>
      --                                  MIDI
      <br/>
      --             Vdelta + 1 = middle C + 4 64
      <br/>
      --             Vdelta + 2 = middle C + 7 67
      <br/>
      --             Vdelta + 3 = middle C + 12 72
    </p>
    <p num="68">Whereas, again referring to FIG. 9, using the diatonic scale yields:</p>
    <p num="69">
      --
      <br/>
      --                                  MIDI
      <br/>
      --             Vdelta + 1 = middle C + 2 62
      <br/>
      --             Vdelta + 2 = middle C + 4 64
      <br/>
      --             Vdelta + 3 = middle C + 5 65
    </p>
    <p num="70">
      Speakers, headphones, and other related audio components such as an amplifier or a mixer are selected according to needs of the particular application of the present invention.
      <br/>
      Selection of these audio components is well understood by those of ordinary skill in the field of audio components.
    </p>
    <p num="71">Specific Example--Method</p>
    <p num="72">
      There are many possible applications and variations of the present invention.
      <br/>
      Described below is a specific embodiment of the present invention in a system for positioning objects in a computer-generated 3-space.
    </p>
    <p num="73">
      1. A user selects a reference object.
      <br/>
      2. The user presses a function key to toggle "reference toning" of the reference object.
      <br/>
      Different function keys are used to toggle the X, Y, and Z axes.
      <br/>
      3. The reference object begins to continuously sound an audio tone of "middle C" in an instrument timbre corresponding to the selected axis.
      <br/>
      For example, the X-axis may sound a square wave, Y-axis may sound a triangle wave, and Z-axis may sound a "breathy" pan pipe sound.
      <br/>
      The user may reconfigure these timbres and reference pitch as desired.
      <br/>
      4. The computer system generates a display which visually indicates the reference object and its toning axis by placing an X, Y, or Z character above the reference object.
      <br/>
      As shown in FIG. 10 an object "Ref" 1002 is toning and typically tones in middle C. A graphical indicator "Y" 1004 indicates that other objects will tone their Y position relative to "Ref" 1002.
      <br/>
      5. Optionally, the user may select a different object in the 3D graphical environment.
      <br/>
      6. The newly-selected object continuously sounds an audio tone corresponding to its distance from the reference object along the selected axis.
      <br/>
      This tone is updated with any positional change of the selected object.
      <br/>
      For example, if the object is moved closer to the reference object, the tone moves closer to that of the reference object.
      <br/>
      In one embodiment, the distance is linearly scaled to produce a corresponding pitch on the chromatic scale.
      <br/>
      However, other distance and pitch scaling may be chosen and are user selectable options.
      <br/>
      As shown in FIG. 11, an object "New" 1102 has been positioned at the same height (Y) as "Ref".
      <br/>
      7. If a third object is selected, the second object stops toning (corresponding to its deselection) and the third object begins toning as described above.
      <br/>
      As shown FIG. 12, an object "Foo" 1202 has been selected and is some distance above reference object, "Ref."
      <br/>
      8. The user may turn off all toning by deselecting all objects and pressing the function key corresponding to the currently-toning axis.
    </p>
    <p num="74">Variations and Options</p>
    <p num="75">
      There are many possible variations of the above-described method, all of which can be user programmable.
      <br/>
      Some examples of this include, but are not limited to, the reference pitch which may be any frequency, not just middle C; the reference timbre which may be anything such as a square wave, a triangle wave, a flute sound and so on; the selected-node timbre likewise can be anything such as a square wave, a triangle wave, a flute sound and so on, and may be a different timbre from the reference timbre; tones can be positioned anywhere in the stereo space for binaural stimulation or separation (i.e., panning); reference tone could be placed in one ear with target tones placed in the other;
    </p>
    <p num="76">Spatial audio can be used as additional coding, although by itself spatial audio is currently believed to be too coarse for accurate measurement.</p>
    <p num="77">
      Different applications may require different scales.
      <br/>
      One example can be found in absolute measurements, where the goal is to reduce the difference between the selected and reference objects, the difference in pitch conveys the magnitude of distance measure between them, with both objects sounding substantially the same pitch at the goal state.
      <br/>
      A second example can found in interval measurements, where the goal is to arrange N objects with K world units between each of them.
      <br/>
      In such a case it is useful to employ a musical scale such that the arrangement of properly spaced objects results in an aesthetically pleasing tonal structure, such as a major chord.
      <br/>
      These different scales may be obtained by configuring the pitch scaler to the desired interval ("grid").
    </p>
    <p num="78">
      Graphical user interfaces for computer system often include icons, which are ornamented areas of the computer display screen where, when the cursor is located in that area of the screen and the mouse is clicked, some action is taken.
      <br/>
      Examples 30 of audio-ruler icon ornamental designs are shown in FIGS. 13(a)-(d).
    </p>
    <p num="79">Alternative Signaling--Speech and Sonar Styles</p>
    <p num="80">
      In an alternative embodiment of the present invention, a constant-pitch audio tone whose output is a timed interval of tone pulsations (beats) per second inversely proportional to VselFreq is used.
      <br/>
      This is referred to as the sonar model:
      <br/>
      bing . . . . bing . . . . bing . . bing . . bing . bing beep
    </p>
    <p num="81">
      In a further alternative embodiment of the present invention, user feedback is provided by synthetic speech rather than variably pitched tones.
      <br/>
      In this embodiment a distance measure is converted into speech based cues.
      <br/>
      These cues include, but are not be limited to, readouts of the coordinates of the moving object, and simple positive/negative words, e.g., "warmer, colder", indicating respectively whether moving object is getting closer to, or farther from the reference object.
      <br/>
      Even a is speech sequence such as the following is possible:
      <br/>
      "Cold.
      <br/>
      Cold. Cold. Warmer. Warmer. Colder. Warmer. Getting Warm.
      <br/>
      WARMER. WARMER� WARMER��� Hot� HOT� BINGO�"
    </p>
    <p num="82">Extending the Audio-ruler to Other Applications</p>
    <p num="83">
      The present invention can be used in virtually any circumstance, graphical or not, to help a user determine any arbitrary attribute, setting, or parameter with respect to some reference.
      <br/>
      Presenting information audibly also frees the visual senses to concentrate on something else.
      <br/>
      For example, applying the functionality of the present invention in a voltage meter, could tell a technician whether the voltage at a test point was higher or lower than a reference voltage without the technician taking his or her eyes off of the probe.
    </p>
    <p num="84">
      Some examples of these applications are provided immediately below.
      <br/>
      a) Adjusting the size of objects in a 2D or 3D graphical environment.
      <br/>
      b) Color matching, where HLS or RGB parameters are the "axis".
      <br/>
      c) Aircraft piloting where a reference tone is set at desired altitude, throttle setting, rotor speed, manifold pressure, or the like.
      <br/>
      The pilot's eyes are then free to focus on other aspects of flying the aircraft.
      <br/>
      d) Measuring the similarity of text files; and
      <br/>
      e) For process control, measuring speed, temperature, percent completion, radiation levels, and so on.
    </p>
    <p num="85">Conclusion</p>
    <p num="86">Rather than trying to replicate all natural information in the artificial domain, alternative techniques to convey the most important information in an efficient way are provided by the present invention.</p>
    <p num="87">
      The present invention provides a mechanism which augments visual information with audible tones.
      <br/>
      Users compare the position, size, or any other attribute of two or more objects by comparing their simultaneous audio tones; the farther apart the objects are in that dimension, the farther apart will be their corresponding tones.
      <br/>
      This technique is useful in a variety of environments, even non-graphical ones like text files, voltage meters, aircraft throttles, and so on.
    </p>
    <p num="88">
      The present invention can be embodied in the form of methods and apparatuses for practicing those methods.
      <br/>
      The present invention can also be embodied in the form of computer program code embodied in tangible media, such as floppy diskettes, CD-ROMs, hard drives, or any other computer-readable storage medium, wherein, when the computer program code is loaded into and executed by a computer, the computer becomes an apparatus for practicing the invention.
      <br/>
      The present invention can also be embodied in the form of computer program code, whether stored in a storage medium, loaded into and/or executed by a computer, or transmitted over some transmission medium, such as over electrical wiring or cabling, through fiber optics, or via electromagnetic radiation, wherein, when the computer program code is loaded into and executed by a computer, the computer becomes an apparatus for practicing the invention.
      <br/>
      When implemented on a general-purpose microprocessor, the computer program code segments combine with the microprocessor to provide a unique device that operates analogously to specific logic circuits.
    </p>
    <p num="89">It will be understood that various changes in the details, materials, and arrangements of the parts and steps which have been described and illustrated in order to explain the nature of this invention may be made by those skilled in the art without departing from the principles and scope of the invention as expressed in the subjoined claims.</p>
  </description>
  <claims format="original" lang="en" id="claim_en">
    <claim num="1">
      <claim-text>What is claimed is:</claim-text>
      <claim-text>1.</claim-text>
      <claim-text>A method of positioning an object, said method comprising:</claim-text>
      <claim-text>a) selecting a first object; b) generating a first audible tone; c) selecting a second object, said second object being positioned a distance from said first object; d) generating a second audible tone, said second audible tone being associated with said second object;</claim-text>
      <claim-text>and e) aligning said second object to said first object; f) wherein the first audible tone and the second audible tone are different when the first and second objects are not aligned, the first and second tones are the same when the first and second objects are aligned and the first and second audible tones are produced substantially simultaneously.</claim-text>
    </claim>
    <claim num="2">
      <claim-text>2. The method of claim 1, wherein said first audible tone has a substantially fixed pitch.</claim-text>
    </claim>
    <claim num="3">
      <claim-text>3. The method of claim 1, wherein said second audible tone has a pitch that varies as a function of the distance between said first and second objects.</claim-text>
    </claim>
    <claim num="4">
      <claim-text>4. The method of claim 1, wherein said step of moving said second object comprises moving said second object along a pre-selected axis.</claim-text>
    </claim>
    <claim num="5">
      <claim-text>5. The method of claim 1, wherein said step of generating said second audible tone comprises the steps of: comparing a first and a second set of coordinates associated respectively with said first and second objects, said coordinates being representative of the relative positions of said first and second objects; determining the distance between said first and second objects;</claim-text>
      <claim-text>translating said distance into audio control information representative of pitch; outputting said audio control information to a tone generator;</claim-text>
      <claim-text>and producing said second audible tone.</claim-text>
    </claim>
    <claim num="6">
      <claim-text>6. The method of claim 5, wherein said first audio tone has a substantially fixed pitch.</claim-text>
    </claim>
    <claim num="7">
      <claim-text>7. The method of claim 5, wherein said distance is a distance along a pre-selected axis.</claim-text>
    </claim>
    <claim num="8">
      <claim-text>8. The method of claim 1, further comprising, prior to said step (e) of moving said objects, the step of selecting an axis in which to move said second object.</claim-text>
    </claim>
    <claim num="9">
      <claim-text>9. A method of co-locating, in at least one axis, computer generated graphical objects in a computer generated scene representing a three-dimensional scene, comprising: a) selecting an axis in three dimensional space along which a distance between a first graphical object and a second graphical object is to be determined; b) selecting a first graphical object; c) generating a first audible tone associated with said first graphical object; d) selecting a second graphical object; e) generating a second audible tone associated with said second graphical object, said second audible tone being a function of the distance between said first graphical object and said second graphical object; f) moving said second graphical object along said selected axis until said first and second audible tones are substantially equal;</claim-text>
      <claim-text>and g) displaying an indication of the selected axis.</claim-text>
    </claim>
  </claims>
</questel-patent-document>