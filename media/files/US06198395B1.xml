<?xml version="1.0" encoding="UTF-8"?>
<questel-patent-document lang="en" date-produced="20180805" produced-by="Questel" schema-version="3.23" file="US06198395B1.xml">
  <bibliographic-data lang="en">
    <publication-reference publ-desc="Granted patent as first publication">
      <document-id>
        <country>US</country>
        <doc-number>06198395</doc-number>
        <kind>B1</kind>
        <date>20010306</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>US6198395</doc-number>
      </document-id>
    </publication-reference>
    <original-publication-kind>B1</original-publication-kind>
    <application-reference is-representative="YES" family-id="21801601" extended-family-id="42109384">
      <document-id>
        <country>US</country>
        <doc-number>09020969</doc-number>
        <kind>A</kind>
        <date>19980209</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>1998US-09020969</doc-number>
      </document-id>
      <document-id data-format="questel_Uid">
        <doc-number>43166149</doc-number>
      </document-id>
    </application-reference>
    <language-of-filing>en</language-of-filing>
    <language-of-publication>en</language-of-publication>
    <priority-claims>
      <priority-claim kind="national" sequence="1">
        <country>US</country>
        <doc-number>2096998</doc-number>
        <kind>A</kind>
        <date>19980209</date>
        <priority-active-indicator>Y</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="1">
        <doc-number>1998US-09020969</doc-number>
      </priority-claim>
    </priority-claims>
    <dates-of-public-availability>
      <publication-of-grant-date>
        <date>20010306</date>
      </publication-of-grant-date>
    </dates-of-public-availability>
    <classifications-ipcr>
      <classification-ipcr sequence="1">
        <text>A61F   9/08        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>A</section>
        <class>61</class>
        <subclass>F</subclass>
        <main-group>9</main-group>
        <subgroup>08</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="2">
        <text>A61H   3/06        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>A</section>
        <class>61</class>
        <subclass>H</subclass>
        <main-group>3</main-group>
        <subgroup>06</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="3">
        <text>G09B  21/00        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>09</class>
        <subclass>B</subclass>
        <main-group>21</main-group>
        <subgroup>00</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
    </classifications-ipcr>
    <classification-national>
      <country>US</country>
      <main-classification>
        <text>340573100</text>
        <class>340</class>
        <subclass>573100</subclass>
      </main-classification>
      <further-classification sequence="1">
        <text>340407100</text>
        <class>340</class>
        <subclass>407100</subclass>
      </further-classification>
      <further-classification sequence="2">
        <text>342024000</text>
        <class>342</class>
        <subclass>024000</subclass>
      </further-classification>
      <further-classification sequence="3">
        <text>434116000</text>
        <class>434</class>
        <subclass>116000</subclass>
      </further-classification>
    </classification-national>
    <classifications-ecla>
      <classification-ecla sequence="1">
        <text>A61F-009/08</text>
        <section>A</section>
        <class>61</class>
        <subclass>F</subclass>
        <main-group>9</main-group>
        <subgroup>08</subgroup>
      </classification-ecla>
      <classification-ecla sequence="2">
        <text>A61H-003/06E</text>
        <section>A</section>
        <class>61</class>
        <subclass>H</subclass>
        <main-group>003</main-group>
        <subgroup>06E</subgroup>
      </classification-ecla>
      <classification-ecla sequence="3">
        <text>G09B-021/00B5</text>
        <section>G</section>
        <class>09</class>
        <subclass>B</subclass>
        <main-group>021</main-group>
        <subgroup>00B5</subgroup>
      </classification-ecla>
    </classifications-ecla>
    <patent-classifications>
      <patent-classification sequence="1">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>A61H-003/061</classification-symbol>
        <section>A</section>
        <class>61</class>
        <subclass>H</subclass>
        <main-group>3</main-group>
        <subgroup>061</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="2">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>A61F-009/08</classification-symbol>
        <section>A</section>
        <class>61</class>
        <subclass>F</subclass>
        <main-group>9</main-group>
        <subgroup>08</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="3">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>A61H-2003/063</classification-symbol>
        <section>A</section>
        <class>61</class>
        <subclass>H</subclass>
        <main-group>2003</main-group>
        <subgroup>063</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="4">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>G09B-021/007</classification-symbol>
        <section>G</section>
        <class>09</class>
        <subclass>B</subclass>
        <main-group>21</main-group>
        <subgroup>007</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="5">
        <classification-scheme office="EP" scheme="ICO"/>
        <classification-symbol>K61H-003/06E1T</classification-symbol>
      </patent-classification>
    </patent-classifications>
    <number-of-claims>20</number-of-claims>
    <exemplary-claim>1</exemplary-claim>
    <figures>
      <number-of-drawing-sheets>2</number-of-drawing-sheets>
      <number-of-figures>6</number-of-figures>
      <image-key data-format="questel">US6198395</image-key>
    </figures>
    <invention-title format="original" lang="en" id="title_en">Sensor for sight impaired individuals</invention-title>
    <references-cited>
      <citation srep-phase="examiner">
        <patcit num="1">
          <text>COLES DONALD K</text>
          <document-id>
            <country>US</country>
            <doc-number>3907434</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US3907434</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="2">
          <text>EWART WADE H</text>
          <document-id>
            <country>US</country>
            <doc-number>4008456</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4008456</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="3">
          <text>STANTON AUSTIN N</text>
          <document-id>
            <country>US</country>
            <doc-number>4322744</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4322744</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="4">
          <text>DALLAS JR STANLEY A, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4378569</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4378569</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="5">
          <text>SCHRADER JENS</text>
          <document-id>
            <country>US</country>
            <doc-number>5807111</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5807111</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="6">
          <text>WILLIAMS ROSCOE CHARLES</text>
          <document-id>
            <country>US</country>
            <doc-number>5818381</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5818381</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="7">
          <text>BENJAMIN J MALVERN JR</text>
          <document-id>
            <country>US</country>
            <doc-number>3654477</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US3654477</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="8">
          <text>FISH R</text>
          <document-id>
            <country>US</country>
            <doc-number>3800082</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US3800082</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="9">
          <text>MORICCA LARRY S, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>3993407</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US3993407</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="10">
          <text>MIER RANDALL</text>
          <document-id>
            <country>US</country>
            <doc-number>3996950</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US3996950</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="11">
          <text>ELCHINGER GILBERT M</text>
          <document-id>
            <country>US</country>
            <doc-number>4280204</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4280204</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="12">
          <text>SPIGHT CARL</text>
          <document-id>
            <country>US</country>
            <doc-number>4462046</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4462046</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="13">
          <text>LENHARDT MARTIN L, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5047994</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5047994</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="14">
          <text>MEIJER PETER B L</text>
          <document-id>
            <country>US</country>
            <doc-number>5097326</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5097326</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="15">
          <text>CHI-SHENG HSIEH</text>
          <document-id>
            <country>US</country>
            <doc-number>5097856</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5097856</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="16">
          <text>JORGENSEN ADAM A, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5107467</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5107467</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="17">
          <text>YOULTON FRANCIS W</text>
          <document-id>
            <country>US</country>
            <doc-number>5341346</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5341346</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="18">
          <text>KATIRAIE KAMYAR</text>
          <document-id>
            <country>US</country>
            <doc-number>5347273</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5347273</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="19">
          <text>TODA MINORU, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5442592</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5442592</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="20">
          <text>BRADY MARK J, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5467634</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5467634</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="21">
          <text>KELK GEORGE F</text>
          <document-id>
            <country>US</country>
            <doc-number>5487669</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5487669</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="22">
          <text>PETROFSKY JOSEPH G, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5573001</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5573001</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="1">
          <text>Visionics Corporation, A Brighter Picture A Fuller Life, "The Visionics Low Vision Enhancement System", 4 pages, Visionics Corp., Minneapolis, MN.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="2">
          <text>Susan Gormezano, O.D., Brochure: Understanding the Low Vision Enhancement System, 4 pages (plus cover), Low Vision Associates, P.C., Southfield, MI.</text>
        </nplcit>
      </citation>
      <citation srep-phase="applicant">
        <nplcit num="3">
          <text>WalkMate brochure, Ultrasonic Mobility Enhancement System, 1993, 4 pages, Distributed by Maxi Aids, Farmingdale, NY.</text>
        </nplcit>
      </citation>
    </references-cited>
    <parties>
      <applicants>
        <applicant app-type="applicant" sequence="1">
          <addressbook lang="en">
            <name>SUSSMAN GARY E.</name>
          </addressbook>
        </applicant>
      </applicants>
      <inventors>
        <inventor data-format="original" sequence="1">
          <addressbook lang="en">
            <name>Sussman, Gary E.</name>
            <address>
              <address-1>Bloomfield, MI, 48301, US</address-1>
              <city>Bloomfield</city>
              <state>MI</state>
              <postcode>48301</postcode>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
      </inventors>
      <agents>
        <agent sequence="1" rep-type="agent">
          <addressbook lang="en">
            <orgname>Brooks &amp; Kushman P.C.</orgname>
          </addressbook>
        </agent>
      </agents>
    </parties>
    <examiners>
      <primary-examiner>
        <name>Wu, Daniel J.</name>
      </primary-examiner>
    </examiners>
    <lgst-data>
      <lgst-status>EXPIRED</lgst-status>
    </lgst-data>
  </bibliographic-data>
  <abstract format="original" lang="en" id="abstr_en">
    <p id="P-EN-00001" num="00001">
      <br/>
      A sensory system and method for a visually impaired user includes an array of laser transmitters for transmitting laser signals in given areas corresponding to each of the laser transmitters.
      <br/>
      An array of laser sensors are provided for receiving laser signals reflected from objects in the corresponding given areas with each laser sensor corresponding to a respective one of the array of laser transmitters.
      <br/>
      The time between transmitted and received laser signals of a laser transmitter and the respective laser sensor is indicative of the distance between the user and an object in the corresponding given area.
      <br/>
      A processor is operable with the laser transmitters and laser sensors to effect scanning of the given areas by the array and to process the transmitted and received signals to determine the distance between the user and an object in the given areas.
      <br/>
      A feedback system is operable with the processor for generating a feedback signal for each of the laser sensors as a function of the received laser signals.
      <br/>
      The sensing system and method may also include an array of ultrasonic transducers.
    </p>
  </abstract>
  <description format="original" lang="en" id="desc_en">
    <heading>TECHNICAL FIELD</heading>
    <p num="1">The present invention relates generally to obstacle detection devices for the blind and, more particularly, to a sensing array for the blind.</p>
    <heading>BACKGROUND ART</heading>
    <p num="2">
      A variety of mobility aids have been developed to provide the blind with a means of traveling independently, confidently, and quickly.
      <br/>
      These aids include sensory devices for sensing potential obstacles.
      <br/>
      The sensory devices typically transmit a signal in a beam towards an area and then listen to receive a signal reflected off of an object in the area.
      <br/>
      Because the propagation velocity of the signal is known, the time between the transmit and reflected signals is indicative of the distance of the object from the sensory device.
    </p>
    <p num="3">
      A primary disadvantage with the prior art is in "sensing" the space between objects, as well as sensing the general outline of the objects themselves.
      <br/>
      For example, a blind person walking down the street would like to distinguish between people, mail boxes, trees, etc., and of the gaps between them.
      <br/>
      In essence, the blind person would like to be able to form a mental picture or "footprint" of the surrounding environment.
      <br/>
      The blind person would also like to be able to form the mental picture from tactile stimulation and/or auditory signals as quickly and accurately as possible.
      <br/>
      The prior art is devoid of such a sensory device.
    </p>
    <heading>SUMMARY OF THE INVENTION</heading>
    <p num="4">Accordingly, it is an object of the present invention to provide a sensory system which enables a visually impaired user to obtain a mental picture of the surrounding environment.</p>
    <p num="5">It is another object of the present invention to provide a sensory system having an array of transmitters and sensors operable to enable the visually impaired user to obtain a mental picture of the surrounding environment.</p>
    <p num="6">It is a further object of the present invention to provide a sensory system having an array of ultrasonic transducers.</p>
    <p num="7">It is still another object of the present invention to provide a sensory system having an array of laser transmitters and laser sensors.</p>
    <p num="8">
      In carrying out the above objects and other objects, a sensory system for a visually impaired user is provided.
      <br/>
      The sensory system includes an array of laser transmitters for transmitting laser signals in given areas corresponding to each of the laser transmitters.
      <br/>
      The system further includes an array of laser sensors for receiving laser signals reflected from objects in the given areas with each laser sensor corresponding to a respective one of the laser transmitters.
      <br/>
      The time between transmitted and received laser signals is indicative of the distance between the user and an object in the corresponding given area.
    </p>
    <p num="9">
      A processor is operable with the laser transmitters and laser sensors to effect scanning of the given areas by the array.
      <br/>
      The processor also processes the transmitted and received signals to determine the distance between the user and objects in the given areas.
      <br/>
      A feedback system is operable with the processor for generating a feedback signal for each of the laser sensors as a function of the received laser signal.
    </p>
    <p num="10">
      Further, in carrying out the above objects and other objects, the present invention provides a method for a visually impaired user.
      <br/>
      The method includes transmitting laser signals in given areas corresponding to respective positions.
      <br/>
      The laser signals reflected from objects in the corresponding given areas are then received at the respective positions.
      <br/>
      The time between transmitted and received laser signals at the respective positions is indicative of the distance between the user and an object in the corresponding given area.
    </p>
    <p num="11">
      The transmission and reception of the laser signals at the respective positions may be enabled and disabled periodically to effect scanning of the given areas.
      <br/>
      A feedback signal is generated for each of the positions as a function of the received laser signal.
    </p>
    <p num="12">
      The advantages accruing to the present invention are numerous.
      <br/>
      The sensory system provides a visually impaired user with a footprint mental picture of the environment by scanning horizontally and vertically across a sensor array.
    </p>
    <p num="13">These and other features, aspects, and embodiments of the present invention will become better understood with regard to the following description, appended claims, and accompanying drawings.</p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading>
    <p num="14">
      FIG. 1 is a block view of an embodiment of the sensory system of the present invention;
      <br/>
      FIG. 2 is a block view of another embodiment of the sensory system of the present invention;
      <br/>
      FIG. 3 illustrates two sets of vibratory feedback devices;
      <br/>
      FIG. 4 illustrates a glove for providing tactile feedback;
      <br/>
      FIG. 5 illustrates an audio alarm; and
      <br/>
      FIG. 6 is a block view of another embodiment of the present invention.
    </p>
    <heading>BEST MODES FOR CARRYING OUT THE INVENTION</heading>
    <p num="15">
      Referring now to FIG. 1, an embodiment of a sensory system 10 is shown.
      <br/>
      Sensory system 10 includes a visor device 11 worn by a visually impaired user 12.
      <br/>
      Visor device 11 may extend on the face of user 12 between the ears of the user to provide feedback on objects in front of the user.
      <br/>
      Visor device 11 may also be configured to be wrapped around the head and face of user 12 (not specifically shown) to provide feedback on objects around the user.
      <br/>
      In this configuration, visor device 11 enables user 12 to have 360 (degree)  peripheral vision.
    </p>
    <p num="16">
      Visor device 11 includes an array of laser transmitters 14 and laser sensors 16.
      <br/>
      As shown in FIG. 1, visor device 11 is divided into a plurality of segments.
      <br/>
      Each segment has a transmitter 14 and a corresponding sensor 16.
    </p>
    <p num="17">
      Transmitters 14 transmit laser signals along a highly directional beam at a given area in front of, or around, user 12.
      <br/>
      Transmitters 14 are arranged and pointed such that each transmitter transmits toward a zone in the given area.
      <br/>
      Preferably, each transmitter 14 transmits a laser signal perpendicular to user 12.
      <br/>
      Thus, the given area is divided into a plurality of zones with each zone corresponding to one of transmitters 14.
    </p>
    <p num="18">
      Sensors 16 are operable to receive laser signals reflected from objects in the given area.
      <br/>
      Each sensor 16 corresponds to one of transmitters 14 and is operable to receive laser signals reflected from objects in one of the plurality zones.
      <br/>
      The time between a laser signal transmitted by one of transmitters 14 and received by a corresponding sensor 16 is indicative of the distance between user 12 and an object within the respective zone.
    </p>
    <p num="19">
      Sensory system 10 further includes a processor 18.
      <br/>
      Processor 18 includes a power supply for supplying power to transmitters 14 for transmitting laser signals.
      <br/>
      Processor 18 processes the laser signals received by sensors 16 to determine the time between transmitted and received signals.
      <br/>
      Processor 18 then determines the distance between user 12 and an object within a respective zone of one of transmitters 14.
    </p>
    <p num="20">
      Processor 18 scans the array of transmitters 14.
      <br/>
      For instance, processor 18 controls the array such that one transmitter 14 transmits a signal while the other transmitters are idle.
      <br/>
      Processor 18 is further operable to determine if sensor 16 corresponding to transmitter 14 receives a reflected signal.
      <br/>
      This process is sequentially continued with the next transmitter 14 transmitting a signal and the corresponding sensor 16 set for receiving a reflected signal.
    </p>
    <p num="21">
      Preferably, array of transmitters 14 are swept along a horizontal sweep and then in a vertical sweep.
      <br/>
      To effect scanning of transmitters 14, processor 18 enables and disables each of the transmitters by turning on and shutting off the supply power to the transmitters periodically.
    </p>
    <p num="22">
      Alternatively, processor 18 is operable to provide continuous supply power to transmitters 14 so that the transmitters are transmitting signals continuously.
      <br/>
      In this alternative embodiment, processor 18 is operable to process all of the reflected signals received by sensors 16.
      <br/>
      Processor 18 then performs multiplexing, such as time division multiplexing (TDM), with the sensors 16 such that the sensors scan the given area for reflected signals.
      <br/>
      Alternatively, processor 18 is operable such that each element of the array continuously transmits and receives signals without scanning.
    </p>
    <p num="23">
      A feedback system 20 is operable with processor 18 to generate audible and/or tactile feedback signals for user 12.
      <br/>
      The feedback signals are generated as a function of the received laser signal, i.e., the distance between user 12 and an object in a zone associated with a respective one of transmitters 14.
      <br/>
      From the different feedback signals generated during the scanning of the array of transmitters 14 and/or sensors 16, user 12 may form a mental picture of the objects within the surrounding environment.
      <br/>
      Because of the large numbers and highly directional capability of transmitters 14, sensory system 10 allows user 12 to distinguish objects in the given area by using the different feedback signals.
    </p>
    <p num="24">
      Transmitters 14 and sensors 16 of the array may be scanned by electrical or mechanical means as known to those of ordinary skill in the art.
      <br/>
      Of course, a single source transmitter/sensor pair may be used in place of the array and scanned by electrical or mechanical means.
    </p>
    <p num="25">
      Referring now to FIG. 2, an alternative embodiment of a sensory system 30 is shown.
      <br/>
      Sensory system 30 differs from sensory system 10 in that visor device 32 includes an array of ultrasonic transducers 34 instead of laser transmitters 14 and laser sensors 16.
      <br/>
      Transducers 34 transmit ultrasonic signals and are arranged in an array such that each transducer transmits toward a zone in the given area.
      <br/>
      Thus, the given area in front of user 12 is divided into a plurality of zones with each zone corresponding to one of transducers 34.
    </p>
    <p num="26">
      Transducers 34 are also operable to receive ultrasonic signals reflected from objects in the given area.
      <br/>
      Transducers 34 have the same reception and transmission patterns so that a transducer receives ultrasonic signals reflected from an object in a respective zone in the given area.
      <br/>
      The time between an ultrasonic signal transmitted by one of transducers 34 and received by that transducer is indicative of the distance between user 12 and an object within the respective zone.
    </p>
    <p num="27">
      Processor 18 determines the time between transmitted and received ultrasonic signals.
      <br/>
      Processor 18 then determines the distance between user 12 and an object within a respective zone of one of transducers 34.
      <br/>
      Feedback system 20 is operable with processor 18 to generate a feedback signal for each of transducers 34 as a function of the received ultrasonic signal.
    </p>
    <p num="28">
      The feedback signals provided by feedback system 20 may take on a variety of forms.
      <br/>
      For instance, as shown in FIG. 3, feedback system 20 may be operable with two sets 42(a-b) of a plurality of vibrators 44.
      <br/>
      Vibrators 44 provide tactile stimulation to user 12.
      <br/>
      Sets 42(a-b) may be formed on an adhesive suitable for placing on the hands or sides of the face of user 16.
      <br/>
      Sets 42(a-b) may also be implanted into the body of user 12.
      <br/>
      Each vibrator 44 is associated with a respective sensor 16 (or transducer 34).
      <br/>
      Thus, each vibrator 44 corresponds to a respective zone within the given area.
      <br/>
      Other feedback forms include force-surface pressure signals.
    </p>
    <p num="29">
      When a sensor 16 receives a reflected signal transmitted by a corresponding transmitter 14, feedback system 20 stimulates that vibrator 44 associated with that sensor to notify user 12 of the distance between him and an object within a zone in a given area.
      <br/>
      Feedback system 20 stimulates the various vibrators 44 as the array of transmitters 14 and sensors 16 are scanned so that user 12 can form a mental picture of the surrounding environment.
    </p>
    <p num="30">
      Feedback system 20 varies the stimulation of a vibrator 44 as a function of the distance between user 12 and an object.
      <br/>
      For instance, the frequency or magnitude of vibration may be varied as a function of the distance between user 12 and an object.
      <br/>
      A higher frequency or magnitude may imply that the object is close to user 12.
      <br/>
      On the other hand, a lower frequency or magnitude may imply that the object is far from user 12.
      <br/>
      Vibrators 44 may also be configured to apply heat and/or force-surface pressure as a function of the distance between user 12 and an object.
    </p>
    <p num="31">
      Referring now to FIG. 4, feedback system 30 is operable with a pair of gloves 46 (only one shown) for the hands of user 12.
      <br/>
      Glove 46 is provided with muscle wires 48.
      <br/>
      Muscle wires 48 include shaped memory alloys (SMA), solenoid switches, or the like.
      <br/>
      Muscle wires 48 tighten in response to a supplied voltage.
      <br/>
      Accordingly, each muscle wire is either on or off and provides two bits of data.
      <br/>
      Glove 46 is shown in FIG. 4 as having three muscle wires 48 for each finger and thumb portion of the glove.
      <br/>
      Thus, each finger and thumb portions have eight bits of data.
      <br/>
      Each finger and thumb portion of glove 46 preferably corresponds to one of transmitters 14 and sensors 16.
      <br/>
      In this case, the array of transmitters 14 and sensors 16 consists of ten transmitter/sensor pairs with each finger and thumb portion corresponding to one of the pairs.
    </p>
    <p num="32">
      The eight bits of data provided by the three muscle wires 48 is indicative of detected objects in the zone covered by the corresponding pair.
      <br/>
      For example, each bit may represent three feet that an object in a zone is near user 12.
      <br/>
      Muscle wires 48 may be configured to provide different amounts of tightening.
      <br/>
      For instance, muscle wires 48 may provide strong and weak tightening in addition to no tightening.
      <br/>
      In this case, each muscle wire 48 provides three (instead of two) bits of information.
      <br/>
      As a result, twenty seven bits of data are provided by three muscle wires and each bit may represent one foot that an object in a zone is near user 12.
    </p>
    <p num="33">
      Referring now to FIG. 5, feedback system 20 may also include an audio amplifier 56.
      <br/>
      Audio amplifier 56 is connected to a pair of ear speakers 58(a-b) by speaker cable 60.
      <br/>
      Audio amplifier 56 is associated with each of sensors 16 (or transducers 34) for providing audio feedback signals to user 12.
      <br/>
      Other devices instead of pair of ear speakers 58(a-b) for providing the audio feedback signals to user 12 include a single ear plug, a "bone phone", etc.
    </p>
    <p num="34">
      Audio amplifier 56 provides an audio feedback signal for a respective one of sensors 16.
      <br/>
      The audio feedback signals may have different tones (frequencies) for each sensor 16.
      <br/>
      For example, there may be twenty different tones for twenty sensors.
    </p>
    <p num="35">
      Each audio feedback signal corresponds to a respective zone within the given area.
      <br/>
      When a transmitter 14 transmits and the corresponding sensor 16 receives a laser signal, audio amplifier 56 generates a particular audio feedback signal associated with that sensor to notify user 12 of the distance between him and an object within a zone in a given area.
      <br/>
      Feedback system 20 generates the various audio feedback signals as transmitters 14 and sensors 16 are scanned so that user 12 can form a mental picture of the surrounding environment.
    </p>
    <p num="36">
      Audio amplifier 56 varies the volume of the audio feedback signals as a function of the distance between user 12 and an object.
      <br/>
      A higher volume may imply that the object is close to user 12.
      <br/>
      On the other hand, a lower volume may imply that the object is far from user 12.
    </p>
    <p num="37">
      Audio amplifier 56 may also provide audio feedback signals to user 12 as a function of the interaural volume.
      <br/>
      In this embodiment, audio amplifier 56 generates audio feedback signals having ten different tones for two sets of ten sensors.
      <br/>
      Each tone is associated with a respective sensor in the two sets.
      <br/>
      Audio amplifier 56 varies the volume of the audio feedback signal provided to each ear as a function of the distance between user 12 and objects within the two zones corresponding to the two transducers.
    </p>
    <p num="38">
      The feedback signals may be provided to user 12 in different patterns during scanning of the array of transmitters 14 and sensors 16.
      <br/>
      Feedback system 20 may generate the audio feedback signal for a respective sensor 16 during the time that the corresponding transmitter 14 is transmitting a laser signal.
      <br/>
      The audio feedback signal is turned off once the associated transmitter 14 and/or sensor 16 is disabled.
      <br/>
      In this embodiment, the audio feedback signals follow the scanning of the array of transmitters 14 and sensors 16.
      <br/>
      For instance, stimulation of vibrators 44 moves horizontally and vertically across the hands of user 12 during scanning.
      <br/>
      Similarly, the tones of the audio alarm signals change during scanning.
    </p>
    <p num="39">
      Alternatively, feedback system 20 may generate an audio feedback signal for a sensor 16 and then hold that signal for a period of time until the sensor receives a different reflected signal.
      <br/>
      Once a different reflected signal is received, feedback system 20 generates a new audio feedback signal.
    </p>
    <p num="40">In another alternative embodiment, feedback system 20 may generate an audio feedback signal for a respective sensor 16 and then hold that signal until a scanning cycle of the array of transmitters 14 and sensors 16 has been completed.</p>
    <p num="41">
      Referring now to FIG. 6, an alternative embodiment of a sensory system 60 is shown.
      <br/>
      Sensory system 60 includes a visor device 62 worn by user 12.
      <br/>
      Visor device has ultrasonic transducers 64.
      <br/>
      Sensory system 60 uses echo location similar to that used by whales, dolphins, and bats.
      <br/>
      To this end, transducers 64 transmit ultrasonic signals for a given period of time.
      <br/>
      Transducers 64 then listen to receive ultrasonic signals which are reflected from objects around user 12.
    </p>
    <p num="42">
      A processor 66 is operable with transducers 64 to perform signal processing.
      <br/>
      Processor 66 sums all of the received signals from transducers 64 to form a sum signal.
      <br/>
      The sum signal corresponds to the objects surrounding user 12.
      <br/>
      Thus, the sum signal changes as the objects surrounding user 12 change orientation and position.
    </p>
    <p num="43">
      A feedback system 68 operable with processor 66 then shifts the frequency of the sum signal down to an audible frequency for user 12.
      <br/>
      User 12 then listens to the downshifted frequency sum signal.
      <br/>
      By being able to differentiate the sum signals through training and experience, user 12 may be able to form a mental picture of the surrounding environment.
    </p>
    <p num="44">
      It should be noted that the present invention may be used in a wide variety of different constructions encompassing many alternatives, modifications, and variations which are apparent to those with ordinary skill in the art.
      <br/>
      Accordingly, the present invention is intended to embrace all such alternatives, modifications, and variations as fall within the spirit and broad scope of the appended claims.
    </p>
    <p num="45">
      The embodiments of the sensory systems of the present invention share the feature of providing enough data to a user such that, through training and experience, the user can use the data to form a mental picture of the environment.
      <br/>
      In essence, instead of processing light for vision like sight unimpaired individuals, the sight impaired user processes the feedback data for vision.
    </p>
    <p num="46">
      To this end, other sensory system embodiments, such as machine vision, may be utilized.
      <br/>
      For instance, information on the surrounding environment is processed by a machine vision computer.
      <br/>
      The computer can then process the information to form a signal indicative of the surrounding environment.
      <br/>
      Through experience, a sight-impaired user can distinguish among signals to form a mental picture of the surrounding environment.
    </p>
  </description>
  <claims format="original" lang="en" id="claim_en">
    <claim num="1">
      <claim-text>What is claimed is:</claim-text>
      <claim-text>1.</claim-text>
      <claim-text>A sensory system for a visually impaired user comprising:</claim-text>
      <claim-text>an array of laser transmitters for transmitting laser signals in a total coverage area consisting of a plurality of given areas, each of the laser transmitters transmitting a laser signal in a corresponding given area; an array of laser sensors for receiving laser signals reflected from objects in the total coverage area, each of the laser sensors corresponding to a respective one of the array of laser transmitters for receiving a reflected laser signal from an object in the corresponding given area, wherein the time between transmitted and received laser signals of a laser transmitter and the respective laser sensor is indicative of the distance between the user and an object in the corresponding given area; a processor operable with the array of laser transmitters and laser sensors to process the transmitted and received laser signals to determine the distance between the user and objects in each of the given areas;</claim-text>
      <claim-text>and a feedback system operable with the processor for generating a feedback signal for each of the laser sensors, the feedback system generating each feedback signal as a function of the distance between an object in the corresponding given area and the user, wherein the user uses all of the feedback signals corresponding to each of the given areas to establish a mental picture of the objects in the total coverage area.</claim-text>
    </claim>
    <claim num="2">
      <claim-text>2. The system of claim 1 wherein: the processor is operable to periodically enable and disable the array of laser transmitters and laser sensors to effect scanning of the given areas of the total coverage area.</claim-text>
    </claim>
    <claim num="3">
      <claim-text>3. The system of claim 2 wherein: the feedback system generates the feedback signal for each of the laser sensors during the time that the respective laser transmitter is enabled.</claim-text>
    </claim>
    <claim num="4">
      <claim-text>4. The system of claim 2 wherein: the feedback system generates the feedback signal for each of the laser sensors until the respective laser transmitter is subsequently enabled.</claim-text>
    </claim>
    <claim num="5">
      <claim-text>5. The system of claim 2 wherein: the feedback system generates the feedback signal for each of the laser transmitters until a scanning cycle of the array of laser transmitters has been completed.</claim-text>
    </claim>
    <claim num="6">
      <claim-text>6. The system of claim 1 wherein the alarm system comprises: a plurality of vibrators with at least one vibrator being associated with each of the laser sensors for generating a vibratory feedback signal for the respective laser sensor.</claim-text>
    </claim>
    <claim num="7">
      <claim-text>7. The system of claim 6 wherein: the plurality of vibrators have a given frequency of vibration, wherein the given frequency of vibration is a function of the distance between the user and the object in the corresponding given area.</claim-text>
    </claim>
    <claim num="8">
      <claim-text>8. The system of claim 1 wherein the feedback system comprises: an audio amplifier associated with each of the laser sensors for generating an audio feedback signal for the respective laser sensor.</claim-text>
    </claim>
    <claim num="9">
      <claim-text>9. The system of claim 8 wherein: the audio feedback signal has a given audio frequency, wherein a given audio frequency is a function of the distance between the user and the object in the corresponding given area.</claim-text>
    </claim>
    <claim num="10">
      <claim-text>10. The system of claim 8 wherein: the audio feedback signal has a given volume, wherein a given volume is a function of the distance between the user and the object in the corresponding given area.</claim-text>
    </claim>
    <claim num="11">
      <claim-text>11. A method for a visually impaired user comprising: transmitting a laser signal in each of a corresponding given area of a total coverage area; receiving laser signals reflected from objects in the total coverage area, each of the reflected laser signals being reflected from an object in the corresponding given area with each received laser signal corresponding to a respective transmitted laser signal, wherein the time between transmitted and received laser signals is indicative of the distance between the user and an object in the corresponding given area; processing the transmitted and received laser signals to determine the distance between the user and objects in each of the given areas;</claim-text>
      <claim-text>and generating a feedback signal for each of the received laser signals, each feedback signal generated as a function of the distance between the object in the corresponding given area and the user, wherein the user uses all of the feedback signals corresponding to each of the given areas to establish a mental picture of the objects in the total coverage area.</claim-text>
    </claim>
    <claim num="12">
      <claim-text>12. The method of claim 11 further comprising: enabling and disabling the transmission and reception of the laser signals periodically to effect scanning of the given areas.</claim-text>
    </claim>
    <claim num="13">
      <claim-text>13. The method of claim 11 wherein: the feedback signal is a plurality of vibrations with at least one vibrator being associated with each of the received signals for generating the feedback signal for the corresponding given areas.</claim-text>
    </claim>
    <claim num="14">
      <claim-text>14. The method of claim 13 wherein: the plurality of vibrators have a given frequency of vibration, wherein the given frequency of vibration is a function of the distance between the user and the object in the corresponding given area.</claim-text>
    </claim>
    <claim num="15">
      <claim-text>15. The method of claim 11 wherein: the feedback signal is an audio signal.</claim-text>
    </claim>
    <claim num="16">
      <claim-text>16. The method of claim 11 wherein: the audio signal has given audio frequencies, wherein a given audio frequency is a function of the distance between the user and the object in the corresponding given area.</claim-text>
    </claim>
    <claim num="17">
      <claim-text>17. The method of claim 16 wherein: the audio signal has a given volume, wherein a given volume is a function of the distance between the user and the object in the corresponding given area.</claim-text>
    </claim>
    <claim num="18">
      <claim-text>18. A sensory system for a visually impaired user comprising: an array of ultrasonic transducers for transmitting ultrasonic signals in the environment surrounding the user, each of the ultrasonic transducers further operable to receive ultrasonic signals reflected from objects in the surrounding environment; a processor operable with the array of ultrasonic transducers to process the reflected ultrasonic signals received by the transducers, wherein the processor processes the received signals to form a sum signal;</claim-text>
      <claim-text>and a feedback system operable with the processor for generating an audio feedback signal as a function of the sum signal by down shifting the frequency of the sum signal to an audible frequency, wherein the user listens to the audio feedback signal to determine objects in the surrounding environment.</claim-text>
    </claim>
    <claim num="19">
      <claim-text>19. A sensing system for a visually impaired user comprising: an array of ultrasonic transducers for transmitting and receiving ultrasonic signals in a total coverage area consisting of a plurality of given areas corresponding to each of the ultrasonic transducers, wherein the time between transmitted and received ultrasonic signals of an ultrasonic transducer is indicative of the distance between the user and an object in the corresponding given area; a processor operable with the array of ultrasonic transducers to process the transmitted and received ultrasonic signals to determine the distance between the user and objects in each of the given area;</claim-text>
      <claim-text>and a feedback system operable with the processor for generating a feedback signal for each of the ultrasonic transducers, the feedback system generating each feedback signal as a function of the distance between an object in the corresponding given area and the user, wherein the user uses all of the feedback signals corresponding to each of the given areas to establish a mental picture of the objects in the total coverage area.</claim-text>
    </claim>
    <claim num="20">
      <claim-text>20. The system of claim 19 wherein: the processor is operable to periodically enable and disable the array of ultrasonic transducers to effect scanning of the given areas by the array.</claim-text>
    </claim>
  </claims>
</questel-patent-document>