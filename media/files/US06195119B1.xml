<?xml version="1.0" encoding="UTF-8"?>
<questel-patent-document lang="en" date-produced="20180805" produced-by="Questel" schema-version="3.23" file="US06195119B1.xml">
  <bibliographic-data lang="en">
    <publication-reference publ-desc="Granted patent as first publication">
      <document-id>
        <country>US</country>
        <doc-number>06195119</doc-number>
        <kind>B1</kind>
        <date>20010227</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>US6195119</doc-number>
      </document-id>
    </publication-reference>
    <original-publication-kind>B1</original-publication-kind>
    <application-reference family-id="27003023" extended-family-id="13699862">
      <document-id>
        <country>US</country>
        <doc-number>09004264</doc-number>
        <kind>A</kind>
        <date>19980108</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>1998US-09004264</doc-number>
      </document-id>
      <document-id data-format="questel_Uid">
        <doc-number>13995104</doc-number>
      </document-id>
    </application-reference>
    <language-of-filing>en</language-of-filing>
    <language-of-publication>en</language-of-publication>
    <priority-claims>
      <priority-claim kind="national" sequence="1">
        <country>US</country>
        <doc-number>426498</doc-number>
        <kind>A</kind>
        <date>19980108</date>
        <priority-active-indicator>N</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="1">
        <doc-number>1998US-09004264</doc-number>
      </priority-claim>
      <priority-claim kind="national" sequence="2">
        <country>US</country>
        <doc-number>50298495</doc-number>
        <kind>A</kind>
        <date>19950717</date>
        <priority-linkage-type>1</priority-linkage-type>
        <priority-active-indicator>Y</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="2">
        <doc-number>1995US-08502984</doc-number>
      </priority-claim>
      <priority-claim kind="national" sequence="3">
        <country>US</country>
        <doc-number>36563694</doc-number>
        <kind>A</kind>
        <date>19941228</date>
        <priority-linkage-type>2</priority-linkage-type>
        <priority-active-indicator>Y</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="3">
        <doc-number>1994US-08365636</doc-number>
      </priority-claim>
    </priority-claims>
    <dates-of-public-availability>
      <publication-of-grant-date>
        <date>20010227</date>
      </publication-of-grant-date>
    </dates-of-public-availability>
    <term-of-grant>
      <disclaimer/>
    </term-of-grant>
    <classifications-ipcr>
      <classification-ipcr sequence="1">
        <text>A61B   5/107       20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>5</main-group>
        <subgroup>107</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="2">
        <text>G01B  11/02        20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>01</class>
        <subclass>B</subclass>
        <main-group>11</main-group>
        <subgroup>02</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
    </classifications-ipcr>
    <classification-national>
      <country>US</country>
      <main-classification>
        <text>348065000</text>
        <class>348</class>
        <subclass>065000</subclass>
      </main-classification>
    </classification-national>
    <classifications-ecla>
      <classification-ecla sequence="1">
        <text>A61B-005/107J</text>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>005</main-group>
        <subgroup>107J</subgroup>
      </classification-ecla>
      <classification-ecla sequence="2">
        <text>G01B-011/02</text>
        <section>G</section>
        <class>01</class>
        <subclass>B</subclass>
        <main-group>11</main-group>
        <subgroup>02</subgroup>
      </classification-ecla>
    </classifications-ecla>
    <patent-classifications>
      <patent-classification sequence="1">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>A61B-005/1076</classification-symbol>
        <section>A</section>
        <class>61</class>
        <subclass>B</subclass>
        <main-group>5</main-group>
        <subgroup>1076</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="2">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>G01B-011/02</classification-symbol>
        <section>G</section>
        <class>01</class>
        <subclass>B</subclass>
        <main-group>11</main-group>
        <subgroup>02</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="3">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>H04N-2005/2255</classification-symbol>
        <section>H</section>
        <class>04</class>
        <subclass>N</subclass>
        <main-group>2005</main-group>
        <subgroup>2255</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>A</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
      <patent-classification sequence="4">
        <classification-scheme office="EP" scheme="ICO"/>
        <classification-symbol>T04N-005/225E</classification-symbol>
      </patent-classification>
    </patent-classifications>
    <number-of-claims>7</number-of-claims>
    <exemplary-claim>1</exemplary-claim>
    <figures>
      <number-of-drawing-sheets>15</number-of-drawing-sheets>
      <number-of-figures>29</number-of-figures>
      <image-key data-format="questel">US6195119</image-key>
    </figures>
    <invention-title format="original" lang="en" id="title_en">Digitally measuring scopes using a high resolution encoder</invention-title>
    <references-cited>
      <citation srep-phase="examiner">
        <patcit num="1">
          <text>CHIKAMA T</text>
          <document-id>
            <country>US</country>
            <doc-number>3730632</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US3730632</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="2">
          <text>KAWAHARA I</text>
          <document-id>
            <country>US</country>
            <doc-number>3817635</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US3817635</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="3">
          <text>KAWAHARA I</text>
          <document-id>
            <country>US</country>
            <doc-number>3819267</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US3819267</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="4">
          <text>GRANT ROBERT L, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4207594</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4207594</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="5">
          <text>HATTORI SHINICHIRO</text>
          <document-id>
            <country>US</country>
            <doc-number>4343300</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4343300</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="6">
          <text>SATO MASAMICHI, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4488039</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4488039</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="7">
          <text>OKADA MINORU</text>
          <document-id>
            <country>US</country>
            <doc-number>4558691</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4558691</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="8">
          <text>SIEGMUND WALTER P</text>
          <document-id>
            <country>US</country>
            <doc-number>4588294</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4588294</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="9">
          <text>ZOBEL JUERGEN</text>
          <document-id>
            <country>US</country>
            <doc-number>4702229</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4702229</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="10">
          <text>SHISHIDO YOSHIO, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4711999</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4711999</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="11">
          <text>SHISHIDO YOSHIO, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4737622</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4737622</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="12">
          <text>NINAN CHAMPIL A, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4742815</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4742815</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="13">
          <text>ISHIDA TOKUJI, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4783701</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4783701</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="14">
          <text>DIENER JOERG</text>
          <document-id>
            <country>US</country>
            <doc-number>4820043</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4820043</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="15">
          <text>CHEN CHINGFA, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4844071</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4844071</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="16">
          <text>OHSAWA AKIRA</text>
          <document-id>
            <country>US</country>
            <doc-number>4905668</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4905668</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="17">
          <text>LIA RAYMOND A</text>
          <document-id>
            <country>US</country>
            <doc-number>4980763</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4980763</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="18">
          <text>LOBB DANIEL R, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5045936</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5045936</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="19">
          <text>SALVATI JON R, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5070401</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5070401</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="20">
          <text>LOBB DANIEL R</text>
          <document-id>
            <country>US</country>
            <doc-number>5214538</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5214538</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="21">
          <text>KUBAN DANIEL P, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5313306</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5313306</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="22">
          <text>FELDSTEIN DAVID A, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5347987</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5347987</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="23">
          <text>SALVATI JON R, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5373317</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5373317</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="24">
          <text>YAMADA KUNIHIKO, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5455649</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5455649</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="25">
          <text>DIANNA ANDREAS E, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5573492</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5573492</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="26">
          <text>DIANNA ANDREAS E, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5801762</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5801762</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="27">
          <text>FUJIKURA LTD</text>
          <document-id>
            <country>JP</country>
            <doc-number>H05288998</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>JP05288998</doc-number>
          </document-id>
        </patcit>
      </citation>
    </references-cited>
    <related-documents>
      <continuation>
        <relation>
          <parent-doc>
            <document-id>
              <country>US</country>
              <doc-number>50298495</doc-number>
              <kind>A</kind>
              <date>19950717</date>
            </document-id>
          </parent-doc>
        </relation>
        <relation>
          <parent-doc>
            <document-id>
              <country>US</country>
              <doc-number>5801762</doc-number>
              <kind>A</kind>
            </document-id>
          </parent-doc>
        </relation>
      </continuation>
      <continuation-in-part>
        <relation>
          <parent-doc>
            <document-id>
              <country>US</country>
              <doc-number>36563694</doc-number>
              <kind>A</kind>
              <date>19941228</date>
            </document-id>
          </parent-doc>
        </relation>
        <relation>
          <parent-doc>
            <document-id>
              <country>US</country>
              <doc-number>5573492</doc-number>
              <kind>A</kind>
            </document-id>
          </parent-doc>
        </relation>
      </continuation-in-part>
    </related-documents>
    <parties>
      <applicants>
        <applicant data-format="original" app-type="applicant" sequence="1">
          <addressbook lang="en">
            <orgname>Olympus America, Inc.</orgname>
            <address>
              <address-1>Melville, NY, US</address-1>
              <city>Melville</city>
              <state>NY</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </applicant>
        <applicant data-format="questel" app-type="applicant" sequence="2">
          <addressbook lang="en">
            <orgname>OLYMPUS AMERICA</orgname>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </applicant>
      </applicants>
      <inventors>
        <inventor data-format="original" sequence="1">
          <addressbook lang="en">
            <name>Dianna, Andreas E.</name>
            <address>
              <address-1>Walnut Port, PA, US</address-1>
              <city>Walnut Port</city>
              <state>PA</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
        <inventor data-format="original" sequence="2">
          <addressbook lang="en">
            <name>Costello, James G.</name>
            <address>
              <address-1>Huntington, NY, US</address-1>
              <city>Huntington</city>
              <state>NY</state>
              <country>US</country>
            </address>
          </addressbook>
          <nationality>
            <country>US</country>
          </nationality>
        </inventor>
      </inventors>
      <agents>
        <agent sequence="1" rep-type="agent">
          <addressbook lang="en">
            <orgname>Kenyon &amp; Kenyon</orgname>
          </addressbook>
        </agent>
      </agents>
    </parties>
    <examiners>
      <primary-examiner>
        <name>Le, Vu</name>
      </primary-examiner>
    </examiners>
    <lgst-data>
      <lgst-status>LAPSED</lgst-status>
    </lgst-data>
  </bibliographic-data>
  <abstract format="original" lang="en" id="abstr_en">
    <p id="P-EN-00001" num="00001">
      <br/>
      A system for determining a dimension of a detail including an optical scope for producing an image of the detail, an image scaling device for providing a scaled image size, and a processor.
      <br/>
      The system also includes a video camera which is either within the optical scope or external to the optical scope.
      <br/>
      The optical scope includes a focusing device for adjusting a focal position of the image of the detail and a device for providing a focus position signal based a position of the focusing device.
      <br/>
      If the system includes an external video camera, the optical scope also includes a viewer for passing the image of the detail to a plane outside of the optical scope.
      <br/>
      The video camera optically is coupled with the viewer of the optical scope or with the image of the detail and produces a video signal of the detail from the image of the detail.
      <br/>
      The processor converts the focus position signal into an object distance signal, or a magnification signal, or both, and determines the dimension of the detail based on the scaled image size and based on the object distance signal, or the magnification signal, or both.
    </p>
  </abstract>
  <description format="original" lang="en" id="desc_en">
    <heading>CONTINUATION APPLICATION INFORMATION</heading>
    <p num="1">
      This application is a continuation of U.S. patent application Ser.
      <br/>
      No. 08/502,984, filed Jul. 17, 1995, now U.S. Pat. No. 5,801,762, which is a continuation-in-part of U.S. patent application Ser.
      <br/>
      No. 08/365,636, filed Dec. 28, 1994, now U.S. Pat. No. 5,573,492.
    </p>
    <heading>BACKGROUND INFORMATION</heading>
    <p num="2">
      The present invention concerns an arrangement, which includes a scope, such as a rigid borescope, a flexible fiberscope, or a videoscope for example, for accurately measuring observed objects, object details, and object defects.
      <br/>
      The arrangement of the present invention is more accurate than known systems, yet is simple to use.
    </p>
    <p num="3">
      Scopes, such as flexible videoscopes and fiberscopes have been used to observe the interior of the body during diagnostic procedures or surgery.
      <br/>
      Scopes, such as rigid borescopes, have been used to observe and inspect manufactured parts otherwise inaccessible to the eye.
      <br/>
      Although such scopes have an almost limitless number of applications, the following example illustrates their value.
    </p>
    <p num="4">
      A gas turbine engine includes a series of compressor and turbine blades, any one of which may become damaged.
      <br/>
      Although the first and last stage compressor/turbine blades of a gas turbine engine can be inspected directly, other intermediate stage compressor/turbine blades cannot be directly inspected.
      <br/>
      In the past, to inspect these intermediate stage compressor/turbine blades, the engine had to be disassembled until the intermediate stage compressor/turbine blade could be directly inspected.
      <br/>
      However, more recent gas turbine engines are provided with apertures (or borescope ports) provided at critical areas.
      <br/>
      These borescope ports permit the intermediate stage blades to be inspected using a borescope.
    </p>
    <p num="5">
      The borescope includes a long, thin, insertion tube having a lens system at its distal end and a viewing means at its proximal end.
      <br/>
      When the insertion tube of the borescope is inserted into a borescope port of the gas turbine engine, the lens system at its distal end relays an image of an otherwise inaccessible intermediate compressor/turbine blade to the viewing means at the proximal end.
      <br/>
      The focus of the image (in some models) can be adjusted by control knobs at the proximal end of the borescope.
      <br/>
      Hence, as illustrated by this example, a borescope permits an intermediate compressor/turbine blade of a gas turbine engine to be inspected without needing to disassemble the engine.
    </p>
    <p num="6">
      Besides being used to indirectly inspect parts which cannot be inspected directly, borescopes can also be used to measure the size of defects on the part.
      <br/>
      For example, U.S. Pat. No. 4,980,763 (hereinafter "the '763 patent") discusses a system for measuring objects viewed through a borescope.
      <br/>
      The system discussed in the '763 patent projects an auxiliary image, such as a shadow, onto the object being viewed.
      <br/>
      Changes in the position or size of the auxiliary image correspond to the distance between the object being viewed and the borescope.
      <br/>
      The image is displayed on a monitor having a magnification and object distance scale overlay on the screen.
      <br/>
      The size of the object on the screen is measured with vernier calipers, or electronically with cursors.
      <br/>
      This size is then divided by the magnification which is determined by observing where the auxiliary image falls on the magnification overlay.
      <br/>
      Unfortunately, the system discussed in the '763 patent requires a user to manually determine the magnification factor based on the position of the auxiliary image on the display screen.
    </p>
    <p num="7">
      U.S. Pat. No. 4,207,594 (hereinafter "the '594 patent") discusses a system in which the dimensions of a defect are determined based upon a manually entered field-of-view value and a ratio of second crosshairs, arranged at edges of a defect image, to first crosshairs, arranged at edges of the field of view.
      <br/>
      Unfortunately, the system discussed in the '594 patent requires probe penetration values to be manually read from a scale on the probe barrel for determining the field-of-view.
      <br/>
      Since such scales do not have fine gradations and since they must be manually read, errors are introduced.
    </p>
    <p num="8">
      U.S. Pat. No. 4,820,043 (hereinafter "the '043 patent") discusses a technoscope for determining the length of a defect.
      <br/>
      The technoscope includes a graduated scale which is displaceable in a direction transverse to the endoscope axis.
      <br/>
      The graduated scale is mechanically coupled with a detector which produces an electrical signal based on the transverse displacement of the graduated scale.
      <br/>
      The distance to the defect is determined by (i) observing the object image at a first terminal position of a fixed stroke Z of the endoscope, (ii) noting the intercept of the object image on the graduated scale, (iii) axially displacing the endoscope by the fixed stroke Z, and (iv) transversely displacing the graduated scale until the defect image intercepts it at the same point as before the axial displacement.
      <br/>
      A calculator uses the electrical signal from the detector and a known focal length of endoscope to determine the object distance.
      <br/>
      The size of the defect can be similarly determined.
      <br/>
      The technoscope of the '043 patent also includes a swing prism with a detector for determining its angular position.
    </p>
    <p num="9">
      Unfortunately, the scope of the '043 patent requires that the focal length of the endoscope be known ahead of time and requires two measurements.
      <br/>
      Moreover, since the distance between the two measurements must be fixed, the scope must be fixed with respect to the object during the two measurements.
      <br/>
      Furthermore, limitations in the gradations of the graduated scale limits the accuracy of the readings.
      <br/>
      Also, by manually reading the intercept point of the defect on the graduated scale, errors are introduced.
    </p>
    <p num="10">
      U.S. Pat. No. 4,702,229 (hereinafter "the '229 patent") discusses a technoscope for measuring an object.
      <br/>
      The technoscope includes an inner shaft which is axially displaceable with respect to an outer shaft.
      <br/>
      A measuring scale is provided in the inner shaft.
      <br/>
      The measurement of the object is determined by (i) placing an edge of the object image on the measuring scale, (ii) fixing the technoscope with respect to the object, (iii) axially displacing the inner shaft by a fixed distance, and (iv) observing how many scaler divisions the object image moved on the measurement scale.
      <br/>
      The object size is determined based on a known system focal length, the length of the displacement, and the number of scales moved by the object.
      <br/>
      The '229 patent is similar to the '043 patent except that with the '043 patent, the graduate scale is transversely repositioned such that the object intercepts it at the same point and the transverse position is determined with a mechanical detector.
      <br/>
      Therefore, the device of the '229 patent suffers the same drawbacks as the '043 patent, namely, (i) the focal length of the technoscope must be known, (ii) two measurements are needed, during which the technoscope must be fixed with respect to the object, (iii) limitations in the gradations of the measurement scale introduces errors, and (iv) the measurement scale must be manually read.
    </p>
    <p num="11">
      Known devices also use a magnification scale ring arranged adjacent to a focusing control ring having an indicator RV, for determining the magnification of the scope.
      <br/>
      Based on the position of the indicator of the focusing control ring with respect to the magnification scale, the magnification of the scope at that object distance is determined.
      <br/>
      Unfortunately, similar to the probe penetration knob in the system discussed in the '594 patent, such devices require magnification values to be manually read from a scale on the magnification barrel.
      <br/>
      Since such scales do not have fine gradations and since the magnification values must be manually read, errors are introduced.
      <br/>
      Even if the scale had fine markings, its diameter would have to be huge to have thousands of distinct "markings."
    </p>
    <p num="12">
      When such a scope is equipped with a graticule and a diopter focus control, this known device can also be used to determine the size of a viewed object.
      <br/>
      A graticule is a scale etched into a surface of a transparent glass plate included in the optical system.
      <br/>
      The diopter focus control is used to focus the graticule scale.
      <br/>
      An object is then focused by means of the focus control.
      <br/>
      Based on the number of graticules covered by the object image and based on the magnification level, the object size is determined.
      <br/>
      Unfortunately, the graticule scale is manually read which introduces errors.
      <br/>
      Manually reading the number of graticules covered by the object image also fatigues the user's eye.
      <br/>
      Moreover, since the number of markings on the graticule is limited, the accuracy is also limited.
      <br/>
      Furthermore, this method is inaccurate because the eye will accommodate an "out of focus" focus barrel position.
    </p>
    <p num="13">
      U.S. Pat. No. 4,558,691 (hereinafter "the '691 patent") discloses an endoscope in which an actual size of an observed object, a magnification of the scope, and an object distance can be determined based on a positional relationship between an indicating index and a stationary reference index.
      <br/>
      The indicating index is formed on a glass plate which moves up and down, perpendicular to the optical axis, as the lens barrel of the optical system moves back and forth along the optical axis.
      <br/>
      Unfortunately, as with the devices discussed above, the indicating index included in the device described in the '691 patent does not have fine gradations and must be manually read.
      <br/>
      This not only permits errors to be introduced, but also fatigues the eye of a user.
    </p>
    <p num="14">
      Japanese Patent Publication No. 5-288988 (hereinafter "the '988 publication") discusses the use of an encoder for determining changes in the magnification of a zoom lens system.
      <br/>
      However, this system is to be used for viewing objects at a fixed distance.
      <br/>
      That is, the encoder in the '988 publication determines changes in magnification of the scope but cannot determine the initial magnification of the scope and cannot determine object distance.
    </p>
    <p num="15">In view of the above described problems with existing scope measurement systems, a system for automatically measuring objects, with high resolution, is needed.</p>
    <heading>SUMMARY OF THE INVENTION</heading>
    <p num="16">
      The present invention fulfills the above mentioned need by providing an arrangement including a scope having a focusing mechanism to which a high resolution encoder is coupled.
      <br/>
      The encoder sends a signal, corresponding to the position of the focusing mechanism, to a processor.
      <br/>
      The image from the scope is also sent to the processor.
      <br/>
      A processor executed program correlates the encoder signal to object size and/or magnification.
    </p>
    <p num="17">
      The encoder may be either a relative encoder or an absolute encoder, and may encode optically, electrically, and/or magnetically.
      <br/>
      However, in a preferred embodiment of the system of the present invention, the encoder is a relative, optical, encoder.
    </p>
    <p num="18">
      In a first embodiment of the present invention, the scope is a swing prism rigid borescope which allows the user to change the scope's direction of view from the scope body.
      <br/>
      The encoder is preferably a rotational optical encoder.
      <br/>
      The image from the scope is preferably sent to the video processor via a camera mounted to an eyepiece.
      <br/>
      In an embodiment of the system of the present invention using a relational encoder, the relational encoder increments and decrements an initial count based on adjustments of the focusing device.
      <br/>
      The initial count is set when the focusing device is placed in a predetermined home position.
    </p>
    <p num="19">
      In a second embodiment of the present invention, the scope is a flexible focusing fiberscope having a focusing lens system at a distal end and a focusing control at a proximal end.
      <br/>
      The focusing control at the proximal end actuates at least one lens of the focusing lens system at the distal end by means of a control cable or by means of a fine pitch flexible screw.
      <br/>
      The encoder may be a linear encoder located at the distal end for measuring the linear movement of the at least one lens of the focusing lens system, or it may be an encoder located at the proximal end for measuring a movement of the focusing control.
      <br/>
      In a third embodiment, the scope is a flexible video fiberscope having a focusing lens system at the distal end and a focusing control at the proximal end.
    </p>
    <p num="20">
      Optical encoders may include rotational encoders or linear encoders.
      <br/>
      The rotation encoders include an encoder disk, a light source and a detection device.
      <br/>
      The encoder disk has apertures, or reflective and non-reflective regions, arranged around its circumference, and is mechanically coupled with the focusing device such that it rotates when the focusing device is adjusted.
      <br/>
      The light source directs light towards a first side of the encoder disk.
      <br/>
      When an encoder disk having apertures is used, the detection device is arranged on a second side of the encoder disk, and generates a pulse when light from the light source passes through an aperture of the encoder disk.
      <br/>
      On the other hand, if an encoder disk with reflecting and non-reflecting regions is used, the detection device is arranged on the first side of the encoder disk and generates a pulse when light from the light source is reflected by the encoder disk.
      <br/>
      A linear encoder is similar to the disk encoder except it has a strip having a plurality of spaced apertures or a plurality of reflecting and non-reflecting regions, and is mechanically coupled with the focusing device such that it is linearly translated when the focusing device is adjusted.
    </p>
    <p num="21">
      The present invention provides a system for determining a dimension of a detail.
      <br/>
      The system at least includes an optical scope and a processor, and may also include a video camera, a display device, and a detail marking device.
      <br/>
      The optical scope produces an image of the detail and includes a focusing device and an encoder.
      <br/>
      Borescopes and fiberscopes also include a viewer.
      <br/>
      The focusing device adjusts a focal position of the image of the detail.
      <br/>
      With borescopes and fiberscopes, the viewer passes the image of the detail to a plane outside of the optical scope.
      <br/>
      With videoscopes, a video signal is provided.
      <br/>
      The encoder provides a focus position signal based a position of the focusing device.
    </p>
    <p num="22">
      The video camera produces a video signal of the detail from the image of the detail.
      <br/>
      In borescopes and fiberscopes, the video camera is optically coupled with the viewer of the scope, while in videoscopes, the video camera is internally mounted.
      <br/>
      The display device displays the detail based on the video signal of the detail.
      <br/>
      The detail marking device permits at least two markers, each having a coordinate value, to be arranged on the display of the detail on the display device.
      <br/>
      The processor converts the focus position signal from the encoder into an object distance signal and then into a magnification signal.
      <br/>
      The processor also determines the dimension of the detail based on the coordinate values of the at least two markers and based on the magnification signal.
    </p>
    <p num="23">
      In a preferred embodiment of the system of the present invention, the optical scope is a swing prism rigid borescope.
      <br/>
      The optical scope may also be a fiberscope or videoscope.
    </p>
    <p num="24">
      In a preferred embodiment of the system of the present invention, the video camera includes a charge coupled device which converts the image of the detail into the video signal of the detail.
      <br/>
      Further, in the preferred embodiment of the system of the present invention, the processor includes a first converter, such as a formula or a look-up table, for converting the focus position signal into an object distance signal, and a second converter, such as a formula or look-up table, for converting the object distance signal from the first converter into a magnification signal.
      <br/>
      In the preferred embodiment, the processor also includes a size processor for producing the dimension of the detail based on the magnification signal from the second converter and based on the coordinate values of the at least two markers.
    </p>
    <p num="25">
      In a preferred embodiment of the present invention, the optical scope includes a distal lens system, a focusing lens, a focus controller, and an encoder.
      <br/>
      If the optical scope is a borescope or fiberscope, it also includes a viewer.
      <br/>
      The distal lens system produces an image of a detail within its field-of-view.
      <br/>
      If the optical scope is a borescope or fiberscope, the viewer passes an image of the detail produced by the lens system to a plane outside of the scope.
      <br/>
      The focusing lens is located between the distal lens system and the viewer and can be linearly translated along its optical axis thereby permitting a focal position of the image to be adjusted.
      <br/>
      The focus controller linearly translates the focusing lens along its optical axis, whereby different positions of the focus controller correspond to different positions of the focusing lens.
      <br/>
      If the optical scope is a videoscope, a video camera, such as a CCD for example, converts an image to a video signal at the distal end.
      <br/>
      The focusing control actuates at least one lens in a lens system at the distal end.
      <br/>
      The encoder produces signals corresponding to the different positions of the focusing lens being translated or of the focus controller.
    </p>
    <p num="26">
      In an alternative embodiment of the present invention, the scope automatically focuses the image onto the video camera.
      <br/>
      In this embodiment, a stepper motor can actuate at least one lens of the focusing lens system.
      <br/>
      Alternatively, a user may make processor guided manual focusing adjustments.
      <br/>
      The user positions a cursor upon the video image of the detail to be measured.
      <br/>
      Alternatively, known methods of pattern detection could be used.
      <br/>
      A number of samples of windows (i.e., a predetermined number of pixels around the area of interest) are taken with the at least one lens of the focusing lens system at different positions.
      <br/>
      The at least one lens is translated by the stepper motor.
      <br/>
      The sampled window with the maximum contrast, as determined by a known method, is considered to be the most in focus.
      <br/>
      If there is more than one maximum contrast, the processor can choose either (i) the near focus side position having a maximum contrast, (ii) the far focus side position having a maximum contrast, or (iii) the average focus position of the maximum contrasts.
      <br/>
      This choice is predetermined.
      <br/>
      While any of the three choices can be used, it must be used consistently and can be the basis for system calibration.
    </p>
    <p num="27">
      In a preferred embodiment of the present invention, an optical distortion, based on an eccentricity of the image with respect to the focusing lens system, is corrected by the video processor.
      <br/>
      The eccentricity is determined based on the location of the image on the video camera and/or on the video monitor.
    </p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading>
    <p num="28">
      FIG. 1 is a side view illustration of a rigid borescope.
      <br/>
      FIG. 2 is an illustration of a marked focus barrel used in known measuring scopes.
      <br/>
      FIG. 3 is a cross-sectional side view of an ocular position rotational encoder to be used with the system of the present invention.
      <br/>
      FIG. 4 is a plan view of the ocular position rotational encoder shown in FIG. 3.
      <br/>
      FIG. 5 is a perspective view illustrating a focus barrel having a spiral groove.
      <br/>
      FIG. 6 is a perspective view illustrating a borescope chassis having a longitudinal slot.
      <br/>
      FIGS. 7a and 7b are cross-sectional side views of the ocular rotational encoder of FIG. 3 illustrating the difference in the positions of the elements of the encoder when viewing relatively distant objects and relatively close objects.
      <br/>
      FIG. 8 is a schematic diagram of an emitter end plate, a code wheel, and an encoder body of a rotational encoder that can be used in the system of the present invention.
      <br/>
      FIGS. 9a through 9d are timing diagrams of outputs of the rotational encoder illustrated in FIG. 8.
      <br/>
      FIG. 10 is a functional block diagram of the system of the present invention.
      <br/>
      FIG. 11 is a graph illustrating the relationship between magnification and object distance for an exemplary borescope.
      <br/>
      FIG. 12 is a look-up table for determining object distance from the count produced by an exemplary encoder.
      <br/>
      FIG. 13 is a flow diagram of a process for using the system of the present invention.
      <br/>
      FIG. 14a is a schematic illustrating an objective fiberscope having a distally mounted encoder and having a proximal focusing cable to mechanically link a focus control with a focusing lens system.
      <br/>
      FIG. 14b is a schematic illustrating an objective fiberscope having a distally mounted encoder and having a flexible fine pitch screw and a fine pitch nut for mechanically linking a proximal focus control with a focusing lens system.
      <br/>
      FIG. 14c is an alternative embodiment of FIGS. 14a and 14b in which a proximally mounted encoder is used instead of a distally mounted encoder.
      <br/>
      FIG. 15a is a schematic illustrating an objective videoscope having a distally mounted encoder and having a focusing cable to mechanically link a focus control with a proximal focusing lens system.
      <br/>
      FIG. 15b is a schematic illustrating an objective videoscope having a distally mounted encoder and having a flexible fine pitch screw and a fine pitch nut for mechanically linking a proximal focus control with a focusing lens system.
      <br/>
      FIG. 15c is an alternative embodiment of FIGS. 15a and 15b in which a proximally mounted encoder is used instead of a distally mounted encoder.
      <br/>
      FIGS. 16a and 16b are video display screens for illustrating a process for determining magnification and radial distortion errors.
      <br/>
      FIGS. 17 is a diagram which illustrates how magnifications are determined.
      <br/>
      FIG. 18 is a flowchart of an automatic focusing procedure of the present invention.
      <br/>
      FIG. 19 is a schematic diagram illustrating additional elements used in an automatically focusing scope.
    </p>
    <heading>DETAILED DESCRIPTION</heading>
    <p num="29">
      The following is a description of an exemplary embodiment of the present invention which employs a rotational encoder with a borescope.
      <br/>
      This example is not intended to limit the scope of the invention to rigid borescopes or to rotational encoders.
      <br/>
      Instead, the scope of the invention is defined by the claims which follow the detailed description.
    </p>
    <p num="30">
      FIG. 1 is a side view illustration of a rigid borescope 1.
      <br/>
      The borescope 1 includes a body 2 and a long, thin arm (or insertion tube) 3.
      <br/>
      The insertion tube 3 is connected, at a proximal end, to the body 2, has an insertion length WL, and includes a lens system 10 at its distal end.
      <br/>
      The lens system 10 has a field-of-view defined by the angle FOV.
      <br/>
      As is illustrated in the partial side views of the insertion tube 3, different lens systems 10a through 10d have different directions of view.
      <br/>
      The distal end of the insertion tube 3 includes a cap 9 for protecting the insertion tube 3 from mechanical shocks resulting from inadvertent collisions.
      <br/>
      In a preferred embodiment of the present invention, the rigid borescope includes a swing prism, i.e., a prism which can be pivoted.
      <br/>
      The direction of view of such a swing prism can be adjusted from 45 (degree)  to 120 (degree) . A sensor may be used to determine the angular position of the prism.
      <br/>
      Any angular prism position deviating from 90 (degree)  introduces optical errors.
      <br/>
      These optical errors can be compensated for based on the sensor output.
    </p>
    <p num="31">
      The body 2 of the borescope 1 includes an orbital scan control 8, a focus control 4, a viewing means 11, and a light guide connector 6.
      <br/>
      The orbital scan control 8 is used for rotating the insertion tube 3 for capturing different views.
      <br/>
      The angular orientation of the insertion tube 3 is indicated with an orbital scan direction indicator 7.
      <br/>
      The focus control 4 permits the image gathered by the lens system 10 to be focused at the viewing means 11.
      <br/>
      The light guide connection 6 permits a light source (not shown) to be coupled with the borescope.
      <br/>
      The light from the light source may be carried by a fiber optic bundle, for example, to the distal end of the insertion tube 3 to illuminate objects within the field-of-view of the lens system 10.
      <br/>
      The viewing means 11 provides the image captured by the lens system 10.
      <br/>
      An eyecup 5 may be connected to the viewing means 11 for direct viewing.
      <br/>
      Alternatively, as illustrated in FIG. 10, a video camera 103 may be mounted to the viewing means by means of a viewing means-to-camera adaptor 116.
      <br/>
      The video camera 103 may transmit a video signal to a video processor 108.
    </p>
    <p num="32">
      A "focusing borescope " is a borescope with adjustable focus which produces a sharp image at a range of object distances.
      <br/>
      The narrow field-of-view (e.g. FOV=20 degrees) results in a shallow depth-of-field (DOF).
      <br/>
      As a consequence, with focusing borescopes, positions of the focus barrel correspond to object distances.
      <br/>
      Therefore, the focus barrel can be calibrated and used as a range finder.
      <br/>
      Since the log of the scope magnification and the log of the object distance have a linear relationship, the actual size of an observed object can also be determined.
    </p>
    <p num="33">
      FIG. 2 is a marked focus barrel used in known measuring scopes, such as a focusing borescope for example.
      <br/>
      Such a marked focus barrel can be provided on the body 2 of the borescope 1 of FIG. 1 for example.
      <br/>
      The focus barrel of FIG. 2 includes a magnification scale 22, a diopter focus control 23, a graticule orientation control 24 and a focusing control 21 having an indicator 211.
      <br/>
      When the observed image is focused, the position of the indicator 211 of the focusing control 21 with respect to the magnification scale is used to determine the magnification of the image.
      <br/>
      As mentioned above, the log of the object distance is linearly related to the log of the scope magnification.
      <br/>
      Therefore, once the magnification of the image is determined, the object distance can be derived.
      <br/>
      Furthermore, as discussed in the Background of the Invention above, when the graticule (i.e., a scale etched into a surface of a coverglass included in the optic system) is focused with the diopter focus control 23, the size of the object being viewed can be determined based on the number of graticules occupied by the image of the object and based on the magnification of the scope.
    </p>
    <p num="34">
      Unfortunately, a number of error sources are introduced when using a scope with a marked focus barrel to determine the object distance, i.e., as a range finder.
      <br/>
      First, the resolution of the magnification value reading is limited by the gradations of the magnification scale.
      <br/>
      Also, there is a very practical limit to the number of graticule markings which limits the accuracy.
      <br/>
      Second, the magnification scale 22 and the number of graticules covered by the focused image must be manually read by a user.
      <br/>
      Third, manual calculations and confusing multiple step procedures introduce the possibility of human errors.
      <br/>
      Furthermore, the poor "ease of use " of scope measuring systems employing marked focus barrels and the experience of eye fatigue when using such scopes has limited their acceptability.
      <br/>
      More importantly however, is that the eye will accommodate slightly "out of focus" images.
      <br/>
      Furthermore, differences in the eyesights of different users will lead to different magnifications needed to focus the image.
      <br/>
      That is, users having different eyesights will result it different focus positions for the object being viewed.
    </p>
    <p num="35">
      FIG. 3 is a cross-sectional side view, and FIG. 4 is a partial plan view, of an ocular position rotational encoder which can be used in the scope measurement system of the present invention.
      <br/>
      The ocular rotational encoder includes an encoder pickup 30 (described in more detail with reference to FIG. 8), an encoder mounting plate 31, a borescope chassis 32, an encoder disk 33, an encoder disk hub 34, a carrier with an ocular lens 35, a ocular position pin 36, a viewing means 37, a focusing knob 38, and a focusing barrel 39.
    </p>
    <p num="36">
      The ocular position rotational encoder can be mounted to a rigid borescope by means of the mounting plate 31.
      <br/>
      The viewing means 37 is an eyepiece.
      <br/>
      However, as illustrated in FIG. 10, in the preferred embodiment of the present invention, a video camera 103 is mounted to the viewing means 37 with a camera adaptor 116.
      <br/>
      The video camera 103 transmits a video signal to a video processor 108 which displays the captured image 110 on a display monitor 109.
      <br/>
      This arrangement permits the image captured to be more accurately focused than would be possible with a human eye at the viewing means 37 because the video camera 103 includes an imaging means, such as a charge coupled device (CCD) 104, which is maintained at a fixed distance from the viewing means 37.
      <br/>
      Accordingly, the video camera 103 provides a "hard plane of focus" which is advantageous when compared with the different focus positions resulting from different users having different eyesights as discussed above and resulting from the eye's ability to accommodate slightly "out-of-focus" images.
    </p>
    <p num="37">
      When the focusing knob 38 is rotated about the optical axis, the ocular lens carrier 35 moves up or down as indicated by the arrows.
      <br/>
      By viewing the image on the display monitor, the focusing knob 38 can be used to precisely focus the image transmitted through the ocular lens held in the carrier 35.
      <br/>
      This is done as follows.
    </p>
    <p num="38">
      The focusing knob 38 is mechanically coupled with the focusing barrel 39.
      <br/>
      In a preferred embodiment of the present invention, the focusing knob 38 is directly attached to the focusing barrel 39.
      <br/>
      As shown in FIG. 5, the focusing barrel 39 is cylindrical and has a spiral groove 51 cut into its inner surface.
      <br/>
      The ocular position pin 36 fits into the spiral groove 51.
      <br/>
      The inner surface of the focusing barrel 39 has a slightly larger diameter that the outer surface of the borescope chassis 32 thereby permitting the focusing barrel 39 to rotate about the borescope chassis 32.
      <br/>
      As shown in FIG. 6, the borescope chassis 32 has a longitudinal slot 61 through which the ocular position pin 36 projects.
      <br/>
      Accordingly, when the focusing knob 38 is rotated, the directly connected focusing barrel 39 also rotates about the borescope chassis 32.
      <br/>
      The spiral groove 51 on the inside surface of the focusing barrel 39 causes the ocular positioning pin 36 to ride up or down in the longitudinal slot 61 of the borescope chassis 32.
      <br/>
      Since the ocular positioning pin 36 is attached to the ocular lens carrier 35, the ocular lens can be moved up and down by rotating the focusing knob 38.
    </p>
    <p num="39">
      FIGS. 7a and 7b illustrate the relative positions of the ocular lens carrier 35 and of the ocular positioning pin 36 within the groove 51 on the inside surface of the focusing barrel 39, for an object relatively far from the borescope and for an object relatively close to the borescope, respectively.
      <br/>
      As illustrated in FIG. 7a, when the object distance is relatively large, the ocular lens carrier 35 is positioned far from the viewing means 37 and the ocular positioning pin 36 is located high in the spiral groove 51 on the inside surface of the focusing barrel 39.
      <br/>
      On the other hand, as illustrated in FIG. 7b, when the object distance is relatively small, the ocular lens carrier 35 is positioned close to the viewing means 37 and the ocular positioning pin 36 is located low in the spiral groove 51 on the inside surface of the focusing barrel 39.
    </p>
    <p num="40">
      As illustrated in FIG. 3, the focusing barrel 39 is also mechanically coupled with and preferably directly connected to, the disk hub 34 of the encoder disk 33.
      <br/>
      In a preferred embodiment of the present invention, as shown in FIG. 4, the encoder disk 33 has a number of slots or holes 40 (only three of which are shown for clarity) arranged around the encoder disk 33, spaced in equal angular increments.
      <br/>
      The encoder pick up 30, described in detail with reference to FIG. 8, determines the angular rotation of the encoder disk 33 by counting pulses received at the pickup 30.
    </p>
    <p num="41">
      The optical encoder which produces pulses corresponding to changes in the focal position, is a so-called "relational encoder".
      <br/>
      That is, an initial condition (i.e., an initial count) must be determined before the count is incremented or decremented.
      <br/>
      This initial condition is determined by locating the focal position to a predetermined "home" position for which the count is known or reset by the electronics, typically to zero.
      <br/>
      The known or reset count is then incremented and/or decremented when the focal position is moved from the "home" position.
      <br/>
      The "home" focal position is preferably located at at least one of the two extreme focal positions.
      <br/>
      Alternatively, a home position can be determined with an "indexing channel" as described below.
    </p>
    <p num="42">As an alternative to such "relational encoders," which increment and/or decrement a known count when the focal position is moved from a predetermined position, an "absolute encoder," which includes information about its absolute angular (or linear) position may be used.</p>
    <p num="43">
      FIG. 8 is a schematic diagram of the encoder optical pick-up 30.
      <br/>
      The encoder optical pick-up 30 includes an emitter end plate 81 arranged adjacent to a first surface of the encoder disk 33 and an encoder body 82 arranged adjacent to a second surface of the encoder disk 33.
    </p>
    <p num="44">
      The encoder end plate 81 includes a series of light emitting diodes 83a through 83c for emitting light which is collimated by lenses 84a through 84c, respectively.
      <br/>
      These collimated light beams are directed towards the first surface of the encoder disk 33.
    </p>
    <p num="45">
      The encoder body 82 includes a phase plate 85, lens pairs 86a through 86c, and detection elements 87a through 87c for three (or two in an alternative embodiment) channels.
      <br/>
      Each of the channels 87a through 87c includes an integrated circuit having two photodiodes each having its own amplifier 88a through 88c.
      <br/>
      The amplifiers 88a1 through 88c1 are electrically coupled with a non-inverting input of comparators 89a through 89c, respectively, while the amplifiers 88a2 through 88c2 are electrically coupled with an inverting input of the comparators 89a through 89c, respectively.
    </p>
    <p num="46">
      The collimated light beams from lenses 84a through 84c must pass through a slit 40 in the encoder disk 33 and through an opening in the phase plate 85 to reach lens pairs 86a through 86c, respectively.
      <br/>
      The apertures in the phase plate 85 are positioned such that, for each photo-diode/amplifier pair 88a through 88c, a light period on one detector always corresponds to a dark period on the other.
      <br/>
      Accordingly, the output state of the comparators 89a through 89c changes when the difference of the two photo currents produced by the photo-diode/amplifier pairs 88a through 88c, respectively, changes sign.
    </p>
    <p num="47">The phase plate 85 is also arranged so that the channel 87a is 90 degrees out of phase (i.e., in quadrature) with the channel 87b as is shown in the timing diagram of FIGS. 9a and 9b. This phase difference permits the direction of rotation to be determined by observing which channel is the leading waveform.</p>
    <p num="48">
      The channel 87c is an optional channel for performing an indexing function.
      <br/>
      Specifically, the channel 87c generates an index pulse for each rotation of the encoder wheel 33.
      <br/>
      This indexing channel can be used to define a "home" focal position having a known count, thereby providing an initial condition, i.e., an initial count, for this "relational" encoder.
    </p>
    <p num="49">
      FIG. 10 is a functional block diagram of the system of the present invention.
      <br/>
      An image captured by the borescope 1 is provided to a video camera 103 which is coupled with the viewing means 37 of the borescope 1 via a viewing means-to-camera adaptor 116.
      <br/>
      The video camera 103 includes an imaging device 104, such as a charge-coupled device (CCD) for example, for converting the optical signal to an analog video signal.
      <br/>
      The analog video signal is supplied to a video processor 108.
      <br/>
      The video processor 108 provides a video signal of the object to a video display monitor 109 via a digital video frame capturing device 115.
      <br/>
      The focusing knob 38 is adjusted until an image of the object 110 being observed appears in focus on the video display monitor 109.
      <br/>
      For objects oriented at an angle to the plane of the optical system of the scope, multiple points or regions of the object can be separately focused.
    </p>
    <p num="50">
      While the focusing knob 38 is being adjusted, the encoder 30 generates pulses corresponding to the angular rotation of the focusing knob 38.
      <br/>
      The encoder 30 transmits the pulses of channels 87a, 87b, and 87c to a counter 101.
      <br/>
      The counter 101 increments or decrements the count, depending upon whether the focusing knob 38 is being rotated clockwise or counter-clockwise, thereby forming a digital count value.
      <br/>
      An initial count is generated when the focus position is at "home" position as discussed above.
      <br/>
      The digital count value is stored in a buffer 102.
    </p>
    <p num="51">
      When the object being observed 110 appears in focus on the video display monitor 109, the user actuates a switch 112, such as a key of a keypad, to "freeze" the image.
      <br/>
      When the switch 112 is actuated, the digital count stored in the buffer 102 is read out and provided as an input to a count-to-object distance converter 106, and the video frame stored in the digital video frame capture 115 is read out and provided to the video display monitor 109.
      <br/>
      If multiple points or regions of an angled object are separately focused as provided above, the counts corresponding to the separate focus positions are averaged.
      <br/>
      In a further embodiment of the present invention, described more fully below, multiple points of a feature, such as a ding, can be separately focused and the measured point distances used to determine the depth (or height) of the feature or of parts of the feature.
    </p>
    <p num="52">
      The count-to-object distance converter 106 can be implemented as a predetermined formula for converting the digital count to an object distance.
      <br/>
      For example:
      <br/>
      log x=a(log y)2 -b (log y)+c
    </p>
    <p num="53">wherein</p>
    <p num="54">
      x=object distance
      <br/>
      y=encoder count
      <br/>
      a,b,c=constants
    </p>
    <p num="55">
      Alternatively, the object distance can be determined from the count by means of a look up table including empirically determined data.
      <br/>
      FIG. 12 shows an example of such a look up table.
      <br/>
      If a look up table is used for converting the digital count to an object distance, an interpolation routine is preferably also used for determining object distances when the digital count falls between count values listed in the look up table.
    </p>
    <p num="56">
      The object distance determined by the count-to-object distance converter 106 is provided as an input to an object distance-to-magnification converter 107.
      <br/>
      Similar to the count-to-object distance converter 106, the object distance-to-magnification converter 107 can be implemented as a predetermined formula for converting the object distance to a magnification value.
      <br/>
      Alternatively, the magnification value can be determined from the object distance by means of a look up table and an optional interpolator.
      <br/>
      As the graph shown in FIG. 11 illustrates, a linear relationship exists between the log of the magnification and the log of the object distance.
    </p>
    <p num="57">
      The system of the present invention also can automatically compensate for changes in the magnification from system to system due to differences in the video cameras (such as CCDs for example) and due to differences in the coupling between the eyepiece and the video camera.
      <br/>
      This automatic compensation eliminates the need to calibrate the magnification for each scope or system.
      <br/>
      The system of the present invention can also compensate for optical distortions due to image eccentricity.
      <br/>
      All optical systems distort.
      <br/>
      The main component of optical distortion for scopes results from images that do not pass through the center point of the lenses of the lens system.
      <br/>
      Specifically, an optical distortion is a function of the radial distance from the center of the lens to the point at which the image passes, i.e., the optical distortion is greater when the image passes through the edges of the lens than when the image passes through the center of the lens.
    </p>
    <p num="58">
      The system of the present invention automatically compensates for variations in magnification and for optical distortion as follows.
      <br/>
      FIG. 16a illustrates a screen of the display 109 showing the image of the defect 110.
      <br/>
      Since the actual image does not fill on the entire area of an imaging device (such as a CCD 104 for example) of the video camera 103, the scope image 161 does not fill the entire screen of the display 109.
      <br/>
      The size of the scope image 161 depends, at least in part, on the optical coupling of the image to the imaging device.
      <br/>
      A user can move and adjust the diameter of the circle 162 with an input device, such as the cursor input control 113, for example.
      <br/>
      The user moves and adjusts the circle 162 such that it coincides with the scope image 161 on the display. (See FIG. 16b).
      <br/>
      A processor, such as the size processor 114, compensates the magnification of the optical scope 1--video camera 103 combination based on the diameter of the circle 162.
      <br/>
      FIG. 17 is a diagram which illustrates the magnification compensation.
      <br/>
      Once the object distance (OD) is determined, the actual size of the diameter of the optical scope (DOS) can be determined since the field of view (FOV=20) is known.
      <br/>
      Specifically DOS =2X=2 �OD(tan THETA )�. The magnification is then determined based on the ratio of the diameter of the circle 162 (when it coincides with the scope image 161) over DOS.
    </p>
    <p num="59">
      FIG. 16a also illustrates a crosshair 164 which can be moved by a user with an input device, such as the cursor input control 113.
      <br/>
      As shown in FIG. 16b, the user can position the crosshair 164 at the center of the scope image 161 so that an eccentricity "E" from the center 165 of the display can be determined.
      <br/>
      A processor, such as the size processor 114 for example, can compensate for optical distortions based on the eccentricity "E".
    </p>
    <p num="60">
      The magnification compensation and the optical distortion compensation may be separately provided.
      <br/>
      However, if both are provided, the present invention preferably combines the crosshair 164 to coincide with the center of the circle 162 so that the image size and eccentricity can be determined based on a single user input.
      <br/>
      Alternatively, a processor executed program can determine the size of the scope image 161 and the eccentricity "E" automatically without requiring a user input.
      <br/>
      The optical scope may also include a non-volatile memory for communicating stored calibration information with other components of the system.
    </p>
    <p num="61">
      Referring back to FIG. 10, after a user actuates the freeze image switch 112, the user manipulates a cursor control input device 113, such as a keypad, a trackball, or a joystick, for example, to position at least two crosshairs 111a and 111b at ends of the object 110 being displayed.
      <br/>
      The coordinate positions of the crosshairs 111a and 111b, as well as the magnification factor, are provided as inputs to a size processor 114 which computes the length or size of the object 110 being displayed.
      <br/>
      Alternatively, a graticule (or reticle) provided in the optical scope can be used for providing scaled image information to the user.
      <br/>
      This scaled image information can be manually input into the size processor 114.
      <br/>
      Alternatively, a micro adjustable reticle can be used for marking and can be coupled with an encoder for providing direct inputs to the processor.
    </p>
    <p num="62">
      The display 109 can also optionally display the current object distance.
      <br/>
      As will be described more fully below, displaying the current, in-focus, object distance is useful for making depth and height measurements.
    </p>
    <p num="63">
      It should be evident from the above description that various combinations of the count-to-object distance converter 106, object distance-to-magnification converter 107 and size processor 114 are readily possible.
      <br/>
      For instance, the formulas or look up tables used to implement the count-to-object distance converter 106 and the object distance-to-magnification converter 107 can be implemented with one formula or look up table which uses the focus count from the buffer 102 as an input and provides the magnification value as an output.
      <br/>
      Similarly, the object distance-to-magnification converter 107 can be combined with the size processor 114 so that the object distance value from the count-to-object distance converter 106 can be used directly by the combined block (107/114) in determining the size of the object 110.
    </p>
    <p num="64">
      Operating the system of the present invention is almost fool-proof.
      <br/>
      Moreover, the system of the present invention reduces eye fatigue.
      <br/>
      As illustrated in the flow diagram of FIG. 13, the user is only required to execute three simple steps.
      <br/>
      First, the user must adjust the focus until the image on the display screen is focused as shown in step 131.
      <br/>
      Next, as shown in step 132, the user freezes the focused image.
      <br/>
      Finally, the user positions first and second crosshairs on the object being displayed as shown in step 133.
      <br/>
      Moreover, in the automatically focusing system described below, the user only has to perform step 133.
      <br/>
      Accordingly, a user is only required to execute three (or one) simple steps.
      <br/>
      This eliminates many errors that could otherwise be introduced by the user.
    </p>
    <p num="65">
      The above is an exemplary embodiment of the system of the present invention.
      <br/>
      One skilled in the art can modify the particular components suggested without departing from the scope of the invention recited in the claims.
      <br/>
      For example, a linear optical encoder for providing the position of the ocular positioning pin 36 can be used in place of the encoder disk 33 and its optical pickup 30.
      <br/>
      Such a linear encoder could be a plastic or metal strip, having equally spaced slits on it, and being mechanically coupled with the ocular positioning pin 36.
      <br/>
      An optical sensor can be used to count light and dark regions as the plastic or metal strip is moved with respect to it.
      <br/>
      Instead of slots, reflective and non-reflective regions can be used.
    </p>
    <p num="66">
      Furthermore, an electrical sensor, such as a rheostat, for example, or a magnetic sensor, such as a hall sensor for example, can be used to determine the position of the ocular lens.
      <br/>
      However, optical encoders are preferred because they provide high resolution in a relatively small package.
    </p>
    <p num="67">
      Similarly, the focusing barrel 39 can be mechanically coupled to the encoder disk hub 34 by gears instead of being directly connected.
      <br/>
      Also, instead of providing an encoder disk 33 with slits 40, a reflective encoder disk with pits can be used.
      <br/>
      Similarly, instead of incrementing and/or decrementing a count of slits (a relational encoder), the encoder disk can include coded angular position information (an absolute encoder).
    </p>
    <p num="68">
      A fiberscope or videoscope may also be used instead of a rigid borescope.
      <br/>
      As shown in FIG. 14a, the fiberscope 140 includes a flexible insertion tube 141 having a focusing lens system 142 at its distal end.
      <br/>
      A coherent fiber bundle 143 carries the image to the proximal end of the insertion tube where a lens system (not shown) provides an image to a viewer.
      <br/>
      Alternatively, the focusing lens system 142 can be provided at the proximal end of the coherent fiber bundle 143.
      <br/>
      The proximal end of the fiberscope 140 includes a scope body 144 with a focus controller 145.
      <br/>
      The focus controller 145 can linearly translate a lens of the focusing lens system 142 by means of a control cable 146 as shown in FIG. 14a or by means of a fine pitch flexible screw 147 and nut 148 as shown in FIG. 14b. In the alternative embodiment having the focusing lens system 142 at the proximal end of the coherent fiber bundle 143, the focus controller 145 can be more directly coupled with the at least one lens.
      <br/>
      As shown in FIG. 14c, the encoder 149 may be located at the proximal end of the fiberscope 140 to encode the movement of the focus controller 145.
      <br/>
      However, as shown in FIGS. 14a and 14b, the encoder is preferably a linear encoder located at the distal end of the fiberscope 140 to encode the linear movement of the lens.
      <br/>
      Providing the encoder at the distal end eliminates mechanical position errors due to flexing or stretching in the focusing control cable 146 or backlash in the flexible screw 147.
    </p>
    <p num="69">
      As shown in FIGS. 15a and 15b, a videoscope 150 has a video camera 151, such as a CCD, adjacent to a focusing lens system 152 at the distal end of a flexible insertion tube 153.
      <br/>
      The video camera 151 converts the image to a video signal which is carried to the proximal end of the videoscope by means of a video signal cable 154.
      <br/>
      Accordingly, a viewer is not required in a videoscope.
    </p>
    <p num="70">
      As discussed above with reference to the fiberscope 140, a focus controller 155, located at the proximal end of the videoscope 150, can linearly translate a lens of the focusing lens system 152 by means of a focusing control cable 156 (See FIG. 15a.) or by means of a flexible screw 157 and nut 158 (See FIG. 15b.).
      <br/>
      Also, as discussed above with reference to the fiberscope 140, the encoder 159 may be located at the proximal end of the videoscope to encode the movement of the focus controller 155 (See FIG. 15c.) but is preferably a linear encoder 159 located at the distal end of the videoscope for encoding the linear movement of the lens (See FIGS. 15a and 15b.).
      <br/>
      In both the videoscope and fiberscope, if a mechanical means is used to couple the focusing control with the at least one lens, mechanical play in the mechanical means can be compensated for by a processor.
    </p>
    <p num="71">
      If a fiberscope 140 is used, the system is similar to the system of FIG. 10 which uses a borescope as the optical scope 1.
      <br/>
      However, if a videoscope 150 is used, the viewing means-to-camera adaptor 116, the externally mounted video camera 103 with CCD 104 are not needed since the videoscope 150 includes an internal video camera such as a CCD 151 for example.
    </p>
    <p num="72">In alternative embodiments of the scopes and systems of the present invention, an automatic focusing device can be used in addition to, or in place of, the focusing knob 38 of the borescope or the focusing controls 145 and 155 of the fiberscope 140 and videoscope 150, respectively.</p>
    <p num="73">
      As shown in FIGS. 19a and 19b, a stepper motor 191 is mechanically coupled with at least one lens of a focusing lens system 192, by the means of a focus control cable or a fine pitch flexible screw for example.
      <br/>
      An optional encoder 193 may also be mechanically coupled with the stepper motor 191 (See FIG. 19a.) or with the at least one lens of the focusing lens system 192 (See FIG. 19b.).
    </p>
    <p num="74">
      The object image is automatically focused with a processor 194 executed program.
      <br/>
      FIG. 18 is a flow chart illustrating the program for automatic focus.
      <br/>
      At step 181, the user selects a point of interest via an input device, such as the cursor input control 113 for example.
      <br/>
      Alternatively, the processor 194 can select a point of interest using a known defect detection algorithm.
      <br/>
      Next, a window is defined around the point of interest at step 182.
      <br/>
      The window may be predetermined or may be defined by the user.
      <br/>
      The window is preferably a rectangle but may be another geometric shape.
      <br/>
      For example, the processor 194 can define a 10 pixel by 10 pixel box centered around the point of interest.
      <br/>
      In a preferred embodiment, the size of the window is limited decrease processing time.
    </p>
    <p num="75">
      In steps 183 and 184, the stepper motor is placed in an initial position and the image within the window is sampled.
      <br/>
      In step 185, the contrast of the sampled image is determined in a known way.
      <br/>
      For example, the average of the magnitude of the differences in brightness between adjacent pixels can be determined.
      <br/>
      The higher the average, the higher the image contrast.
      <br/>
      Alternatively, a Fast Fourier Transform (FFT) of the image can be determined.
      <br/>
      The higher frequency, the more complex the image and the higher the contrast.
    </p>
    <p num="76">
      Step 186 determines whether the contrast determined in Step 185 is a maximum.
      <br/>
      Since the maximum contrast value corresponds to the "best focus," a focus position value is related to the current stepper motor position in step 187 when the contrast is a maximum.
    </p>
    <p num="77">
      Steps 184 through 187 are repeated until a range of stepper motor positions is complete, as illustrated by steps 188 and 189.
      <br/>
      If a maximum contrast is determined at more than one stepper motor position, the best focus is selected from either (i) a maximum contrast position nearest to the focus size, (ii) a maximum contrast position farthest from the focus side, or (iii) an average of the maximum contrast positions.
      <br/>
      In an embodiment having automatic focusing and a defect detection process, a display is not required.
    </p>
    <p num="78">
      As can be inferred from FIG. 19a, it is possible to eliminate the encoder 193 and base the focus position on the number of steps executed by the stepper motor 191.
      <br/>
      This would be especially practical if a small stepper motor could be arranged at the distal end of the insertion tube.
      <br/>
      If however, the stepper motor 191 is located at the proximal end of the scope as in FIG. 19b, an encoder 193 is preferably included at the distal end of the insertion tube to eliminate any errors due to mechanical "play" in the mechanical coupling between the stepper motor 191 and the at least one lens of the focusing lens system 192.
      <br/>
      Alternatively, a user may make processor guided manual adjustments.
    </p>
    <p num="79">
      In a preferred embodiment of the present invention, a focus knob (or focusing controller) is used with the stepper motor in a "hybrid" focusing operation.
      <br/>
      A user first uses the focusing knob to coarsely focus the image.
      <br/>
      The stepper motor then performs a fine image focus under control of the processor as described with respect to FIG. 18. Such a "hybrid" operation is advantageous because the range of stepper positions at which the image is to be sampled and analyzed is decreased.
    </p>
    <p num="80">
      The system of the present invention can also be used to measure depth and height by determining the difference between the measured object distances of two points.
      <br/>
      For example, the depth of a round-bottomed ding in the surface of a turbine blade can be determined by 1) first focusing the device of the present invention on the surface of the blade and determining the object distance, 2) recording the measured object distance to the blade surface, 3) focusing on the bottom of the ding and determining the object distance, and 4) subtracting the recorded object distance to the blade surface from the object distance to the ding bottom, thereby yielding the depth of the ding.
    </p>
    <p num="81">
      The above-described procedure can be carried out manually using any of the embodiments of the present invention described thus far.
      <br/>
      For example, with the embodiment of FIG. 10, the user can note the object distances displayed on the display 109 for the two points of interest (i.e., the blade surface and the ding bottom) and subtract the two quantities to determine the depth of the ding.
    </p>
    <p num="82">
      The system of the present invention can provide various features to facilitate the depth/height measurement procedure.
      <br/>
      The embodiment of FIG. 10, for example, can be adapted to provide such features by modifying the software used to program the system.
      <br/>
      For example, once the user causes the system to focus on the first point, e.g., the blade surface, and has hit image freeze switch 112, the object distance, as determined by the count-to-object distance converter 106, is temporarily stored by the software in a register or memory location (not shown).
      <br/>
      When the user causes the system to focus on the second point, e.g., the ding bottom, and hits the image freeze switch 112, the system will subtract the stored object distance from the current object distance, as generated by the converter 106, and display the difference on the display 109.
    </p>
    <p num="83">
      Optionally, once the first point has been focused and measured, as the system is being re-focused and before the freeze switch 112 is pressed again, the system can display the current object distance, the stored object distance of the previously frozen image, and/or the difference between the two object distances.
      <br/>
      By updating and displaying, in real time, the current object distance and/or the difference between the current object distance and the stored object distance, the user can search for the lowest point (or highest point) of the feature whose depth (or height) is to be determined.
    </p>
    <p num="84">
      The procedure for determining the depth or height of a feature can be automated even further with the system of the present invention.
      <br/>
      For example, in one embodiment, once the user has aimed the scope at a feature whose depth or height is to be determined, the system can then automatically carry out the depth/height measurement procedure.
      <br/>
      Using the automatic focusing procedure described above, the system can focus on and determine the object distance of each of a predefined number of uniformly distributed points within a window of a predefined size surrounding the feature.
      <br/>
      Once all points within the window have been measured, the system then determines the minimum and maximum values of the plurality of object distances measured.
      <br/>
      The system then determines the difference between the minimum and maximum object distance values, which difference represents the depth or height of the feature.
    </p>
    <p num="85">
      It will be appreciated that when measuring the depth or height of a feature relative to a surrounding flat surface, the scope should preferably be oriented so that its optical axis is perpendicular to the surface.
      <br/>
      If this, however, is not the case, at least three points on the surface must be measured in order to identify the plane of the surface.
      <br/>
      Once the plane has been identified, the feature's depth or height relative to the plane can then be determined.
      <br/>
      Such a procedure can be carried out, for instance, with the embodiment of FIG. 10 with modified software.
      <br/>
      Using the cursor control, the user can define three points in the plane surrounding the feature of interest.
      <br/>
      The system of the present invention then automatically focuses on and determines the object distance for each of the three points.
      <br/>
      Using the cursor position information and the object distance calculated for each point, the system thereby has three-dimensional coordinates for each point.
      <br/>
      The system then uses those coordinates to determine the plane of the surface surrounding the feature of interest.
      <br/>
      When the user then positions the cursor on the displayed image of the feature, the system measures the object distance to the point defined by the cursor and calculates and displays the depth or height of that point on the feature relative to the plane surrounding the feature.
      <br/>
      The user can then iteratively position the cursor on the displayed image of the feature until he determines the maximum depth or height, or any intermediate depth or height of interest.
      <br/>
      This procedure can also be further automated, as discussed above, by programming the system to automatically focus and measure multiple points in a window surrounding a feature.
    </p>
    <p num="86">
      For applications which only call for the measurement of the depth or height of features, as opposed to their width, a simplification of the system of the present invention can be achieved by eliminating the determination of the magnification.
      <br/>
      In this case, only the object distance is required.
    </p>
  </description>
  <claims format="original" lang="en" id="claim_en">
    <claim num="1">
      <claim-text>What is claimed is:</claim-text>
      <claim-text>1.</claim-text>
      <claim-text>A system for determining a dimension of a feature, the system comprising:</claim-text>
      <claim-text>a) an optical scope for gathering an image of the feature, the optical scope including</claim-text>
      <claim-text>- i) a focusing device for adjusting a focal position of the image of the feature, - ii) a device for detecting a position of the focusing device and for providing a focus position signal based on the position of the focusing device, and - iii) an image-to-video converter for producing a video signal of the feature from the image of the feature having its focal position adjusted by the focusing device; b) an image scaling device for providing a scaled image size;</claim-text>
      <claim-text>and c) a processor, the processor - i) converting the focus position signal into at least one of an object distance signal and a magnification signal, and - ii) determining the dimension of the feature based on the scaled image size and based on the at least one of the object distance signal and the magnification signal, wherein the dimension of the feature includes at least one of a depth and height of the feature.</claim-text>
    </claim>
    <claim num="2">
      <claim-text>2. The system of claim 1, wherein the processor includes: i) a converter for converting the focus position into an object distance signal;</claim-text>
      <claim-text>and ii) means for determining the dimension of the feature by determining the difference between a first object distance signal and a second object distance signal.</claim-text>
    </claim>
    <claim num="3">
      <claim-text>3. A system for determining a dimension of a feature, the system comprising: a) an optical scope for producing an image of the feature, the optical scope including - i) a focusing device for adjusting a focal position of the image of the feature, - ii) a viewer for passing the image of the feature to a plane outside of the optical scope, and - iii) a device for detecting a position of the focusing device and for providing a focus position signal based on the position of the focusing device; b) a video camera optically coupled with the viewer of the optical scope and producing a video signal of the feature from the image of the feature; c) an image scaling device for providing a scaled image size;</claim-text>
      <claim-text>and d) a processor, the processor - i) a converting the focus position signal into at least one of an object distance signal and a magnification signal, and - ii) determining the dimension of the feature based on the scaled image size and based on the at least one of the object distance signal and the magnification signal, wherein the dimension of the feature includes at least one of a depth and height of the feature.</claim-text>
    </claim>
    <claim num="4">
      <claim-text>4. The system of claim 3, wherein the processor includes: i) a converter for converting the focus position into an object distance signal;</claim-text>
      <claim-text>and ii) means for determining the dimension of the feature by determining the difference between a first object distance signal and a second object distance signal.</claim-text>
    </claim>
    <claim num="5">
      <claim-text>5. A system for determining a dimension of a feature, the system comprising: a) an optical scope for gathering an image of the feature, the optical scope including - i) a focusing device for adjusting a focal position of the image of the feature, and - ii) a device for detecting a position of the focusing device and for providing a focus position signal based on the position of the focusing device; b) a processor, the processor - i) converting the focus position signal into an object distance signal, and - ii) determining a dimension of the feature based on the object distance signal.</claim-text>
    </claim>
    <claim num="6">
      <claim-text>6. The system of claim 5, wherein the dimension of the feature includes at least one of a depth and height of the feature.</claim-text>
    </claim>
    <claim num="7">
      <claim-text>7. The system of claim 6, wherein the processor includes: i) a converter for converting the focus position into an object distance signal;</claim-text>
      <claim-text>and ii) means for determining the dimension of the feature by determining the difference between a first object distance signal and a second object distance signal.</claim-text>
    </claim>
  </claims>
</questel-patent-document>