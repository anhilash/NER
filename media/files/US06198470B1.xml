<?xml version="1.0" encoding="UTF-8"?>
<questel-patent-document lang="en" date-produced="20180805" produced-by="Questel" schema-version="3.23" file="US06198470B1.xml">
  <bibliographic-data lang="en">
    <publication-reference publ-desc="Granted patent as first publication">
      <document-id>
        <country>US</country>
        <doc-number>06198470</doc-number>
        <kind>B1</kind>
        <date>20010306</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>US6198470</doc-number>
      </document-id>
    </publication-reference>
    <original-publication-kind>B1</original-publication-kind>
    <application-reference family-id="22828316" extended-family-id="1231389">
      <document-id>
        <country>US</country>
        <doc-number>09221561</doc-number>
        <kind>A</kind>
        <date>19981228</date>
      </document-id>
      <document-id data-format="questel">
        <doc-number>1998US-09221561</doc-number>
      </document-id>
      <document-id data-format="questel_Uid">
        <doc-number>1277420</doc-number>
      </document-id>
    </application-reference>
    <language-of-filing>en</language-of-filing>
    <language-of-publication>en</language-of-publication>
    <priority-claims>
      <priority-claim kind="national" sequence="1">
        <country>US</country>
        <doc-number>22156198</doc-number>
        <kind>A</kind>
        <date>19981228</date>
        <priority-active-indicator>Y</priority-active-indicator>
      </priority-claim>
      <priority-claim data-format="questel" sequence="1">
        <doc-number>1998US-09221561</doc-number>
      </priority-claim>
    </priority-claims>
    <dates-of-public-availability>
      <publication-of-grant-date>
        <date>20010306</date>
      </publication-of-grant-date>
    </dates-of-public-availability>
    <classifications-ipcr>
      <classification-ipcr sequence="1">
        <text>G06F   3/033       20060101A I20051008RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>06</class>
        <subclass>F</subclass>
        <main-group>3</main-group>
        <subgroup>033</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20051008</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="2">
        <text>G06F   3/038       20060101ALI20060310RMJP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>06</class>
        <subclass>F</subclass>
        <main-group>3</main-group>
        <subgroup>038</subgroup>
        <symbol-position>L</symbol-position>
        <classification-value>I</classification-value>
        <generating-office>
          <country>JP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20060310</date>
        </action-date>
      </classification-ipcr>
      <classification-ipcr sequence="3">
        <text>G06F   3/043       20060101A I20070721RMEP</text>
        <ipc-version-indicator>
          <date>20060101</date>
        </ipc-version-indicator>
        <classification-level>A</classification-level>
        <section>G</section>
        <class>06</class>
        <subclass>F</subclass>
        <main-group>3</main-group>
        <subgroup>043</subgroup>
        <classification-value>I</classification-value>
        <generating-office>
          <country>EP</country>
        </generating-office>
        <classification-status>R</classification-status>
        <classification-data-source>M</classification-data-source>
        <action-date>
          <date>20070721</date>
        </action-date>
      </classification-ipcr>
    </classifications-ipcr>
    <classification-national>
      <country>US</country>
      <main-classification>
        <text>345157000</text>
        <class>345</class>
        <subclass>157000</subclass>
      </main-classification>
      <further-classification sequence="1">
        <text>178018010</text>
        <class>178</class>
        <subclass>018010</subclass>
      </further-classification>
      <further-classification sequence="2">
        <text>178018030</text>
        <class>178</class>
        <subclass>018030</subclass>
      </further-classification>
      <further-classification sequence="3">
        <text>178018040</text>
        <class>178</class>
        <subclass>018040</subclass>
      </further-classification>
      <further-classification sequence="4">
        <text>345173000</text>
        <class>345</class>
        <subclass>173000</subclass>
      </further-classification>
      <further-classification sequence="5">
        <text>345177000</text>
        <class>345</class>
        <subclass>177000</subclass>
      </further-classification>
    </classification-national>
    <classifications-ecla>
      <classification-ecla sequence="1">
        <text>G06F-003/043</text>
        <section>G</section>
        <class>06</class>
        <subclass>F</subclass>
        <main-group>3</main-group>
        <subgroup>043</subgroup>
      </classification-ecla>
    </classifications-ecla>
    <patent-classifications>
      <patent-classification sequence="1">
        <classification-scheme office="EP" scheme="CPC">
          <date>20130101</date>
        </classification-scheme>
        <classification-symbol>G06F-003/043</classification-symbol>
        <section>G</section>
        <class>06</class>
        <subclass>F</subclass>
        <main-group>3</main-group>
        <subgroup>043</subgroup>
        <symbol-position>F</symbol-position>
        <classification-value>I</classification-value>
        <classification-status>B</classification-status>
        <classification-data-source>H</classification-data-source>
        <action-date>
          <date>20130101</date>
        </action-date>
      </patent-classification>
    </patent-classifications>
    <number-of-claims>20</number-of-claims>
    <exemplary-claim>1</exemplary-claim>
    <figures>
      <number-of-drawing-sheets>3</number-of-drawing-sheets>
      <number-of-figures>4</number-of-figures>
      <image-key data-format="questel">US6198470</image-key>
    </figures>
    <invention-title format="original" lang="en" id="title_en">Computer input device</invention-title>
    <references-cited>
      <citation srep-phase="examiner">
        <patcit num="1">
          <text>THORNTON WILLIAM E</text>
          <document-id>
            <country>US</country>
            <doc-number>4375674</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4375674</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="2">
          <text>BARRY GEORGE</text>
          <document-id>
            <country>US</country>
            <doc-number>5059959</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5059959</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <patcit num="3">
          <text>HIRSHIK ANDREW S</text>
          <document-id>
            <country>US</country>
            <doc-number>5757361</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5757361</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="4">
          <text>DAVISON KEITH K</text>
          <document-id>
            <country>US</country>
            <doc-number>4682159</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4682159</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="5">
          <text>ZIMMERMAN THOMAS G, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>4988981</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US4988981</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="6">
          <text>WAMBACH MARK L</text>
          <document-id>
            <country>US</country>
            <doc-number>5444462</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5444462</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="7">
          <text>SEEBACH JURGEN</text>
          <document-id>
            <country>US</country>
            <doc-number>5453759</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5453759</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="8">
          <text>ZLOOF MOSHE M</text>
          <document-id>
            <country>US</country>
            <doc-number>5489922</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5489922</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="9">
          <text>FREEMAN WILLIAM T, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5594469</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5594469</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="10">
          <text>ENG TOMMY K, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5638092</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5638092</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="11">
          <text>ITO EIJI, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5683092</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5683092</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="12">
          <text>BALL JAMES V</text>
          <document-id>
            <country>US</country>
            <doc-number>5686942</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5686942</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="13">
          <text>MIZOUCHI SATORU</text>
          <document-id>
            <country>US</country>
            <doc-number>5717413</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5717413</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="14">
          <text>ROMANIK JR CARL J</text>
          <document-id>
            <country>US</country>
            <doc-number>5729475</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5729475</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="15">
          <text>CHEUNG NINA T</text>
          <document-id>
            <country>US</country>
            <doc-number>5736976</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5736976</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="16">
          <text>HILBRINK JOHAN O, et al</text>
          <document-id>
            <country>US</country>
            <doc-number>5754126</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5754126</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="applicant">
        <patcit num="17">
          <text>KIKINIS DAN</text>
          <document-id>
            <country>US</country>
            <doc-number>5790100</doc-number>
            <kind>A</kind>
          </document-id>
          <document-id data-format="questel">
            <doc-number>US5790100</doc-number>
          </document-id>
        </patcit>
      </citation>
      <citation srep-phase="examiner">
        <nplcit num="1">
          <text>"Virtual Integrated Mouse," IBM Technical Disclosure Bulletin, Mar. 1, 1988, vol. 30, Iss. 10, pp 398-401.</text>
        </nplcit>
      </citation>
    </references-cited>
    <parties>
      <applicants>
        <applicant app-type="applicant" sequence="1">
          <addressbook lang="en">
            <name>AGAM URI</name>
          </addressbook>
        </applicant>
        <applicant app-type="applicant" sequence="2">
          <addressbook lang="en">
            <name>GAL ELI</name>
          </addressbook>
        </applicant>
        <applicant app-type="applicant" sequence="3">
          <addressbook lang="en">
            <name>BEN-BASSAT ELI</name>
          </addressbook>
        </applicant>
        <applicant app-type="applicant" sequence="4">
          <addressbook lang="en">
            <name>JASHEK RONEN</name>
          </addressbook>
        </applicant>
        <applicant app-type="applicant" sequence="5">
          <addressbook lang="en">
            <name>BARATZ YARON</name>
          </addressbook>
        </applicant>
      </applicants>
      <inventors>
        <inventor data-format="original" sequence="1">
          <addressbook lang="en">
            <name>Agam, Uri</name>
            <address>
              <address-1>Petach Tikvah 49553, IL</address-1>
              <city>Petach Tikvah 49553</city>
              <country>IL</country>
            </address>
          </addressbook>
          <nationality>
            <country>IL</country>
          </nationality>
        </inventor>
        <inventor data-format="original" sequence="2">
          <addressbook lang="en">
            <name>Gal, Eli</name>
            <address>
              <address-1>Ramat Gan 52289, IL</address-1>
              <city>Ramat Gan 52289</city>
              <country>IL</country>
            </address>
          </addressbook>
          <nationality>
            <country>IL</country>
          </nationality>
        </inventor>
        <inventor data-format="original" sequence="3">
          <addressbook lang="en">
            <name>Ben-Bassat, Eli</name>
            <address>
              <address-1>Holon 58483, IL</address-1>
              <city>Holon 58483</city>
              <country>IL</country>
            </address>
          </addressbook>
          <nationality>
            <country>IL</country>
          </nationality>
        </inventor>
        <inventor data-format="original" sequence="4">
          <addressbook lang="en">
            <name>Jashek, Ronen</name>
            <address>
              <address-1>Yashresh Village 76838, IL</address-1>
              <city>Yashresh Village 76838</city>
              <country>IL</country>
            </address>
          </addressbook>
          <nationality>
            <country>IL</country>
          </nationality>
        </inventor>
        <inventor data-format="original" sequence="5">
          <addressbook lang="en">
            <name>Baratz, Yaron</name>
            <address>
              <address-1>Givatayim 53229, IL</address-1>
              <city>Givatayim 53229</city>
              <country>IL</country>
            </address>
          </addressbook>
          <nationality>
            <country>IL</country>
          </nationality>
        </inventor>
      </inventors>
      <agents>
        <agent sequence="1" rep-type="agent">
          <addressbook lang="en">
            <orgname>Abelman, Frayne &amp; Schwab</orgname>
          </addressbook>
        </agent>
      </agents>
    </parties>
    <examiners>
      <primary-examiner>
        <name>Hjerpe, Richard A.</name>
      </primary-examiner>
    </examiners>
    <lgst-data>
      <lgst-status>LAPSED</lgst-status>
    </lgst-data>
  </bibliographic-data>
  <abstract format="original" lang="en" id="abstr_en">
    <p id="P-EN-00001" num="00001">
      <br/>
      A digital input device for entering data into a digital system, comprises at least two remote sensors and a control unit.
      <br/>
      A working area is set up in proximity to the sensors and the position of an object within the working area is detected by the input device based on data provided by each sensor and transmitted, by said control unit, to said digital system.
      <br/>
      The position of an object outside said working area, if detected by said input device, is rejected by said control unit and is not transmitted to said digital system.
      <br/>
      The sensors can be ultrasonic sensors and the object can be a part of the body of a user.
      <br/>
      Use of a third sensor allows three-dimensional detection of the object.
    </p>
  </abstract>
  <description format="original" lang="en" id="desc_en">
    <heading>FIELD OF THE INVENTION</heading>
    <p num="1">The present invention relates to a computer input device, and more particularly but not exclusively to a computer input device which is operable to manipulate a point or cursor, in an image or on a screen.</p>
    <heading>BACKGROUND OF THE INVENTION</heading>
    <p num="2">
      Computer input devices such as the keyboard, touchscreen and mouse are well known.
      <br/>
      However these comprise moving parts and are thus subject to wear.
      <br/>
      Wired input devices which are at least in part worn or held by the user, are known from U.S. Pat. Nos. 4,682,159 and 4,988,981.
    </p>
    <p num="3">In some cases wires are found to be inconvenient or otherwise undesirable, and there are thus known wireless cursor manipulation devices which are worn or held by a user and examples are known from U.S. Pat. Nos. 5,444,462, 5,453,759, 5,754,126, 5,736,976, 5,638,092, 5,790,100, 5,489,922 and 5,729,475.</p>
    <p num="4">
      Systems in which no device is either worn or held by the user and which use electromagnetic radiation to monitor the location of a body part are known from U.S. Pat. Nos. 5,717,413 and 5,686,942.
      <br/>
      In the latter case it is also disclosed to use ultrasonic waves.
    </p>
    <heading>SUMMARY OF THE INVENTION</heading>
    <p num="5">
      According to a first aspect of the present invention there is provided a digital input device for entering data into a digital system.
      <br/>
      The device comprises at least two remote sensors and a control unit.
      <br/>
      A working area is set, that is to say typically but not necessarily predefined, in proximity to the remote sensors, and the position of an object within the working area is detected by the input device based on data provided by the sensors and transmitted, by the control unit, to the digital system.
      <br/>
      However, if any of the parameters of the position of an object are detected to be outside the working area, then they are rejected by the control unit and are not transmitted to the digital system.
      <br/>
      Preferably such parameters are replaced by values indicating the edge of said working area.
    </p>
    <p num="6">
      The sensors are preferably ultrasonic transducers, and preferably each detect the distance of the object therefrom.
      <br/>
      The control unit may convert the detected distances into co-ordinates relative to an origin within the working area.
      <br/>
      The object may typically be a part of the body of a user, for example a finger.
    </p>
    <p num="7">Preferably, the second sensor is placed perpendicularly to the first sensor.</p>
    <p num="8">In a particularly preferred embodiment, an angle subtended by the object with an axis normal to at least one of the sensors, is ignored in obtaining the distance of the object from the sensor.</p>
    <p num="9">There may be provided a third sensor, enabling the control unit to calculate 3-dimensional co-ordinates of the object within the working area.</p>
    <p num="10">
      According to a second aspect of the present invention there is provided a digital input device for entering data into a digital system, comprising a first and a second remote sensor and a control unit.
      <br/>
      A working area is set in proximity to the sensors and the position of an object within the working area is detected by the input device based on data provided by each sensor and transmitted, by the control unit, to the digital system.
      <br/>
      The position of an object outside the working area, if detected by the input device, is rejected by the control unit and is not transmitted to the digital system.
      <br/>
      The object is part of the body of a user.
      <br/>
      Any parameters that are found to be beyond the working area are preferably replaced by parameters indicating the edge of the working area.
      <br/>
      Thus the point or cursor never moves off the screen.
    </p>
    <p num="11">
      According to a third aspect of the present invention there is provided a digital input device for entering data into a digital system, comprising at least a first and a second remote sensor and a control unit.
      <br/>
      A working volume is set in proximity to the remote sensors and the position of an object within the working volume is detected by the input device based on data provided by each sensor and transmitted, by the control unit, to the digital system.
      <br/>
      However the position of an object outside the working volume, if detected by the input device, is rejected by the control unit and is not transmitted to the digital system.
      <br/>
      Any parameters that are found to be beyond the working area are preferably replaced by parameters indicating the edge of the working area.
      <br/>
      Thus the point or cursor never moves off the screen.
    </p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading>
    <p num="12">
      For a better understanding of the invention and to show how the same may be carried into effect, reference will now be made, purely by way of example, to the accompanying drawings in which,
      <br/>
      FIG. 1 shows the layout of a first embodiment of the present invention,
      <br/>
      FIG. 2 shows how the layout of FIG. 1 may be used to obtain the co-ordinates of an object within the working area,
      <br/>
      FIG. 3 is a block diagram of an embodiment of the present invention, and
      <br/>
      FIG. 4 is a flow chart showing operation of an embodiment of a device according to the present invention.
    </p>
    <heading>DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
    <p num="13">
      FIG. 1 is a diagram showing the layout of a first embodiment of a computer input device operative in accordance with the present invention.
      <br/>
      In FIG. 1 a working area 10 is defined in which a user, or at least a moving part of the user's body, such as a finger, may be expected to be located.
      <br/>
      A first sensor 12 is located at a given distance from the working area 10 in a first direction and a second sensor 14 is preferably located at the same distance away from the working area 10 but in a second direction which is perpendicular to the aforementioned first direction.
    </p>
    <p num="14">A defined working area, such as that indicated by reference numeral 10 in FIG. 1, is desirable as it permits the user easily to choose not to transmit his movements to the cursor, in the same way that lifting a mouse deactivates movement of the cursor.</p>
    <p num="15">Although FIG. 1 shows two sensors, and is limited to two-dimensional measurement, it is further possible to add a third sensor so that three-dimensional information can be obtained.</p>
    <p num="16">Although the example has been given of a user's finger, the object does not need to be as small as a finger, in contrast to the prior art citation 5,686,942, in which a point object is required.</p>
    <p num="17">
      Each sensor, 12, 14 is preferably an ultrasonic transducer, and is able to scan a cone shaped area denoted by triangles 16 and 18 respectively by emitting a series of ultrasonic pulses.
      <br/>
      By timing the reflections a distance to the object is calculated, as described below with respect to FIG. 2.
      <br/>
      The distance to a reference point, preferably at the center of the working area 10, is known to the system and, as shown in FIG. 2, this enables distances to be calculated from the reference point to an object, such as the user's hand, which is being measured.
      <br/>
      Furthermore the ability to discern distance allows objects outside the working area to be rejected.
    </p>
    <p num="18">
      FIG. 2 shows how the embodiment of FIG. 1 may be used to obtain the co-ordinates of an object 20 with respect to an origin 22 at the center of the working area 10.
      <br/>
      The distance from each sensor 12, 14, to the origin 22 is known in advance.
      <br/>
      A distance d1 is measured from object 20 to the sensor 12, and distance d2 is measured to the second sensor 14.
      <br/>
      The projection of these lengths d1 and d2 onto the respective X and Y axes are indicated by D1 and D2.
    </p>
    <p num="19">Thus D1=d1 cos  ALPHA , and D2=d2 cos  BETA</p>
    <p num="20">where  ALPHA  and  BETA  are the angles respectively which d1 and d2 subtend with their respective axes.</p>
    <p num="21">
      If the system is appropriately set up it can generally be assumed that  ALPHA  and  BETA  are small angles.
      <br/>
      Thus cos  ALPHA  and cos  BETA  may be approximated to 1.
      <br/>
      Thus D1=d1 and D2=d2.
      <br/>
      Each co-ordinate from the origin can then be obtained by subtracting D1 and D2 from the known distance of the sensor to the origin.
    </p>
    <p num="22">
      The resulting co-ordinates are in a form that can be utilized by appropriate software in order to manipulate a cursor on a screen.
      <br/>
      The co-ordinates, which are in relation to the working area can preferably be reused directly with a new origin in the center of the screen, or in the case of 3D, in the center of a box receding from the screen.
    </p>
    <p num="23">
      In addition to detecting position, as described above, it is also possible to compare successive position measurements and thus to obtain motion.
      <br/>
      It is thus possible to arrange the device so that stationary objects are ignored.
    </p>
    <p num="24">
      FIG. 3 is a block diagram of au embodiment operative in accordance with the present invention incorporated into a computer system.
      <br/>
      Transducers 12 and 14 each comprise a transmitter 30 which emits ultrasonic pulses and a detector 32 which senses ultrasonic energy, from the pulses, as it is reflected back from an object.
    </p>
    <p num="25">
      A control unit 34 comprises a microcontroller 36, which operates a sensor driver 38 to send a burst signal to the transmitters 30.
      <br/>
      Each burst signal is transformed into an ultrasonic pulse and an echo is received by the detector 32.
      <br/>
      The frequency of the burst signal is typically 50-200 Hz but the frequency is not critical.
    </p>
    <p num="26">
      The echo is reconverted by the detectors 32 into an electronic signal and is amplified by amplifier 40.
      <br/>
      It is then converted into a digital signal using comparator 42 and the microcontroller then compares the time of receipt with the time of transmission to arrive at a distance of the object from the sensor.
      <br/>
      At this point any echoes from the outside of the defined working volume 10 are rejected and co-ordinates are calculated as described with respect to FIG. 2 above.
    </p>
    <p num="27">
      The control unit is connected to a computer 44, preferably by a standard serial link in the same way that a mouse is generally connected.
      <br/>
      The co-ordinates of the object, obtained as above, are transmitted via the serial link and, as they are in digital form, can be utilized by the computer 44 without needing extensive driver software.
    </p>
    <p num="28">
      FIG. 4 is a flow chart showing the sequence of operation of an embodiment according to the present invention.
      <br/>
      In FIG. 4 signals are sent via each of the sensors and a counter is activated.
      <br/>
      As echoes are received the appropriate delays are recorded and the distances D1 and D2 are calculated as described above with respect to FIG. 2.
      <br/>
      A loop is then entered in which values outside the working volume, defined by Dmax and Dmin are excluded, or preferably are forced to Dmax and Dmin, the current values are stored and the co-ordinates themselves are calculated and transmitted to the computer.
      <br/>
      Although the step labeled "transmit to PC" is shown below the step "calculate co-ordinates" at the base of the flowchart of FIG. 4, the skilled person will appreciate that this step could equally well be inserted at numerous other points therein.
    </p>
    <p num="29">The forcing of the values to Dmax and Dmin as appropriate ensures that, when the object passes the edge of the screen, the cursor remains at the edge of the screen and continues to respond only to that value or those values which are still within the working area.</p>
    <p num="30">
      There is thus provided a digital data input device which relies solely on remote sensing of the position of an object, wherein the object is any object not transparent to the sensing pulses, and does not require the user to wear or hold any device.
      <br/>
      Furthermore the user can deactivate data input simply by moving the object outside the predefined working area.
    </p>
    <p num="31">
      It is appreciated that various features of the invention which are, for clarity, described in the contexts of separate embodiments may also be provided in combination in a single embodiment.
      <br/>
      Conversely, various features of the invention which are, for brevity, described in the context of a single embodiment may also be provided separately or in any suitable subcombination.
    </p>
    <p num="32">
      It will be appreciated by persons skilled in the art that the present invention is not limited to what has been particularly shown and described hereinabove.
      <br/>
      Rather, the scope of the present invention is defined only by the claims that follow:
    </p>
  </description>
  <claims format="original" lang="en" id="claim_en">
    <claim num="1">
      <claim-text>What is claimed is:</claim-text>
      <claim-text>1.</claim-text>
      <claim-text>A digital input device for entering data into a digital system, comprising at least a first and a second remote sensor and a control unit, wherein a working area is set in proximity to the remote sensors and wherein parameters, within the working area, of a position of an object are detected by the input device based on data provided by each sensor and transmitted, by said control unit, to said digital system, and parameters outside said working area, of said object, if detected by said input device are rejected by said control unit and are not transmitted to said digital systems, wherein the second sensor is placed perpendicularly to the first sensor and an angle subtended by the object with an axis normal to at least one of said sensors, is ignored in obtaining the distance of the object from said sensor.</claim-text>
    </claim>
    <claim num="2">
      <claim-text>2. A device according to claim 1, wherein said rejected parameters are replaced by parameters representing an edge of said working area.</claim-text>
    </claim>
    <claim num="3">
      <claim-text>3. A device according to claim 1, wherein said working area is predefined with respect to said remote sensors.</claim-text>
    </claim>
    <claim num="4">
      <claim-text>4. A device according to claim 1, wherein said sensors are ultrasonic transducers.</claim-text>
    </claim>
    <claim num="5">
      <claim-text>5. A device according to claim 1, wherein said sensors each detect the distance of said object therefrom and wherein said control unit converts said detected distances into co-ordinates relative to an origin within said working area.</claim-text>
    </claim>
    <claim num="6">
      <claim-text>6. A device according to claim 1, wherein the object is a part of the body of a user.</claim-text>
    </claim>
    <claim num="7">
      <claim-text>7. A device according to claim 4, wherein the object is a part of the body of a user.</claim-text>
    </claim>
    <claim num="8">
      <claim-text>8. A device according to claim 5, wherein the object is a part of the body of a user.</claim-text>
    </claim>
    <claim num="9">
      <claim-text>9. A device according to claim 1, having a third sensor, and wherein said control unit is operable to calculate 3-dimensional co-ordinates of said object within said working area.</claim-text>
    </claim>
    <claim num="10">
      <claim-text>10. A device according to claim 9, wherein the object is a part of the body of a user.</claim-text>
    </claim>
    <claim num="11">
      <claim-text>11. A device according to claim 1, wherein the position of the object is translated into a point in a computerized image.</claim-text>
    </claim>
    <claim num="12">
      <claim-text>12. A digital input device for entering data into a digital system, comprising at least a first and a second remote sensor and a control unit, wherein a working volume is set in proximity to the remote sensors and wherein parameters, within the working volume, of a position of an object are detected by the input device based on data provided by each sensor and transmitted, by said control unit, to said digital system, and parameters outside said working volume, of said object, if detected by said input device are rejected by said control unit and are not transmitted to said digital system, wherein the second sensor is placed perpendicularly to the first sensor and an angle subtended by the object with an axis normal to at least one of said sensors, is ignored in obtaining the distance of the object from said sensor.</claim-text>
    </claim>
    <claim num="13">
      <claim-text>13. A device according to claim 12, wherein said rejected parameters are replaced by parameters representing an edge of said working area.</claim-text>
    </claim>
    <claim num="14">
      <claim-text>14. A device according to claim 12, wherein said sensors are ultrasonic transducers.</claim-text>
    </claim>
    <claim num="15">
      <claim-text>15. A device according to claim 12, wherein said sensors each detect the distance of said object therefrom and wherein said control unit converts said detected distances into co-ordinates relative to an origin within said working volume.</claim-text>
    </claim>
    <claim num="16">
      <claim-text>16. A device according to claim 12, wherein the object is a part of the body of a user.</claim-text>
    </claim>
    <claim num="17">
      <claim-text>17. A device according to claim 14, wherein the object is a part of the body of a user.</claim-text>
    </claim>
    <claim num="18">
      <claim-text>18. A device according to claim 15, wherein the object is a part of the body of a user.</claim-text>
    </claim>
    <claim num="19">
      <claim-text>19. A device according to claim 12, having a third sensor, and wherein said control unit is operable to calculate 3-dimensional co-ordinates of said object within said working volume.</claim-text>
    </claim>
    <claim num="20">
      <claim-text>20. A device according to claim 12, wherein said working volume is predefined with respect to said remote sensors.</claim-text>
    </claim>
  </claims>
</questel-patent-document>